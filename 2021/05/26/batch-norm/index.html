<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="1xbHmVEsS5Fta14x1d1FXkHFr93LRr1pMxipdYVcNI4">
  <meta name="baidu-site-verification" content="code-Srzz2vTuzA">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;swag1ong.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;Searching...&quot;,&quot;empty&quot;:&quot;We didn&#39;t find any results for the search: ${query}&quot;,&quot;hits_time&quot;:&quot;${hits} results found in ${time} ms&quot;,&quot;hits&quot;:&quot;${hits} results found&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:3,&quot;unescape&quot;:false,&quot;preload&quot;:true}}</script>
<meta name="description" content="Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Internal covariate shift describes the phenomenon in training DNN that the distribution of each layer&#39;s inp">
<meta property="og:type" content="article">
<meta property="og:title" content="Batch Normalization">
<meta property="og:url" content="https://swag1ong.github.io/2021/05/26/batch-norm/index.html">
<meta property="og:site_name" content="GoGoGogo!">
<meta property="og:description" content="Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Internal covariate shift describes the phenomenon in training DNN that the distribution of each layer&#39;s inp">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://swag1ong.github.io/images/ML/batch_norm_1.png">
<meta property="og:image" content="https://swag1ong.github.io/images/ML/batch_norm_2.png">
<meta property="og:image" content="https://swag1ong.github.io/images/ML/batch_norm_3.png">
<meta property="og:image" content="https://swag1ong.github.io/images/ML/batch_norm_4.png">
<meta property="article:published_time" content="2021-05-26T02:10:03.000Z">
<meta property="article:modified_time" content="2021-06-02T08:41:04.000Z">
<meta property="article:author" content="Zhu, Zhaoyang">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Normalization">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://swag1ong.github.io/images/ML/batch_norm_1.png">


<link rel="canonical" href="https://swag1ong.github.io/2021/05/26/batch-norm/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;en&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;swag1ong.github.io&#x2F;2021&#x2F;05&#x2F;26&#x2F;batch-norm&#x2F;&quot;,&quot;path&quot;:&quot;2021&#x2F;05&#x2F;26&#x2F;batch-norm&#x2F;&quot;,&quot;title&quot;:&quot;Batch Normalization&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Batch Normalization | GoGoGogo!</title><script src="/js/config.js"></script>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?852b005027c5efa81663f6f5c4c5b7fd"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">GoGoGogo!</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-igloo fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">107</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">16</span></a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">25</span></a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-address-card fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift"><span class="nav-number">1.</span> <span class="nav-text">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-number">1.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#towards-reducing-internal-covariate-shift"><span class="nav-number">1.2.</span> <span class="nav-text">Towards Reducing Internal Covariate Shift</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#normalization-via-mini-batch-statistics"><span class="nav-number">1.3.</span> <span class="nav-text">Normalization via Mini-Batch Statistics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#training-and-inference-with-batch-normalized-network"><span class="nav-number">1.4.</span> <span class="nav-text">Training and Inference with Batch Normalized Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#training"><span class="nav-number">1.4.1.</span> <span class="nav-text">Training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#testing"><span class="nav-number">1.4.2.</span> <span class="nav-text">Testing</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#batch-norm-in-cov-net"><span class="nav-number">1.5.</span> <span class="nav-text">Batch Norm in Cov Net</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#side-effects-of-batch-norm"><span class="nav-number">1.6.</span> <span class="nav-text">Side Effects of Batch Norm</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhu, Zhaoyang"
      src="/images/others/favicon.jpeg">
  <p class="site-author-name" itemprop="name">Zhu, Zhaoyang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">107</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/swag1ong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;swag1ong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyang.zhu@mail.utoronto.ca" title="E-Mail → mailto:zhaoyang.zhu@mail.utoronto.ca" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zhuzhaoyangzzyu0616" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zhuzhaoyangzzyu0616" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/05/26/batch-norm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Batch Normalization
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-05-26 10:10:03" itemprop="dateCreated datePublished" datetime="2021-05-26T10:10:03+08:00">2021-05-26</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-06-02 16:41:04" itemprop="dateModified" datetime="2021-06-02T16:41:04+08:00">2021-06-02</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/" itemprop="url" rel="index"><span itemprop="name">DL</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/Techniques/" itemprop="url" rel="index"><span itemprop="name">Techniques</span></a>
        </span>
    </span>

  
    <span id="/2021/05/26/batch-norm/" class="post-meta-item leancloud_visitors" data-flag-title="Batch Normalization" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>6.6k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>6 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</h1>
<p><strong>Internal covariate shift</strong> describes the phenomenon in training DNN that the distribution of each layer's inputs changes during training, as the parameters of the previous layer change. This slow down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating non-linearities (tanh, sigmoid). This problem can be addressed by normalizing the layer inputs.</p>
<h2 id="introduction">Introduction</h2>
<p>To see the input distribution shift in terms of sub-network or a layer. Consider a network computing:</p>
<p><span class="math display">\[l = F_2(F_1(\textbf{u}, \Theta_1), \Theta_2)\]</span></p>
<p>Where <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span> are arbitrary transformations, and the parameters <span class="math inline">\(\Theta_1, \Theta_2\)</span> are to be learned to minimize the loss <span class="math inline">\(l\)</span>. Learning <span class="math inline">\(\Theta_2\)</span> can be viewed as if inputs <span class="math inline">\(\textbf{x} = F_1(\textbf{u}, \Theta_1)\)</span> and fed into the sub-network:</p>
<p><span class="math display">\[l = F_2(\textbf{x}, \Theta_2)\]</span></p>
<p>Thus, a fixed input distribution (ie. the distribution of <span class="math inline">\(\textbf{x}\)</span>) would make the training more effective, because <span class="math inline">\(\Theta_2\)</span> does not need to adapt to the new distribution. At the same time, if the mapping <span class="math inline">\(F_2\)</span> is saturating non-linearity, a fixed-distribution will help to prevent the gradient of parameters from vanishing.</p>
<span id="more"></span>
<h2 id="towards-reducing-internal-covariate-shift">Towards Reducing Internal Covariate Shift</h2>
<p>Consider a layer with the input <span class="math inline">\(u\)</span> that adds the learned bias <span class="math inline">\(b\)</span>, and normalizes the result by subtracting the mean of the activation computed over the training data <span class="math inline">\(\hat{x} = x - \bar{x}\)</span>, where <span class="math inline">\(\bar{x} = \frac{1}{N} \sum_{i=1}^{N} x_i\)</span>. If we <strong>ignore the dependence of <span class="math inline">\(b\)</span> in <span class="math inline">\(\bar{x}\)</span></strong> (ie. <span class="math inline">\(\hat{x} = x - \mu\)</span> we just center the input by some constant <span class="math inline">\(\mu\)</span>, does not take into account <span class="math inline">\(b\)</span>), then</p>
<p><span class="math display">\[\frac{\partial L}{\partial b} = -\frac{\partial L}{\partial \hat{x}} \frac{\partial \hat{x}}{\partial b} = -\frac{\partial L}{\partial \hat{x}}\]</span></p>
<p>Then, the update rule is:</p>
<p><span class="math display">\[b \leftarrow b - \frac{1}{N}\sum_{i}\frac{\partial L}{\partial \hat{x}}\]</span></p>
<p>Let, <span class="math inline">\(\nabla b = - \frac{1}{N}\sum_{i}\frac{\partial L}{\partial \hat{x}}\)</span> Then, new <span class="math inline">\(\hat{x}\)</span> after update <span class="math inline">\(b\)</span> is:</p>
<p><span class="math display">\[u + (b + \nabla b) + \frac{1}{N} \sum_i [u + (b + \nabla b)] = u + b - E[u + b]\]</span></p>
<p>Thus, the combination of the update to <span class="math inline">\(b\)</span> and subsequent change in normalization led to no change in the output of the layer nor, consequently, the loss (ie. center the input without taking account of <span class="math inline">\(b\)</span> will result useless update to <span class="math inline">\(b\)</span> that has no effect on the loss). However, as the training continues, <span class="math inline">\(b\)</span> will grow indefinitely while the loss remains the same.</p>
<p>This is caused because the gradient descent optimization does not take into account that the normalization takes place.</p>
<h2 id="normalization-via-mini-batch-statistics">Normalization via Mini-Batch Statistics</h2>
<p>Normalize the layer inputs are expensive, as it requires computing the covariance matrix, the inverse square root, the expectation over all training samples, as well as the derivative of these transformations. Several simplifications are proposed to deal with these.</p>
<ol type="1">
<li><p>Instead normalize the features in layer inputs and outputs jointly, we will normalize each scalar feature independently by making it have the mean of zero and the variance of 1. For a layer with <span class="math inline">\(d\)</span>-dimensional input <span class="math inline">\(x = (x^{1}, ..., x^{d})\)</span>, we will normalize each individual feature:</p>
<p><span class="math display">\[\hat{x}^{k} = \frac{x^k - E[x^k]}{\sqrt{Var[x^k]}}\]</span></p></li>
</ol>
<p>Where the expectation and variance are computed over the training dataset (i.e first input feature is normalized by first features of all input samples). However, this approach may decrease the expressive power of the activation function, for instance, if we scale the input of sigmoid activation to have mean 0 and variance 1, then we will be limited at the middle region of the sigmoid function (<strong>In this paper, batch norm is applied right after activation and before activation function</strong>)</p>
<p><img src="/images/ML/batch_norm_1.png"></p>
<p>To address this, we make sure that the transformation inserted in the network can represent the identity transformation, that is for each activation <span class="math inline">\(x^k\)</span>, a pair of parameters <span class="math inline">\(\gamma^k, \beta^k\)</span>, which scales and shift the normalized value:</p>
<p><span class="math display">\[y^k = \gamma^k \hat{x}^k + \beta^k\]</span></p>
<p>These parameters are learned through back propagation, and by setting <span class="math inline">\(\gamma^k = \sqrt{Var[x^k]}, \beta^k = E[x^k]\)</span>, we could recover the original activation, if that were the optimal thing to do.</p>
<ol start="2" type="1">
<li>Instead of using whole batch of data for estimating mean and variance which is impractical in practice. We only use mini-batches to estimate means and variance, because mini-batches produce unbiased estimates of the mean and variance. Note that the use of mini-batches is enabled by computation of <strong>per-dimension variances rather than joint covariances</strong> (we do not estimate the covariance). In joint case, regularization would be required since the mini-batch size is likely to be smaller than the number of activations, resulting in singular covariance matrices.</li>
</ol>
<p>Together, we can describe the batch norm transformation for each activation <span class="math inline">\(x\)</span> as:</p>
<p><img src="/images/ML/batch_norm_2.png"></p>
<p>Where <span class="math inline">\(\epsilon\)</span> is a constant added to the mini-batch variance for numerical stability, <span class="math inline">\(BN_{\gamma, \beta}: x_{1, ..., m} \rightarrow y_{1, ..., m}\)</span> is the batch norm transformation. Since the normalization is applied to each activation independently, <span class="math inline">\(k\)</span> which indicates the activation or dimension is dropped.</p>
<p>The backward pass can be write as:</p>
<p><img src="/images/ML/batch_norm_3.png"></p>
<p>Notice that, <span class="math inline">\(\gamma, \beta\)</span> have superscript <span class="math inline">\(k\)</span>, they bind to each activation.</p>
<h2 id="training-and-inference-with-batch-normalized-network">Training and Inference with Batch Normalized Network</h2>
<p>To <strong>Batch-Normalize</strong> a network, any layer that previously received <span class="math inline">\(x\)</span> as input now receive <span class="math inline">\(BN(x)\)</span>.</p>
<h3 id="training">Training</h3>
<p>A model employing Batch Normalization can be trained using batch gradient descent, or Stochastic Gradient Descent with a mini-batch size <span class="math inline">\(m &gt; 1\)</span> or any variant such as Adam. The normalization of activations that depends on the mini-batch allows efficient training. Using moving averages instead, we can track the accuracy of a model as it trains.</p>
<h3 id="testing">Testing</h3>
<p>During testing, it is neither necessary nor desirable to estimate mean and variance using mini-batches, thus, once the network has been trained, we use the normalization:</p>
<p><span class="math display">\[\hat{x} = \frac{x - E[x]}{\sqrt{Var[x] - \epsilon}}\]</span></p>
<p><span class="math display">\[Var[x] = \frac{m}{m - 1} E_{B}[\sigma^2_{B}]\]</span></p>
<p>where <span class="math inline">\(E[x]\)</span> is the mean for the whole dataset. The variance is a bias-corrected version of mini-batch sample variance over all training mini-batches weighted by their occurrence during training.</p>
<p><img src="/images/ML/batch_norm_4.png"></p>
<h2 id="batch-norm-in-cov-net">Batch Norm in Cov Net</h2>
<p>For convolutional layers, we additionally want the normalization to obey the convolutional property ----- so that different elements of the same feature map, at different locations, are normalized in the same way (ie. features in the same feature are normalized in same way). So in the <span class="math inline">\(BN(x)\)</span>, we replace <span class="math inline">\(B\)</span>: mini-batch by <span class="math inline">\(B\)</span>: the set of all values in a feature map across both the elements of a mini-batch and spatial locations, for a mini-batch of size <span class="math inline">\(m\)</span> and feature map of size <span class="math inline">\(p * q\)</span> <span class="math inline">\(|B| = m * p * q\)</span>. We learn <span class="math inline">\(\gamma^k, \beta^k\)</span> per feature map. So in total, we have <span class="math inline">\(C\)</span> different means and variances, where <span class="math inline">\(C\)</span> is number of input channels.</p>
<h2 id="side-effects-of-batch-norm">Side Effects of Batch Norm</h2>
<ol type="1">
<li><p>With Batch Norm, back-propagation through a layer is unaffected by the scale of its parameters. <span class="math display">\[ BN(\alpha W u) = BN(Wu) \]</span> <span class="math display">\[ \frac{\partial BN(\alpha Wu)}{\partial u} = \frac{\partial BN(Wu)}{\partial u}\]</span> <span class="math display">\[ \frac{\partial BN(\alpha Wu)}{\partial \alpha W} = \frac{1}{\alpha} \frac{\partial BN(Wu)}{\partial u}\]</span></p></li>
<li><p>Larger weights leads to smaller gradients, Batch Normalization stabilize the parameter growth:</p>
<p><span class="math display">\[\alpha = 100, \implies \frac{\partial BN(100 Wu)}{\partial 100 W} = \frac{1}{100} \frac{\partial BN(Wu)}{\partial u}\]</span></p></li>
<li><p>Use sample statistics from mini-batches introduce random noise, which helps with the overfitting.</p></li>
</ol>

    </div>

    
    
    
      
  <div class="popular-posts-header">Related Posts</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/06/02/layer-norm/" rel="bookmark">Layer Normalization</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/07/29/adaptive-lr/" rel="bookmark">Basic Adaptive LR Algorithms</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/06/22/alex-net/" rel="bookmark">Alex Net</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/06/24/VGG/" rel="bookmark">VGG (Under Construction)</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/08/06/attention/" rel="bookmark">Attention</a></div>
    </li>
  </ul>


    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
              <a href="/tags/Normalization/" rel="tag"># Normalization</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/05/25/drop-out/" rel="prev" title="Dropout">
                  <i class="fa fa-chevron-left"></i> Dropout
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/05/27/stochastic-graph/" rel="next" title="Stochastic Graph">
                  Stochastic Graph <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div><script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhu, Zhaoyang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Symbols count total">865k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">13:07</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.3" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{&quot;enable&quot;:true,&quot;app_id&quot;:&quot;gPOc95wWW2Hwp2pkVSAAz28m-MdYXbMMI&quot;,&quot;app_key&quot;:&quot;7Sf65xCHXfEEdBvu29UHSYdV&quot;,&quot;server_url&quot;:&quot;https:&#x2F;&#x2F;swag1ong.github.io&quot;,&quot;security&quot;:true}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{&quot;enable&quot;:true,&quot;repo&quot;:&quot;swag1ong&#x2F;swag1ong.github.io&quot;,&quot;issue_term&quot;:&quot;pathname&quot;,&quot;theme&quot;:&quot;github-light&quot;,&quot;pathname&quot;:&quot;pathname&quot;}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
