<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="1xbHmVEsS5Fta14x1d1FXkHFr93LRr1pMxipdYVcNI4">
  <meta name="baidu-site-verification" content="code-Srzz2vTuzA">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;swag1ong.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;Searching...&quot;,&quot;empty&quot;:&quot;We didn&#39;t find any results for the search: ${query}&quot;,&quot;hits_time&quot;:&quot;${hits} results found in ${time} ms&quot;,&quot;hits&quot;:&quot;${hits} results found&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:3,&quot;unescape&quot;:false,&quot;preload&quot;:true}}</script>
<meta name="description" content="Recurrent Neural Network Introduction Consider the recurrent equation: \[\mathbf{s}^{t} &#x3D; f(\mathbf{s}^{t-1}; \; \boldsymbol{\theta})\] For a finite time step \(\tau\), this equation can be unfolded b">
<meta property="og:type" content="article">
<meta property="og:title" content="RNN">
<meta property="og:url" content="https://swag1ong.github.io/2021/08/02/rnn/index.html">
<meta property="og:site_name" content="GoGoGogo!">
<meta property="og:description" content="Recurrent Neural Network Introduction Consider the recurrent equation: \[\mathbf{s}^{t} &#x3D; f(\mathbf{s}^{t-1}; \; \boldsymbol{\theta})\] For a finite time step \(\tau\), this equation can be unfolded b">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://swag1ong.github.io/images/ML/rnn_1.png">
<meta property="og:image" content="https://swag1ong.github.io/images/ML/rnn_2.png">
<meta property="og:image" content="https://swag1ong.github.io/images/ML/rnn_3.png">
<meta property="og:image" content="https://swag1ong.github.io/images/ML/rnn_4.png">
<meta property="og:image" content="https://swag1ong.github.io/images/ML/rnn_5.png">
<meta property="og:image" content="https://swag1ong.github.io/images/ML/rnn_6.png">
<meta property="og:image" content="https://swag1ong.github.io/images/ML/rnn_7.png">
<meta property="og:image" content="https://swag1ong.github.io/images/ML/lstm_1.png">
<meta property="og:image" content="https://swag1ong.github.io/images/ML/lstm_2.png">
<meta property="og:image" content="https://swag1ong.github.io/images/ML/gru_1.png">
<meta property="og:image" content="https://swag1ong.github.io/images/ML/gru_2.png">
<meta property="article:published_time" content="2021-08-02T05:09:55.000Z">
<meta property="article:modified_time" content="2021-08-25T06:17:31.000Z">
<meta property="article:author" content="Zhu, Zhaoyang">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://swag1ong.github.io/images/ML/rnn_1.png">


<link rel="canonical" href="https://swag1ong.github.io/2021/08/02/rnn/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;en&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;swag1ong.github.io&#x2F;2021&#x2F;08&#x2F;02&#x2F;rnn&#x2F;&quot;,&quot;path&quot;:&quot;2021&#x2F;08&#x2F;02&#x2F;rnn&#x2F;&quot;,&quot;title&quot;:&quot;RNN&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>RNN | GoGoGogo!</title><script src="/js/config.js"></script>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?852b005027c5efa81663f6f5c4c5b7fd"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">GoGoGogo!</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-igloo fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">99</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">16</span></a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">25</span></a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-address-card fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#recurrent-neural-network"><span class="nav-number">1.</span> <span class="nav-text">Recurrent Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-number">1.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#forward-pass"><span class="nav-number">1.2.</span> <span class="nav-text">Forward Pass</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#teacher-forcing-and-networks-with-output-recurrence"><span class="nav-number">1.3.</span> <span class="nav-text">Teacher Forcing and Networks with Output Recurrence</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#backward-pass"><span class="nav-number">1.4.</span> <span class="nav-text">Backward Pass</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bidirectional-rnns"><span class="nav-number">1.5.</span> <span class="nav-text">Bidirectional RNNs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#encoder-decoder"><span class="nav-number">1.6.</span> <span class="nav-text">Encoder-Decoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gated-rnn"><span class="nav-number">1.7.</span> <span class="nav-text">Gated RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#lstm"><span class="nav-number">1.7.1.</span> <span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gru"><span class="nav-number">1.7.2.</span> <span class="nav-text">GRU</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ref"><span class="nav-number">2.</span> <span class="nav-text">Ref</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhu, Zhaoyang"
      src="/images/others/favicon.jpeg">
  <p class="site-author-name" itemprop="name">Zhu, Zhaoyang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">99</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/swag1ong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;swag1ong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyang.zhu@mail.utoronto.ca" title="E-Mail → mailto:zhaoyang.zhu@mail.utoronto.ca" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zhuzhaoyangzzyu0616" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zhuzhaoyangzzyu0616" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/08/02/rnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          RNN
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-08-02 13:09:55" itemprop="dateCreated datePublished" datetime="2021-08-02T13:09:55+08:00">2021-08-02</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-08-25 14:17:31" itemprop="dateModified" datetime="2021-08-25T14:17:31+08:00">2021-08-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/" itemprop="url" rel="index"><span itemprop="name">DL</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/RNN/" itemprop="url" rel="index"><span itemprop="name">RNN</span></a>
        </span>
    </span>

  
    <span id="/2021/08/02/rnn/" class="post-meta-item leancloud_visitors" data-flag-title="RNN" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>13k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>11 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="recurrent-neural-network">Recurrent Neural Network</h1>
<h2 id="introduction">Introduction</h2>
<p>Consider the recurrent equation:</p>
<p><span class="math display">\[\mathbf{s}^{t} = f(\mathbf{s}^{t-1}; \; \boldsymbol{\theta})\]</span></p>
<p>For a finite time step <span class="math inline">\(\tau\)</span>, this equation can be unfolded by applying the definition <span class="math inline">\(\tau - 1\)</span> times:</p>
<p><span class="math display">\[f(f(....f(\mathbf{s}^{1}; \; \boldsymbol{\theta}) ... ; \; \boldsymbol{\theta}) ; \; \boldsymbol{\theta})\]</span></p>
<p>Then, this expression can now be represented as a DAG because it no longer involves recurrence:</p>
<p><img src="/images/ML/rnn_1.png" width="600"></p>
<p>Notice here, the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> are shared. The idea extends smoothly to:</p>
<p><span class="math display">\[\mathbf{s}^{t} = f(\mathbf{s}^{t-1}, \mathbf{x}^{t}; \; \boldsymbol{\theta})\]</span></p>
<p>We can see that now, <span class="math inline">\(s^{t}\)</span> contains information about the whole past <span class="math inline">\(\mathbf{x}^1 , ....., \mathbf{x}^t\)</span></p>
<p>Many Recurrent Neural Networks use similar idea to express their hidden units:</p>
<span class="math display">\[\begin{aligned}
\mathbf{h}^t &amp;= f(\mathbf{h}^{t-1}, \mathbf{x}^t ; \; \boldsymbol{\theta})\\
&amp;= g^t (\mathbf{x}^1 , ....., \mathbf{x}^t)
\end{aligned}\]</span>
<p>Typically, RNN will have output layers to output predictions at given timesteps. When the recurrent network is trained to perform a task that requires predicting the future from the past, the network typically learns to use <span class="math inline">\(\mathbf{h}^t\)</span> to give a lossy summary of past sequence up to time <span class="math inline">\(t\)</span>. The summary is lossy because we are mapping <span class="math inline">\(\mathbf{x}^1 , ....., \mathbf{x}^t\)</span> to a fixed length <span class="math inline">\(\mathbf{h}^t\)</span></p>
<p><img src="/images/ML/rnn_2.png" width="600"></p>
<p>The unfolded structure has several advantages:</p>
<ol type="1">
<li>The learned model <span class="math inline">\(f\)</span> is defined as transition from hidden units (input) <span class="math inline">\(h^{t - 1}\)</span> to <span class="math inline">\(h^{t}\)</span> (output) regardless the value of time <span class="math inline">\(t\)</span>. Thus, we can have one model for different lengths of sequences.</li>
<li>The parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> are shared.</li>
</ol>
<span id="more"></span>
<h2 id="forward-pass">Forward Pass</h2>
<p><img src="/images/ML/rnn_3.png" width="600"></p>
<p>In formulas, the forward pass for <strong>same output and input length</strong> RNN with <strong>tanh non-linearity and probability as prediction</strong> can be described as starting from <span class="math inline">\(\mathbf{h}^0\)</span>:</p>
<p><span class="math display">\[\mathbf{a}^{t} = \mathbf{b} + W\mathbf{h}^{t-1} + U\mathbf{x}^{t}\]</span> <span class="math display">\[\mathbf{h}^t = \tanh (\mathbf{a}^t)\]</span> <span class="math display">\[\mathbf{o}^t = \mathbf{c} + V\mathbf{h}^t \]</span> <span class="math display">\[\hat{\mathbf{y}}^t = softmax(\mathbf{o}^t)\]</span></p>
<ul>
<li><span class="math inline">\(W\)</span> is the hidden to hidden weights that encodes information about past sequences.</li>
<li><span class="math inline">\(V\)</span> is the hidden to output weights that is responsible for prediction using current hidden units.</li>
<li><span class="math inline">\(U\)</span> is the input to hidden weights that is responsible for input parameterization.</li>
<li><span class="math inline">\(\mathbf{b}\)</span> is the bias associated with <span class="math inline">\(W\)</span></li>
<li><span class="math inline">\(\mathbf{c}\)</span> is the bias associated with <span class="math inline">\(V\)</span></li>
</ul>
<p>Then the multi-class cross entropy loss of the sequence of <span class="math inline">\(\{(\mathbf{x}^{1}, \mathbf{y}^{1}) ,...., (\mathbf{x}^{\tau}, \mathbf{y}^{\tau})\}\)</span> (<span class="math inline">\(\mathbf{y}\)</span> is encoded as one hot vector) is just the sum of per timestep loss:</p>
<span class="math display">\[\begin{aligned}
L(\{\mathbf{y}^{1}, ..., \mathbf{y}^{\tau}\}, \; \{\mathbf{\hat{y}}^{1}, ..., \mathbf{\hat{y}}^{\tau}\}) &amp;= \sum^{\tau}_{i=1} L(\mathbf{y}^i, \; \mathbf{\hat{y}}^{i})\\
&amp;= -\sum^{\tau}_{i=1} \sum_{k} y_k \log (\hat{y}^{i}_{k})
\end{aligned}\]</span>
<p>Where <span class="math inline">\(\hat{y}^{i}_{k}\)</span> is the <span class="math inline">\(k\)</span>th element of <span class="math inline">\(\mathbf{\hat{y}}^{i}\)</span>. Since we are using one-hot encoding, we can also write it as:</p>
<p><span class="math display">\[L(\{\mathbf{y}^{1}, ..., \mathbf{y}^{\tau}\}, \; \{\mathbf{\hat{y}}^{1}, ..., \mathbf{\hat{y}}^{\tau}\}) = -\sum^{\tau}_{i=1} \log (P_{\hat{Y}}(y^{i} | \{\mathbf{x}^{1}, ..., \mathbf{x}^{\tau}\}))\]</span></p>
<p>Where <span class="math inline">\(P_{\hat{Y}}(y^{i} | \{\mathbf{x}^{1}, ..., \mathbf{x}^{\tau}\})\)</span> is the value of the target position (ie. the position of <span class="math inline">\(\mathbf{y}^{i}\)</span> with 1) in <span class="math inline">\(\mathbf{\hat{y}}\)</span>.</p>
<h2 id="teacher-forcing-and-networks-with-output-recurrence">Teacher Forcing and Networks with Output Recurrence</h2>
<p>RNNs are expensive to train. The runtime is <span class="math inline">\(O(\tau)\)</span> and cannot be reduced by parallelization because the forward propagation graph is inherently sequential (i.e calculation of <span class="math inline">\(\mathbf{h}^t\)</span> depends on <span class="math inline">\(\mathbf{h}^{t-1}\)</span>, the total loss is sum of per-timestep loss and per-timestep loss has to calculate in order); each time step may only be computed after the previous one. States computed in the forward pass must be stored until they are reused during the backward pass, so the memory cost is also <span class="math inline">\(O(\tau)\)</span>.</p>
<p>To reduce computational and memory cost, one way is to remove the hidden to hidden recurrence by a target to hidden recurrence. Thus, for any loss function based on comparing the prediction at time <span class="math inline">\(t\)</span> to the training target at time <span class="math inline">\(t\)</span>, all time steps are decoupled and can be parallelized (because the loss calculation at each time step <span class="math inline">\(t\)</span> is no longer dependent on <span class="math inline">\(\mathbf{h}^{t-1}\)</span>, it only depends on <span class="math inline">\(\mathbf{y}^{t-1}\)</span>).</p>
<p>This technique is called <code>teacher forcing</code>. Teacher forcing is a procedure that emerges from the maximum likelihood criterion, in which during training the model receives the ground truth output <span class="math inline">\(\mathbf{y}^{t-1}\)</span> as input at time <span class="math inline">\(t\)</span>. <strong>It can be applied to train models that have connections from the output at one time step to the value computed in the next time step</strong>.</p>
<p>The disadvantage of strict teacher forcing arises if the network is going to be later used in an open-loop model, with the network outputs fed back as input. In this case, there is a gap between training and testing. One approach to collapse this gap is to mitigate the gap between the inputs seen at train time and the inputs seen at test time randomly chooses to use generated values or actual data values as input.</p>
<p><img src="/images/ML/rnn_4.png" width="600"></p>
<h2 id="backward-pass">Backward Pass</h2>
<p>The backward propagation for RNN is called <code>back propagation through time</code>:</p>
<p>Assume we have RNN structure as forward pass (<strong>gradients are reshaped to column vectors, <span class="math inline">\(\nabla_{\mathbf{v}} L = (\frac{\partial L}{\partial \mathbf{v}})^T\)</span></strong>). Then, for any time step <span class="math inline">\(t\)</span> <span class="math display">\[\frac{\partial L}{\partial L^t} = 1\]</span></p>
<p><span class="math display">\[\frac{\partial L}{\partial L^t} \frac{\partial L^t}{\partial \mathbf{\hat{y}}^t} \frac{\partial \mathbf{\hat{y}}^t}{o^t_i} = \hat{y}^t_i - y^t_i\]</span></p>
<p>Then the gradient for hidden units in last time step <span class="math inline">\(\tau\)</span> is:</p>
<p><span class="math display">\[(\frac{\partial L}{\partial \mathbf{o}^t} \frac{\partial \mathbf{o}^t}{\partial \mathbf{h}^t})^T = V^T \frac{\partial L}{\partial \mathbf{o}^t}\]</span></p>
<p>Work our way backwards for <span class="math inline">\(t = 1, ...., \tau - 1\)</span>, since <span class="math inline">\(\mathbf{h}^t\)</span> is descendants of both <span class="math inline">\(\mathbf{o}^t\)</span> and <span class="math inline">\(\mathbf{h}^{t+1}\)</span>. We have:</p>
<p><span class="math display">\[(\frac{\partial L}{\partial \mathbf{h}^t})^T = (\frac{\partial L}{\partial \mathbf{o}^t}\frac{\partial \mathbf{o}^t}{\partial \mathbf{h}^t})^T + (\frac{\partial L}{\partial \mathbf{h}^{t+1}}\frac{\partial \mathbf{h}^{t+1}}{\partial \mathbf{h}^t})^T\]</span></p>
<p>Since <span class="math inline">\(\frac{\partial \tanh (\mathbf{a}^{t+1})}{\partial \mathbf{a}^{t+1}}\)</span> is a diagonal matrix with <span class="math inline">\(1 - \tanh (h^{t+1}_i)^2\)</span> terms in the diagonal, we can denoted it as:</p>
<span class="math display">\[\begin{aligned}
\frac{\partial \tanh (\mathbf{a}^{t+1})}{\partial \mathbf{a}^{t+1}} &amp;= diag (1 - \tanh (\mathbf{h}^{t+1})^2)\\
\end{aligned}\]</span>
<p><span class="math display">\[\implies (\frac{\partial \mathbf{h}^{t+1}}{\partial \mathbf{h}^{t+1}})^T = W^T diag (1 - \tanh (\mathbf{h}^{t+1})^2)\]</span></p>
<p>Then:</p>
<p><span class="math display">\[(\frac{\partial L}{\partial \mathbf{h}^t})^T = V^T (\frac{\partial L}{\partial \mathbf{o}^t})^T + W^T diag (1 - \tanh (\mathbf{h}^{t+1})^2)(\frac{\partial L}{\partial \mathbf{h}^{t+1}})^T\]</span></p>
<p>Recall that in RNN, all parameters are shared, so <span class="math inline">\(\frac{\partial L}{\partial W}\)</span> represents the contribution of <span class="math inline">\(W\)</span> to the value of <span class="math inline">\(L\)</span> due to all edges in the computational graph. To resolve the ambiguity, we index <span class="math inline">\(W\)</span> by time <span class="math inline">\(W^t\)</span> where <span class="math inline">\(W^t\)</span> is copy of <span class="math inline">\(W\)</span> and only used in time step <span class="math inline">\(t\)</span>. Thus, we can use <span class="math inline">\(\frac{\partial f}{\partial W^t}\)</span> to denote the contribution of the weights to the gradient at time step t:</p>
<p><span class="math display">\[(\frac{\partial L}{\partial \mathbf{c}})^T = \sum^{\tau}_{t=1} I (\frac{\partial L}{\partial \mathbf{o}^t})^T = \sum^{\tau}_{t=1} (\frac{\partial L}{\partial \mathbf{o}^t})^T\]</span> <span class="math display">\[(\frac{\partial L}{\partial \mathbf{b}})^T = \sum^{\tau}_{t=1} diag (1 - \tanh (\mathbf{h}^{t})^2) I (\frac{\partial L}{\partial \mathbf{h}^{t}})^T = \sum^{\tau}_{t=1} diag (1 - \tanh (\mathbf{h}^{t})^2) (\frac{\partial L}{\partial \mathbf{h}^{t}})^T \]</span> <span class="math display">\[\frac{\partial L}{\partial V}= \sum^{\tau}_{t=1} (\frac{\partial L}{\partial \mathbf{o}^t})^T (\mathbf{h}^t)^T\]</span> <span class="math display">\[\frac{\partial L}{\partial W}= \sum^{\tau}_{t=1}  diag (1 - \tanh (\mathbf{h}^{t})^2) (\frac{\partial L}{\partial \mathbf{h}^{t}})^T (\mathbf{h}^{t-1})^T\]</span> <span class="math display">\[\frac{\partial L}{\partial U}= \sum^{\tau}_{t=1}  diag (1 - \tanh (\mathbf{h}^{t})^2) (\frac{\partial L}{\partial \mathbf{h}^{t}})^T (\mathbf{x}^{t})^T\]</span></p>
<h2 id="bidirectional-rnns">Bidirectional RNNs</h2>
<p>The RNN structures so far only captures information from the past (i.e output at time step <span class="math inline">\(t\)</span> is calculated using <span class="math inline">\((\mathbf{x}^1, \mathbf{y}^1) , ...., (\mathbf{x}^{t-1}, \mathbf{y}^{t-1}), \mathbf{x}^t\)</span>). It is sometimes useful to have another RNN that moves from the back of the sequence.</p>
<p><img src="/images/ML/rnn_5.png" width="600"></p>
<ul>
<li><span class="math inline">\(\mathbf{h}^t\)</span> is the hidden units of sub-RNN that moves forward through time.</li>
<li><span class="math inline">\(\mathbf{g}^t\)</span> is the hidden units of sub-RNN that moves backward through time.</li>
<li><span class="math inline">\(\mathbf{o}^t\)</span> is calculated using both <span class="math inline">\(\mathbf{h}^t\)</span>, <span class="math inline">\(\mathbf{g}^t\)</span> but is more sensitive to input values around <span class="math inline">\(t\)</span> without having to specify a fixed-size window around <span class="math inline">\(t\)</span>.</li>
</ul>
<h2 id="encoder-decoder">Encoder-Decoder</h2>
<p><img src="/images/ML/rnn_6.png" width="600"> <img src="/images/ML/rnn_7.png" width="600"></p>
<p>We know that we can use RNN to encode inputs to a fixed length vector, and we know that we can map a fixed-size vector to a sequence. Previously, we have one output corresponding to one input, the input sequence and the output sequence have same length. Using <code>encoder-decoder</code> or <code>sequence to sequence</code> structure, we can train an <code>encoder</code> RNN to process the input sequence <span class="math inline">\(\mathbf{x} = (\mathbf{x}^1, ...., \mathbf{x}^{n_x})\)</span> and emits a <code>context vector</code> <span class="math inline">\(C\)</span>, then this context vector <span class="math inline">\(C\)</span> is mapped to an output sequence <span class="math inline">\(\mathbf{y} = (\mathbf{y}^1, ....., \mathbf{y}^{n_y})\)</span> which is not necessarily of the same length using a <code>decoder</code> RNN.</p>
<p>In the sequence to sequence structure, the two RNNs are trained jointly to maximize the average of log conditional probability <span class="math inline">\(\log P_{\mathbf{Y}}(\mathbf{y}^{1} ,...., \mathbf{y}^{n_y} | \; \mathbf{x}^1 , ...., \mathbf{x}^{n_x})\)</span> or minimize the cross entropy over all sequence in the training set. The <strong>last state <span class="math inline">\(\mathbf{h}_{n_x}\)</span> of the encoder RNN</strong> is typically used as a representation <span class="math inline">\(C\)</span> of the input sequence that is provided as input to the decoder RNN.</p>
<p>One clear limitation of this architecture is when the context <span class="math inline">\(C\)</span> output by the encoder RNN has a dimension that is too small to properly summarize a long sequence. Thus, rather than a fixed length of context vector, a variable-length sequence should be used which is the result of <code>attention</code> that learns to associate elements of the sequence <span class="math inline">\(C\)</span> to elements of the output sequence.</p>
<h2 id="gated-rnn">Gated RNN</h2>
<p><code>Gated RNN</code> includes <code>long short-term memory</code> and networks based on the <code>gated recurrent unit</code>. Gated RNNs are based on the idea of creating paths through time that have derivatives that neither banish nor explode. Gated RNNs did this with connection weights that may change at each time step.</p>
<h3 id="lstm">LSTM</h3>
<p>Instead of a unit that simply applies an element wise linearity to the affine transformation of inputs and recurrent units, LSTM recurrent networks have <strong>LSTM cells</strong> that have an internal recurrence in addition to the outer recurrence of the RNN. Each cell has more parameters and a system of gating units that controls the flow of information. The most important component is the state unit <span class="math inline">\(\mathbf{c}^{(t)}_i\)</span> that is a linear function.</p>
<p>For each timestep <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[\mathbf{z} = \tanh (W * [\mathbf{x}^t, \mathbf{h}^{t-1}] + \mathbf{b})\]</span></p>
<p><span class="math display">\[\mathbf{z}^i = \sigma(W^i * [\mathbf{x}^t, \mathbf{h}^{t-1}] + \mathbf{b}_i)\]</span></p>
<p><span class="math display">\[\mathbf{z}^f = \sigma(W^f * [\mathbf{x}^t, \mathbf{h}^{t-1}] + \mathbf{b}_f)\]</span></p>
<p><span class="math display">\[\mathbf{z}^o = \sigma(W^o * [\mathbf{x}^t, \mathbf{h}^{t-1}] + \mathbf{b}_o)\]</span></p>
<p>Where <span class="math inline">\((W^i, \mathbf{b}_i), (W^f, \mathbf{b}_f), (W^o, \mathbf{b}_o)\)</span> are the parameters associated with input gate, forget gate, output gate respectively. Notice each gate is a value between <span class="math inline">\([0, 1]\)</span>. The internal states is computed as:</p>
<p><span class="math display">\[\mathbf{c}^{t} = \mathbf{z}^f \odot \mathbf{c}^{t-1} + \mathbf{z}^i \odot \mathbf{z}\]</span></p>
<p>We can see that if <span class="math inline">\(\mathbf{z}^f = 1\)</span> and <span class="math inline">\(\mathbf{z}^i = 1\)</span>, we transfer the current and past information directly to next layer without any transformation.</p>
<p>Then:</p>
<p><span class="math display">\[\mathbf{h}^t = \mathbf{z}^o \odot \tanh (\mathbf{c}^t)\]</span></p>
<p><span class="math display">\[\mathbf{y}^t = \sigma (V\mathbf{h}^t + \mathbf{b}_y)\]</span></p>
<p><img src="/images/ML/lstm_1.png" width="600"> <img src="/images/ML/lstm_2.png" width="600"></p>
<p>In LSTMs, you have the state <span class="math inline">\(\mathbf{c}^t\)</span>. The derivative there is of the form:</p>
<p><span class="math display">\[\frac{\partial \mathbf{c}^{t^{\prime}}}{\partial \mathbf{c}^{t}} = \prod^{t^{\prime} - t}_{k=1} \sigma (\mathbf{v}_{t+k})\]</span></p>
<p>Where <span class="math inline">\(\mathbf{v}_{t+k}\)</span> is the input to the forget gate. We can see that compare with the <span class="math inline">\(\frac{\partial \mathbf{h}^{t^{\prime}}}{\partial \mathbf{h}^{t}}\)</span> in RNN, we do not have <span class="math inline">\(W^T\)</span> term. Thus, we have somewhat solved the vanishing gradient problem (at least one path the gradient is not vanishing). However, we still have vanishing gradient problem in LSTM, but not as much as RNN (no gradient of sigmoid or multiplication of <span class="math inline">\(W\)</span>).</p>
<h3 id="gru">GRU</h3>
<p>The main difference with the LSTM is that a single gating unit simultaneously controls the forgetting factor and the decision to update the state unit so we have less parameters.</p>
<p>The input and output to GRU is same as RNN</p>
<p><img src="/images/ML/gru_1.png" width="600"></p>
<p>GRU has two gates at each timestep <span class="math inline">\(t\)</span>:</p>
<ul>
<li><p><strong>Reset gate</strong> <span class="math inline">\(\mathbf{r}^{t}\)</span>: <span class="math display">\[\mathbf{r}^{t} = \sigma(W^r * [\mathbf{x}^t, \mathbf{h}^{t-1}] + \mathbf{b}_r)\]</span></p></li>
<li><p><strong>Update gate</strong> <span class="math inline">\(\mathbf{u}^t\)</span>: <span class="math display">\[\mathbf{u}^t = \sigma(W^u * [\mathbf{x}^t, \mathbf{h}^{t-1}] + \mathbf{b}_u)\]</span></p></li>
</ul>
<p>The hidden unit update equation is:</p>
<p><span class="math display">\[\mathbf{h}^{t-1, \prime} = h^{t-1} \odot \mathbf{r}\]</span></p>
<p><span class="math display">\[\mathbf{h}^{t, \prime} = \tanh(W * [\mathbf{h}^{t-1, \prime}, \mathbf{x}_t] + \mathbf{b})\]</span></p>
<p><span class="math display">\[\mathbf{h}^{t} = \mathbf{u}^t \odot \mathbf{h}^{t-1} + (1 - \mathbf{u}^t) \odot \mathbf{h}^{t, \prime}\]</span></p>
<p>Where <span class="math inline">\(\mathbf{h}^{t, \prime}\)</span> is the candidate at timestep <span class="math inline">\(t\)</span> that has some new information <span class="math inline">\(\mathbf{x}^t\)</span> and old information <span class="math inline">\(\mathbf{h}^{t-1, \prime}\)</span>. When <span class="math inline">\(\mathbf{u}_t = 1\)</span> we simply keep the old information and the new information <span class="math inline">\(\mathbf{x}_t\)</span> is ignored. In contrast, whenever <span class="math inline">\(Z_t\)</span> is close to 0, the candidate is used inplace.</p>
<p>In summary:</p>
<ul>
<li>Reset gates help capture short-term dependencies in sequences (by integrating part of past information with new information to produce the candidate).</li>
<li>Update gates help capture long-term dependencies in sequences.</li>
</ul>
<p><img src="/images/ML/gru_2.png" width="600"></p>
<h1 id="ref">Ref</h1>
<p>https://zhuanlan.zhihu.com/p/32085405</p>
<p>https://zhuanlan.zhihu.com/p/32481747</p>
<p>https://stats.stackexchange.com/questions/185639/how-does-lstm-prevent-the-vanishing-gradient-problem</p>
<p>https://d2l.ai/chapter_recurrent-modern/gru.html</p>

    </div>

    
    
    
      
  <div class="popular-posts-header">Related Posts</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/06/24/VGG/" rel="bookmark">VGG (Under Construction)</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/07/29/adaptive-lr/" rel="bookmark">Basic Adaptive LR Algorithms</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/06/22/alex-net/" rel="bookmark">Alex Net</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/08/06/attention/" rel="bookmark">Attention</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/05/26/batch-norm/" rel="bookmark">Batch Normalization</a></div>
    </li>
  </ul>


    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/07/29/adam/" rel="prev" title="Adam">
                  <i class="fa fa-chevron-left"></i> Adam
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/08/06/attention/" rel="next" title="Attention">
                  Attention <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div><script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhu, Zhaoyang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Symbols count total">762k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">11:33</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.3" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{&quot;enable&quot;:true,&quot;app_id&quot;:&quot;gPOc95wWW2Hwp2pkVSAAz28m-MdYXbMMI&quot;,&quot;app_key&quot;:&quot;7Sf65xCHXfEEdBvu29UHSYdV&quot;,&quot;server_url&quot;:&quot;https:&#x2F;&#x2F;swag1ong.github.io&quot;,&quot;security&quot;:true}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{&quot;enable&quot;:true,&quot;repo&quot;:&quot;swag1ong&#x2F;swag1ong.github.io&quot;,&quot;issue_term&quot;:&quot;pathname&quot;,&quot;theme&quot;:&quot;github-light&quot;,&quot;pathname&quot;:&quot;pathname&quot;}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
