<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="1xbHmVEsS5Fta14x1d1FXkHFr93LRr1pMxipdYVcNI4">
  <meta name="baidu-site-verification" content="code-Srzz2vTuzA">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;swag1ong.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;Searching...&quot;,&quot;empty&quot;:&quot;We didn&#39;t find any results for the search: ${query}&quot;,&quot;hits_time&quot;:&quot;${hits} results found in ${time} ms&quot;,&quot;hits&quot;:&quot;${hits} results found&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:3,&quot;unescape&quot;:false,&quot;preload&quot;:true}}</script>
<meta name="description" content="Proximal Policy Optimization Algorithms Notations and Backgrounds Policy gradient estimator \[\hat{g} &#x3D; \hat{E}_t [\nabla_{\theta} \log \pi_{\theta} (a_t | s_t) \hat{A_t}]\] Where, \(\pi_{\theta}\) is">
<meta property="og:type" content="article">
<meta property="og:title" content="PPO">
<meta property="og:url" content="https://swag1ong.github.io/2021/06/11/PPO/index.html">
<meta property="og:site_name" content="GoGoGogo!">
<meta property="og:description" content="Proximal Policy Optimization Algorithms Notations and Backgrounds Policy gradient estimator \[\hat{g} &#x3D; \hat{E}_t [\nabla_{\theta} \log \pi_{\theta} (a_t | s_t) \hat{A_t}]\] Where, \(\pi_{\theta}\) is">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://swag1ong.github.io/images/RL/pg/ppo_1.png">
<meta property="og:image" content="https://swag1ong.github.io/images/RL/pg/ppo_2.png">
<meta property="og:image" content="https://swag1ong.github.io/images/RL/pg/ppo_3.png">
<meta property="article:published_time" content="2021-06-11T06:56:13.000Z">
<meta property="article:modified_time" content="2021-06-18T09:16:55.000Z">
<meta property="article:author" content="Zhu, Zhaoyang">
<meta property="article:tag" content="Policy Gradient">
<meta property="article:tag" content="Policy Search">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://swag1ong.github.io/images/RL/pg/ppo_1.png">


<link rel="canonical" href="https://swag1ong.github.io/2021/06/11/PPO/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;en&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;swag1ong.github.io&#x2F;2021&#x2F;06&#x2F;11&#x2F;PPO&#x2F;&quot;,&quot;path&quot;:&quot;2021&#x2F;06&#x2F;11&#x2F;PPO&#x2F;&quot;,&quot;title&quot;:&quot;PPO&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>PPO | GoGoGogo!</title><script src="/js/config.js"></script>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?852b005027c5efa81663f6f5c4c5b7fd"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">GoGoGogo!</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-igloo fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">107</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">16</span></a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">25</span></a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-address-card fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#proximal-policy-optimization-algorithms"><span class="nav-number">1.</span> <span class="nav-text">Proximal Policy Optimization Algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#notations-and-backgrounds"><span class="nav-number">1.1.</span> <span class="nav-text">Notations and Backgrounds</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#policy-gradient-estimator"><span class="nav-number">1.1.1.</span> <span class="nav-text">Policy gradient estimator</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#trust-region-methods"><span class="nav-number">1.1.2.</span> <span class="nav-text">Trust Region Methods</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#clipped-surrogate-objective"><span class="nav-number">1.2.</span> <span class="nav-text">Clipped Surrogate Objective</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adaptive-kl-penalty-coefficient"><span class="nav-number">1.3.</span> <span class="nav-text">Adaptive KL Penalty Coefficient</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#algorithm"><span class="nav-number">1.4.</span> <span class="nav-text">Algorithm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#implementation"><span class="nav-number">1.5.</span> <span class="nav-text">Implementation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reference"><span class="nav-number">1.6.</span> <span class="nav-text">Reference</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhu, Zhaoyang"
      src="/images/others/favicon.jpeg">
  <p class="site-author-name" itemprop="name">Zhu, Zhaoyang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">107</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/swag1ong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;swag1ong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyang.zhu@mail.utoronto.ca" title="E-Mail → mailto:zhaoyang.zhu@mail.utoronto.ca" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zhuzhaoyangzzyu0616" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zhuzhaoyangzzyu0616" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/06/11/PPO/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PPO
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-06-11 14:56:13" itemprop="dateCreated datePublished" datetime="2021-06-11T14:56:13+08:00">2021-06-11</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-06-18 17:16:55" itemprop="dateModified" datetime="2021-06-18T17:16:55+08:00">2021-06-18</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/RL/" itemprop="url" rel="index"><span itemprop="name">RL</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/RL/PG/" itemprop="url" rel="index"><span itemprop="name">PG</span></a>
        </span>
    </span>

  
    <span id="/2021/06/11/PPO/" class="post-meta-item leancloud_visitors" data-flag-title="PPO" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>9.1k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>8 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="proximal-policy-optimization-algorithms">Proximal Policy Optimization Algorithms</h1>
<h2 id="notations-and-backgrounds">Notations and Backgrounds</h2>
<h3 id="policy-gradient-estimator">Policy gradient estimator</h3>
<p><span class="math display">\[\hat{g} = \hat{E}_t [\nabla_{\theta} \log \pi_{\theta} (a_t | s_t) \hat{A_t}]\]</span></p>
<p>Where, <span class="math inline">\(\pi_{\theta}\)</span> is the stochastic policy and <span class="math inline">\(\hat{A_t}\)</span> is an estimate of advantage function at timestep <span class="math inline">\(t\)</span>. The expectation here <span class="math inline">\(\hat{E}_t\)</span> indicates the empirical average over a finite batch of samples, in an algorithm that alternates between sampling and optimization.</p>
<p><span class="math inline">\(\hat{g}\)</span> is obtained by differentiating the objective</p>
<p><span class="math display">\[L^{PG} (\theta) = \hat{E}_t [\log \pi_{\theta} (a_t | s_t) \hat{A_t}]\]</span></p>
<p>While it is appealing to perform multiple steps of optimization on this loss <span class="math inline">\(L^{PG} (\theta)\)</span> (reuse the same samples to update <span class="math inline">\(\theta\)</span>), using the same trajectory, doing so is not well-justi ed, and empirically it often leads to destructively large policy updates.</p>
<h3 id="trust-region-methods">Trust Region Methods</h3>
<p>The optimization problem is defined as:</p>
<span class="math display">\[\begin{aligned}
\max_{\theta} \quad &amp; \hat{E}_{s \sim \rho_{\theta_{old}} (\cdot), \; a \sim \pi_{\theta_{old}}(\cdot | s)} [\frac{\pi_{\theta} (a | s)}{\pi_{\theta_{old}} (a | s)} \hat{A}_{\theta_{old}} (s, a)]\\
\textrm{s.t.} \quad &amp; \hat{E}_{s \sim \rho_{\theta_{old}} (\cdot)}[D_{KL} (\pi_{\theta_{old}} (\cdot | s) || \pi_{\theta} (\cdot | s))] \leq \delta
\end{aligned}\]</span>
<p></br></p>
<p>It is also possible to remove the hard constraint and convert the problem to an unconstrained objective:</p>
<span class="math display">\[\begin{aligned}
\max_{\theta} \quad &amp; \hat{E}_{s \sim \rho_{\theta_{old}} (\cdot), \; a \sim \pi_{\theta_{old}}(\cdot | s)} [\frac{\pi_{\theta} (a | s)}{\pi_{\theta_{old}} (a | s)} \hat{A}_{\theta_{old}} (s, a) - \beta D_{KL}[\pi_{\theta_{old}} (\cdot | s) || \pi_{\theta} (\cdot | s))]]\\
\end{aligned}\]</span>
<p></br></p>
<p>The problem with this objective is that, the penalty prefers small steps, so it is very hard to choose an appropriate <span class="math inline">\(\beta\)</span> that performs well across different problems or even within a single problem where the characteristics change over the course of learning.</p>
<span id="more"></span>
<h2 id="clipped-surrogate-objective">Clipped Surrogate Objective</h2>
<p>Let <span class="math inline">\(r_t (\theta)\)</span> denotes the probability ratio <span class="math inline">\(r_t (\theta) = \frac{\pi_{\theta} (a_t | s_t)}{\pi_{\theta_{old}} (a_t | s_t)}\)</span>, so <span class="math inline">\(r (\theta_{old}) = 1\)</span>. The above unconstrained TRPO objective without penalty becomes:</p>
<p><span class="math display">\[L^{CPI} (\theta) = \hat{E}_{s \sim \rho_{\theta_{old}} (\cdot), \; a \sim \pi_{\theta_{old}} (\cdot | s)} [r_{t} (\theta) \hat{A}_{\theta_{old}} (s, a)]\]</span></p>
<p>Without the penalty, the step size can be super big maximizing <span class="math inline">\(L^{CPI}\)</span>, thus, we can add penalty to the changes to the policy that move <span class="math inline">\(r_t (\theta)\)</span> away from 1 (make big updates or take big step)</p>
<p>The main objective is then:</p>
<p><span class="math display">\[L^{CLIP} (\theta) = \hat{E}_{s \sim \rho_{\theta_{old}} (\cdot), \; a \sim \pi_{\theta_{old}} (\cdot | s)} [\min (r_{t} (\theta) \hat{A}_{\theta_{old}} (s, a), \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon)\hat{A}_{\theta_{old}} (s, a))]\]</span> </br></p>
<p>Notice here, the clipped objective gives a lower bound of the unclipped objective, to see this:</p>
<ol type="1">
<li>If <span class="math inline">\(A_t &lt; 0\)</span>, then one way to maximize the objective is to make <span class="math inline">\(r_t\)</span> very small, then we will have a really large step size, However, with clipped objective, the largest change is clipped to <span class="math inline">\(1 - \epsilon\)</span></li>
<li>If <span class="math inline">\(A_t &gt; 0\)</span>, then one way to maximize the objective is to make <span class="math inline">\(r_t\)</span> very large, then we will have a really large step size. However, with clipped objective, the largest change is clipped to <span class="math inline">\(1 + \epsilon\)</span></li>
</ol>
<p>At the same time, the minimum gives the above behavior to account for <span class="math inline">\(A_t &gt; 0, A_t &lt; 0\)</span> situation.</p>
<p><img src="/images/RL/pg/ppo_1.png" width="600"></p>
<p><img src="/images/RL/pg/ppo_2.png" width="600"></p>
<h2 id="adaptive-kl-penalty-coefficient">Adaptive KL Penalty Coefficient</h2>
<p>Another approach, which can be used as an alternative to the clipped surrogate objective, or in additional to it is to use a penalty on KL divergence <span class="math inline">\(d_{targ}\)</span> each policy update. In practice, KL performs worse than the clipped surrogate objective.</p>
<p>In the simplest instantiation of this algorithm, the following steps are used in each policy update:</p>
<ul>
<li><p>Using several epochs of mini-batch SGD, optimizing the KL-penalized objective:</p>
<p><span class="math display">\[L^{KLPEN} (\theta) = \hat{E}_t [\frac{\pi_\theta (a_t | s_t)}{\pi_{\theta_{old}} (a_t | s_t)} \hat{A}_t - \beta_t D_{KL} (\pi_{\theta_{old}} (\cdot | s) || \pi_{\theta} (\cdot | s))]\]</span></p></li>
<li><p>Compute and update <span class="math inline">\(\theta_{t+1}\)</span>:</p>
<p><span class="math display">\[d = \hat{E}_t [\beta_t D_{KL} (\pi_{\theta_{old}} (\cdot | s) || \pi_{\theta} (\cdot | s))]\]</span></p>
<ul>
<li>If <span class="math inline">\(d &lt; d_{targ} \;/\; 1.5, \quad \beta_{t + 1} \leftarrow \beta_{t} \;/\; 2\)</span></li>
<li>If <span class="math inline">\(d &gt; d_{targ} * 1.5, \quad \beta_{t + 1} \leftarrow \beta_{t} * 2\)</span></li>
</ul></li>
</ul>
<p><span class="math inline">\(\beta_{t + 1}\)</span> is used in the next policy update. The algorithm is not very sensitive to the choice of <span class="math inline">\(2\)</span> and <span class="math inline">\(1.5\)</span> above. The initial value of <span class="math inline">\(\beta\)</span> is another hyperparameter but is not important in practice.</p>
<h2 id="algorithm">Algorithm</h2>
<p>The advantage function can be estimated using a separate value function <span class="math inline">\(V(x)\)</span>, or a shared parameter value and policy network can be used. If the parameters are shared between policy network and value network, we need to modify the objective by adding the value loss negative value function loss <span class="math inline">\(- L^{VF} (\theta)\)</span> and maybe an entropy term to promote exploration:</p>
<p><span class="math display">\[L^{CLIP + VF + S} (\theta) = \hat{E} [L_t^{CLIP} (\theta) - c_1 L_t^{VF} (\theta) + c_2 S[\pi_{\theta}] (s_t)]\]</span> </br></p>
<p>Where, <span class="math inline">\(c_1, c_2\)</span> are two coefficients, <span class="math inline">\(S[\pi_{\theta}] (s_t)\)</span> is the entropy bonus on <span class="math inline">\(s_t\)</span> and <span class="math inline">\(L_t^{VF} = \|V_{\theta} - V_t^{targ}\|^2\)</span> is the square loss.</p>
<p>One style of advantage estimate is n-step TD estimate:</p>
<p><span class="math display">\[\hat{A}_t = r_t + \gamma r_{t+1} + .... + \gamma^{T-t+1} r_{T-1} + \gamma^{T-t} V(s_T) - V(s_t)\]</span></p>
<p>Where <span class="math inline">\(t\)</span> specifies the time index in <span class="math inline">\([0, T]\)</span>, within a given length-<span class="math inline">\(T\)</span> trajectory segment. Using this idea, we can use a truncated version of generalized advantage estimation:</p>
<p><span class="math display">\[\hat{A}_t = \delta_t + \gamma \lambda \delta_{t+1} + .... + \lambda \gamma^{T-t+1}\delta_{T-1}\]</span></p>
<p><span class="math display">\[\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)\]</span> </br></p>
<p><img src="/images/RL/pg/ppo_3.png" width="600"> </br></p>
<p>A PPO algorithm that uses fixed-length trajectory segments is shown above. Each iteration, each <span class="math inline">\(N\)</span> parallel actors collect <span class="math inline">\(T\)</span> timesteps of data. Then we construct the surrogate loss on these <span class="math inline">\(NT\)</span> timesteps of data and optimize it with mini-batch SGD for <span class="math inline">\(K\)</span> epochs.</p>
<h2 id="implementation">Implementation</h2>
<p>One implementation from parl</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#   Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment"># you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment"># You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> parl</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> paddle.optimizer <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> paddle.distribution <span class="keyword">import</span> Normal</span><br><span class="line"></span><br><span class="line">__all__ = [<span class="string">&#x27;PPO&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PPO</span>(<span class="params">parl.Algorithm</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 model,</span></span></span><br><span class="line"><span class="function"><span class="params">                 clip_param,</span></span></span><br><span class="line"><span class="function"><span class="params">                 value_loss_coef,</span></span></span><br><span class="line"><span class="function"><span class="params">                 entropy_coef,</span></span></span><br><span class="line"><span class="function"><span class="params">                 initial_lr,</span></span></span><br><span class="line"><span class="function"><span class="params">                 eps=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_grad_norm=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 use_clipped_value_loss=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; PPO algorithm</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            model (parl.Model): model that contains both value network and policy network</span></span><br><span class="line"><span class="string">            clip_param (float): the clipping strength for value loss clipping</span></span><br><span class="line"><span class="string">            value_loss_coef (float): the coefficient for value loss (c_1)</span></span><br><span class="line"><span class="string">            entropy_coef (float): the coefficient for entropy (c_2)</span></span><br><span class="line"><span class="string">            initial_lr (float): initial learning rate.</span></span><br><span class="line"><span class="string">            eps (None or float): epsilon for Adam optimizer</span></span><br><span class="line"><span class="string">            max_grad_norm (float): threshold for grad norm clipping</span></span><br><span class="line"><span class="string">            use_clipped_value_loss (bool): whether use value loss clipping</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(model)</span><br><span class="line">        self.clip_param = clip_param</span><br><span class="line"></span><br><span class="line">        self.value_loss_coef = value_loss_coef</span><br><span class="line">        self.entropy_coef = entropy_coef</span><br><span class="line">        self.use_clipped_value_loss = use_clipped_value_loss</span><br><span class="line">        clip = nn.ClipGradByNorm(max_grad_norm)</span><br><span class="line"></span><br><span class="line">        self.optimizer = optim.Adam(parameters=model.parameters(),</span><br><span class="line">                                    learning_rate=initial_lr,</span><br><span class="line">                                    epsilon=eps,</span><br><span class="line">                                    grad_clip=clip)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, obs_batch, actions_batch, value_preds_batch, return_batch,</span></span></span><br><span class="line"><span class="function"><span class="params">              old_action_log_probs_batch, adv_targ</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; update the value network and policy network parameters.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        values = self.model.value(obs_batch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># log std so the std is always positive after e^&#123;log_std&#125;</span></span><br><span class="line">        mean, log_std = self.model.policy(obs_batch)</span><br><span class="line">        dist = Normal(mean, log_std.exp())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Continuous actions are usually considered to be independent,</span></span><br><span class="line">        <span class="comment"># so we can sum components of the ``log_prob`` or the entropy.</span></span><br><span class="line">        action_log_probs = dist.log_prob(actions_batch).<span class="built_in">sum</span>(axis=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        dist_entropy = dist.entropy().<span class="built_in">sum</span>(axis=-<span class="number">1</span>).mean()</span><br><span class="line"></span><br><span class="line">        ratio = paddle.exp(action_log_probs - old_action_log_probs_batch)</span><br><span class="line">        surr1 = ratio * adv_targ</span><br><span class="line">        surr2 = paddle.clip(ratio, <span class="number">1.0</span> - self.clip_param, <span class="number">1.0</span> + self.clip_param) * adv_targ</span><br><span class="line">        action_loss = -paddle.minimum(surr1, surr2).mean()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># calculate value loss using semi gradient TD</span></span><br><span class="line">        <span class="keyword">if</span> self.use_clipped_value_loss:</span><br><span class="line">            value_pred_clipped = value_preds_batch + \</span><br><span class="line">                (values - value_preds_batch).clip(-self.clip_param, self.clip_param)</span><br><span class="line">            value_losses = (values - return_batch).<span class="built_in">pow</span>(<span class="number">2</span>)</span><br><span class="line">            value_losses_clipped = (value_pred_clipped - return_batch).<span class="built_in">pow</span>(<span class="number">2</span>)</span><br><span class="line">            value_loss = <span class="number">0.5</span> * paddle.maximum(value_losses, value_losses_clipped).mean()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            value_loss = <span class="number">0.5</span> * (return_batch - values).<span class="built_in">pow</span>(<span class="number">2</span>).mean()</span><br><span class="line"></span><br><span class="line">        (value_loss * self.value_loss_coef + action_loss - dist_entropy * self.entropy_coef).backward()</span><br><span class="line">        self.optimizer.step()</span><br><span class="line">        self.optimizer.clear_grad()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> value_loss.numpy(), action_loss.numpy(), dist_entropy.numpy()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, obs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Sample action from parameterized policy</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        value = self.model.value(obs)</span><br><span class="line">        mean, log_std = self.model.policy(obs)</span><br><span class="line">        dist = Normal(mean, log_std.exp())</span><br><span class="line">        action = dist.sample([<span class="number">1</span>])</span><br><span class="line">        action_log_probs = dist.log_prob(action).<span class="built_in">sum</span>(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> value, action, action_log_probs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, obs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Predict action from parameterized policy, action with maximum probability is selected as greedy action</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        mean, _ = self.model.policy(obs)</span><br><span class="line">        <span class="keyword">return</span> mean</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">value</span>(<span class="params">self, obs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Predict value from parameterized value function</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.model.value(obs)</span><br></pre></td></tr></table></figure>
<h2 id="reference">Reference</h2>

    </div>

    
    
    
      
  <div class="popular-posts-header">Related Posts</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/06/09/DDPG/" rel="bookmark">Deep Deterministic Policy Gradient</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/05/06/DPG/" rel="bookmark">Deterministic Policy Gradient</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/05/30/policy-gradient-2/" rel="bookmark">Policy Gradient (2)</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/05/23/policy-gradient/" rel="bookmark">Policy Gradient (1)</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/06/05/policy-gradient-3/" rel="bookmark">Policy Gradient (3)</a></div>
    </li>
  </ul>


    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Policy-Gradient/" rel="tag"># Policy Gradient</a>
              <a href="/tags/Policy-Search/" rel="tag"># Policy Search</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/06/11/TRPO/" rel="prev" title="TRPO">
                  <i class="fa fa-chevron-left"></i> TRPO
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/06/12/vfa-1/" rel="next" title="Value Function Approximation (1)">
                  Value Function Approximation (1) <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div><script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhu, Zhaoyang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Symbols count total">872k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">13:12</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.3" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{&quot;enable&quot;:true,&quot;app_id&quot;:&quot;gPOc95wWW2Hwp2pkVSAAz28m-MdYXbMMI&quot;,&quot;app_key&quot;:&quot;7Sf65xCHXfEEdBvu29UHSYdV&quot;,&quot;server_url&quot;:&quot;https:&#x2F;&#x2F;swag1ong.github.io&quot;,&quot;security&quot;:true}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{&quot;enable&quot;:true,&quot;repo&quot;:&quot;swag1ong&#x2F;swag1ong.github.io&quot;,&quot;issue_term&quot;:&quot;pathname&quot;,&quot;theme&quot;:&quot;github-light&quot;,&quot;pathname&quot;:&quot;pathname&quot;}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
