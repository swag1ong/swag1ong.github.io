<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="1xbHmVEsS5Fta14x1d1FXkHFr93LRr1pMxipdYVcNI4">
  <meta name="baidu-site-verification" content="code-Srzz2vTuzA">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;swag1ong.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;Searching...&quot;,&quot;empty&quot;:&quot;We didn&#39;t find any results for the search: ${query}&quot;,&quot;hits_time&quot;:&quot;${hits} results found in ${time} ms&quot;,&quot;hits&quot;:&quot;${hits} results found&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:3,&quot;unescape&quot;:false,&quot;preload&quot;:true}}</script>
<meta name="description" content="Continuous Control with Deep Reinforcement Learning Notations A trajectory is defined as \((s_1, a_1, r_1, ...)\) The initial state distribution: \(\rho (s_1)\) Transition dynamics \(p(s_{t+1} | s_t,">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Deterministic Policy Gradient">
<meta property="og:url" content="https://swag1ong.github.io/2021/06/09/DDPG/index.html">
<meta property="og:site_name" content="GoGoGogo!">
<meta property="og:description" content="Continuous Control with Deep Reinforcement Learning Notations A trajectory is defined as \((s_1, a_1, r_1, ...)\) The initial state distribution: \(\rho (s_1)\) Transition dynamics \(p(s_{t+1} | s_t,">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://swag1ong.github.io/images/RL/pg/ddpg_1">
<meta property="article:published_time" content="2021-06-09T03:13:00.000Z">
<meta property="article:modified_time" content="2021-07-15T06:11:23.000Z">
<meta property="article:author" content="Zhu, Zhaoyang">
<meta property="article:tag" content="Policy Gradient">
<meta property="article:tag" content="Continuous Action Space">
<meta property="article:tag" content="Policy Search">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://swag1ong.github.io/images/RL/pg/ddpg_1">


<link rel="canonical" href="https://swag1ong.github.io/2021/06/09/DDPG/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;en&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;swag1ong.github.io&#x2F;2021&#x2F;06&#x2F;09&#x2F;DDPG&#x2F;&quot;,&quot;path&quot;:&quot;2021&#x2F;06&#x2F;09&#x2F;DDPG&#x2F;&quot;,&quot;title&quot;:&quot;Deep Deterministic Policy Gradient&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Deep Deterministic Policy Gradient | GoGoGogo!</title><script src="/js/config.js"></script>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?852b005027c5efa81663f6f5c4c5b7fd"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">GoGoGogo!</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-igloo fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">97</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">16</span></a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">25</span></a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-address-card fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#continuous-control-with-deep-reinforcement-learning"><span class="nav-number">1.</span> <span class="nav-text">Continuous Control with Deep Reinforcement Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#notations"><span class="nav-number">1.1.</span> <span class="nav-text">Notations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#algorithm"><span class="nav-number">1.2.</span> <span class="nav-text">Algorithm</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhu, Zhaoyang"
      src="/images/others/favicon.jpeg">
  <p class="site-author-name" itemprop="name">Zhu, Zhaoyang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">97</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/swag1ong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;swag1ong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyang.zhu@mail.utoronto.ca" title="E-Mail → mailto:zhaoyang.zhu@mail.utoronto.ca" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zhuzhaoyangzzyu0616" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zhuzhaoyangzzyu0616" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/06/09/DDPG/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Deep Deterministic Policy Gradient
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-06-09 11:13:00" itemprop="dateCreated datePublished" datetime="2021-06-09T11:13:00+08:00">2021-06-09</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-15 14:11:23" itemprop="dateModified" datetime="2021-07-15T14:11:23+08:00">2021-07-15</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/RL/" itemprop="url" rel="index"><span itemprop="name">RL</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/RL/PG/" itemprop="url" rel="index"><span itemprop="name">PG</span></a>
        </span>
    </span>

  
    <span id="/2021/06/09/DDPG/" class="post-meta-item leancloud_visitors" data-flag-title="Deep Deterministic Policy Gradient" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>5.3k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>5 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="continuous-control-with-deep-reinforcement-learning">Continuous Control with Deep Reinforcement Learning</h1>
<h2 id="notations">Notations</h2>
<p>A trajectory is defined as <span class="math inline">\((s_1, a_1, r_1, ...)\)</span></p>
<p>The initial state distribution: <span class="math inline">\(\rho (s_1)\)</span></p>
<p>Transition dynamics <span class="math inline">\(p(s_{t+1} | s_t, a_t)\)</span></p>
<p>Reward function <span class="math inline">\(r(s_t, a_t)\)</span></p>
<p>The return from a state is defined as the sum of discounted future reward:</p>
<p><span class="math display">\[R_t = \sum_{i=t}^{T} \gamma^{i - t} r(s_i, a_i)\]</span></p>
<p>The discounted state distribution following <span class="math inline">\(\pi\)</span> is denoted as <span class="math inline">\(\rho^{\pi}\)</span></p>
<p>The action-value function:</p>
<p><span class="math display">\[Q^{\pi} (s_t, a_t) = E_{r_{i \geq t}, s_{i &gt; t} \sim P, a_{i &gt;t} \sim \pi} [R_t | s_t, a_t]\]</span></p>
<p>And the action-value function can be expanded using bellman equation:</p>
<p><span class="math display">\[Q^{\pi} (s_t, a_t) = E_{r_{t}, s_{t+1} \sim P} [r(s_t, a_t) + \gamma E_{a_{t+1} \sim \pi (\cdot | s_{t+1})}[Q^{\pi} (s_{t+1}, a_{t+1})] | s_t, a_t]\]</span></p>
<p>If we have deterministic policy <span class="math inline">\(\mu: S \rightarrow A\)</span>, we can remove the integral over next actions and have:</p>
<p><span class="math display">\[Q^{\mu} (s_t, a_t) = E_{r_{t}, s_{t+1} \sim P} [r(s_t, a_t) + \gamma Q^{\mu} (s_{t+1}, \mu(s_{t + 1})) | s_t, a_t]\]</span></p>
<p>This implies that we can learn <span class="math inline">\(Q^{\mu}\)</span> off policy using state samples from another behavior policy <span class="math inline">\(\beta\)</span>.</p>
<p>The off policy loss for parameterized function approximators <span class="math inline">\(\theta^Q\)</span> Bellman Residual Minimization is:</p>
<p><span class="math display">\[L(\theta^{Q}) = E_{s_t \sim \rho^\beta. a_t \sim \beta, r_t} [(Q(s_t, a_t | \theta^{Q}) - y_t)^2]\]</span></p>
<p>where:</p>
<p><span class="math display">\[y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, \mu(s_{t+1}) | \theta^{Q})\]</span></p>
<p>The use of large, non-linear function approximators for learning value or action-value functions has often been avoided in the past since theoretical performance guarantees are impossible, and practically learning tends to be unstable.</p>
<p>The off policy deterministic policy gradient:</p>
<p><span class="math display">\[\nabla_{\theta^{\mu}} J \approx E_{s_{t} \sim \rho^{\beta}} [\nabla_{a} Q(s, a | \theta^Q) |_{s=s_t, a=\mu(s_t)}\nabla_{\theta_{\mu}} \mu(s | \theta^\mu) |_{s=s_t}]\]</span></p>
<p>As with Q learning, introducing non-linear function approximators means that convergence is no longer guaranteed for off-policy deterministic policy gradient.</p>
<span id="more"></span>
<h2 id="algorithm">Algorithm</h2>
<p><img src="/images/RL/pg/ddpg_1" width="600"></p>
<ol type="1">
<li>Initialize:
<ol type="a">
<li>the weights <span class="math inline">\(\theta^Q\)</span> for critic network <span class="math inline">\(Q\)</span> and <span class="math inline">\(\theta^\mu\)</span> for actor network <span class="math inline">\(\mu\)</span></li>
<li>the weights <span class="math inline">\(\theta^{Q^{\prime}} \leftarrow \theta^{Q}\)</span> for target critic <span class="math inline">\(Q^{\prime}\)</span> and <span class="math inline">\(\theta^{\mu^{\prime}} \leftarrow \theta^{\mu}\)</span> for target actor <span class="math inline">\(\mu^{\prime}\)</span></li>
<li>the replay buffer <span class="math inline">\(R\)</span></li>
</ol></li>
<li>For each episode:
<ol type="a">
<li><p>Initialize a random process <span class="math inline">\(\mathcal{N}_1, ....,\)</span> for exploration</p></li>
<li><p>Observe initial state <span class="math inline">\(s_1 \sim \rho^{\mu^{\beta}} \;\;\;\;\;\;\;\;\;\;\;\)</span> (remember we are learning off-policy)</p></li>
<li><p>For each timestep <span class="math inline">\(t = 1, ..., T\)</span> in the episode:</p>
<p><span style="color:red"><em>Data generation</em></span></p>
<ul>
<li><span class="math inline">\(a_t = \mu^{\beta} (s_t) = \mu (s_t | \theta^{\mu}) + \mathcal{N}_t \;\;\;\;\;\;\;\;\;\;\;\)</span> (actions is selected with some noise)</li>
<li>Observe <span class="math inline">\(s_{t+1}, r_t\)</span> by following <span class="math inline">\(a_t\)</span></li>
<li>Store <span class="math inline">\((s_t, a_t, r_t, s_{t+1})\)</span> in <span class="math inline">\(R \;\;\;\;\;\;\;\;\;\;\;\)</span> (for batch learning)</li>
</ul>
<p><span style="color:red"><em>Now we evaluate <span class="math inline">\(Q^{\mu_{\theta}}\)</span> and update <span class="math inline">\(\theta^{\mu}, \theta^{Q}\)</span></em></span></p>
<ul>
<li><p>Sample a random minibatch of <span class="math inline">\(N\)</span> transitions <span class="math inline">\((s_i, a_i, r_i, s_{i+1}) \;\;\;\;\;\;\;\;\;\;\;\)</span> (for batch learning)</p></li>
<li><p>Calculate <span class="math inline">\(y_i = r_i + \gamma Q^{\prime} (s_{i+1}, \mu^{\prime} (s_{i+1}))\;\;\;\;\;\;\;\;\;\;\;\)</span> (<span class="math inline">\(\hat{T}^{\mu^{\prime}} Q^{\prime}\)</span>)</p></li>
<li><p>Update critic by minimizing the loss <span class="math inline">\(L = \frac{1}{N} \sum_{i} (Q(s_i, a_i) - y_i)^2 \;\;\;\;\;\;\;\;\;\;\;\)</span> (update for several times, to have a better estimate of <span class="math inline">\(\hat{T}^{\mu^{\prime}} Q^{\prime}\)</span>)</p></li>
<li><p>Update the actor policy using the sampled policy gradient:</p>
<p><span class="math display">\[\frac{1}{N} \sum_i \nabla_{a} Q(s, a) |_{s=s_i, a=\mu(s_i)} \nabla_{\theta^{\mu}} \mu(s_i)\]</span></p></li>
<li><p>Update the target critic and target actor:</p>
<p><span class="math display">\[\theta^{Q^{\prime}} \leftarrow \tau \theta^{Q} + (1 - \tau) \theta^{Q^{\prime}}\]</span> <span class="math display">\[\theta^{\mu^{\prime}} \leftarrow \tau \theta^{\mu} + (1 - \tau) \theta^{\mu^{\prime}}\]</span></p></li>
</ul></li>
</ol></li>
</ol>
<p>Features:</p>
<ol type="1">
<li><p>Use replay buffer to address correlated samples. Because DDPG is an off-policy algorithm, the replay buffer can be large, allowing the algorithm to benefit from learning across a set of uncorrelated transitions.</p></li>
<li><p>Similar to target network in DQN, a soft target updates rather than directly copying the weights is used. We create a copy of the actor and critic networks, <span class="math inline">\(Q^{\prime} (s, a | \theta^{Q^{\prime}})\)</span> and <span class="math inline">\(\mu^{\prime} (s | \theta^{\mu^{\prime}})\)</span>. Notice here, the target is calculated using these delayed parameters. One problem with this is that:</p>
<p><span class="math display">\[E[r(s_t, a_t) + \gamma Q^{\prime} (s, \mu^{\prime} (s_{t+1} | \theta^{\mu^{\prime}}) | \theta^{Q^{\prime}})] = E[\hat{T}^{\mu^{\prime}} Q^{\prime} (s, a)] = T^{\mu^{\prime}} Q^{\prime} (s, a)\]</span></p>
<p>So, if we minimize the square loss of the critic w.r.t this target for some iterations, then follow the approximate value iteration, we end up evaluating <span class="math inline">\(Q^{\mu^{\prime}} (s, a)\)</span>.</p>
<p>The weights of these target networks are then updated by having them slowly track the learned networks: <span class="math inline">\(\theta^{\prime} \leftarrow (1 - \tau) \theta^{\prime} + \theta\)</span> (stochastic approximation of mean of <span class="math inline">\(\theta\)</span>) with <span class="math inline">\(\tau &lt;&lt; 1\)</span>. This means that the target values are constrained to change slowly, greatly improving the stability of learning. The authors found that having both a delayed <span class="math inline">\(\mu^{\prime}\)</span> and a delayed <span class="math inline">\(Q^{\prime}\)</span> was required to have stable <span class="math inline">\(y_t\)</span> in order to consistently train the critic without divergence.</p></li>
<li><p>In low dimensional feature vector observation space, Batch Normalization is used to normalize each dimension across the samples in a mini-batch to have unit mean and variance. A running average of mean and variance is maintained in training time and used in test time in this case evaluation or exploration. This technique is applied on the state input and all layers of the <span class="math inline">\(\mu\)</span> network and all layers of the <span class="math inline">\(Q\)</span> network prior to the action input. This help with the learning speed and scalability across different envs and units.</p></li>
<li><p>Exploration problem is resolved by having a separate exploration policy <span class="math inline">\(\mu^{\prime}\)</span> (different from target network <span class="math inline">\(\mu^{\prime} (s | \theta^{\mu^{\prime}})\)</span>) by adding noise sampled from a noise process <span class="math inline">\(N\)</span> to our current actor policy:</p>
<p><span class="math display">\[\mu^{\prime} (s_t) = \mu (s_t | \theta_t^{\mu}) + \mathcal{N}\]</span></p>
<p>This <span class="math inline">\(N\)</span> can be chosen to suit the environment.</p></li>
</ol>

    </div>

    
    
    
      
  <div class="popular-posts-header">Related Posts</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/05/06/DPG/" rel="bookmark">Deterministic Policy Gradient</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/05/30/policy-gradient-2/" rel="bookmark">Policy Gradient (2)</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/06/05/policy-gradient-3/" rel="bookmark">Policy Gradient (3)</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/05/23/policy-gradient/" rel="bookmark">Policy Gradient (1)</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/06/23/td3/" rel="bookmark">td3</a></div>
    </li>
  </ul>


    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Policy-Gradient/" rel="tag"># Policy Gradient</a>
              <a href="/tags/Continuous-Action-Space/" rel="tag"># Continuous Action Space</a>
              <a href="/tags/Policy-Search/" rel="tag"># Policy Search</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/06/06/sort/" rel="prev" title="Sorts">
                  <i class="fa fa-chevron-left"></i> Sorts
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/06/11/TRPO/" rel="next" title="TRPO">
                  TRPO <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div><script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhu, Zhaoyang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Symbols count total">693k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">10:30</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.3" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{&quot;enable&quot;:true,&quot;app_id&quot;:&quot;gPOc95wWW2Hwp2pkVSAAz28m-MdYXbMMI&quot;,&quot;app_key&quot;:&quot;7Sf65xCHXfEEdBvu29UHSYdV&quot;,&quot;server_url&quot;:&quot;https:&#x2F;&#x2F;swag1ong.github.io&quot;,&quot;security&quot;:true}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{&quot;enable&quot;:true,&quot;repo&quot;:&quot;swag1ong&#x2F;swag1ong.github.io&quot;,&quot;issue_term&quot;:&quot;pathname&quot;,&quot;theme&quot;:&quot;github-light&quot;,&quot;pathname&quot;:&quot;pathname&quot;}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
