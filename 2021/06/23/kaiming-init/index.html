<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="1xbHmVEsS5Fta14x1d1FXkHFr93LRr1pMxipdYVcNI4">
  <meta name="baidu-site-verification" content="code-Srzz2vTuzA">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;swag1ong.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;Searching...&quot;,&quot;empty&quot;:&quot;We didn&#39;t find any results for the search: ${query}&quot;,&quot;hits_time&quot;:&quot;${hits} results found in ${time} ms&quot;,&quot;hits&quot;:&quot;${hits} results found&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:3,&quot;unescape&quot;:false,&quot;preload&quot;:true}}</script>
<meta name="description" content="Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification PReLU Parametric Rectifiers Formally, we consider an activation function defined as: \[ f(y_i)&#x3D; \begin{cases">
<meta property="og:type" content="article">
<meta property="og:title" content="Kaiming Initialization">
<meta property="og:url" content="https://swag1ong.github.io/2021/06/23/kaiming-init/index.html">
<meta property="og:site_name" content="GoGoGogo!">
<meta property="og:description" content="Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification PReLU Parametric Rectifiers Formally, we consider an activation function defined as: \[ f(y_i)&#x3D; \begin{cases">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://swag1ong.github.io/images/ML/kaiming_init.png">
<meta property="article:published_time" content="2021-06-23T07:36:03.000Z">
<meta property="article:modified_time" content="2021-08-18T07:07:36.000Z">
<meta property="article:author" content="Zhu, Zhaoyang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://swag1ong.github.io/images/ML/kaiming_init.png">


<link rel="canonical" href="https://swag1ong.github.io/2021/06/23/kaiming-init/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;en&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;swag1ong.github.io&#x2F;2021&#x2F;06&#x2F;23&#x2F;kaiming-init&#x2F;&quot;,&quot;path&quot;:&quot;2021&#x2F;06&#x2F;23&#x2F;kaiming-init&#x2F;&quot;,&quot;title&quot;:&quot;Kaiming Initialization&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Kaiming Initialization | GoGoGogo!</title><script src="/js/config.js"></script>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?852b005027c5efa81663f6f5c4c5b7fd"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">GoGoGogo!</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-igloo fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">102</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">16</span></a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">25</span></a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-address-card fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification"><span class="nav-number">1.</span> <span class="nav-text">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#prelu"><span class="nav-number">1.1.</span> <span class="nav-text">PReLU</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#parametric-rectifiers"><span class="nav-number">1.1.1.</span> <span class="nav-text">Parametric Rectifiers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#optimization"><span class="nav-number">1.1.2.</span> <span class="nav-text">Optimization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#initialization-of-filter-weights-for-rectifiers"><span class="nav-number">1.2.</span> <span class="nav-text">Initialization of Filter Weights for Rectifiers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#forward-propagation-case"><span class="nav-number">1.2.1.</span> <span class="nav-text">Forward Propagation Case</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#backward-propagation-case"><span class="nav-number">1.2.2.</span> <span class="nav-text">Backward Propagation Case</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#discussions"><span class="nav-number">1.3.</span> <span class="nav-text">Discussions</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhu, Zhaoyang"
      src="/images/others/favicon.jpeg">
  <p class="site-author-name" itemprop="name">Zhu, Zhaoyang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">102</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/swag1ong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;swag1ong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyang.zhu@mail.utoronto.ca" title="E-Mail → mailto:zhaoyang.zhu@mail.utoronto.ca" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zhuzhaoyangzzyu0616" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zhuzhaoyangzzyu0616" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/06/23/kaiming-init/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Kaiming Initialization
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-06-23 15:36:03" itemprop="dateCreated datePublished" datetime="2021-06-23T15:36:03+08:00">2021-06-23</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-08-18 15:07:36" itemprop="dateModified" datetime="2021-08-18T15:07:36+08:00">2021-08-18</time>
      </span>

  
    <span id="/2021/06/23/kaiming-init/" class="post-meta-item leancloud_visitors" data-flag-title="Kaiming Initialization" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>8.5k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>8 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</h1>
<h2 id="prelu">PReLU</h2>
<h3 id="parametric-rectifiers">Parametric Rectifiers</h3>
<p>Formally, we consider an activation function defined as: <span class="math display">\[
f(y_i)=
\begin{cases}
y_i, \quad &amp; y_i &gt; 0\\
a_i y_i, \quad &amp; y_i \leq 0\\
\end{cases}
\]</span></p>
<p>Here <span class="math inline">\(y_i\)</span> is the input of the nonlinear activation <span class="math inline">\(f\)</span> on the <span class="math inline">\(i\)</span>th channel, and <span class="math inline">\(a_i\)</span> is a coefficient controlling the slop of the negative part of the function. The subscript <span class="math inline">\(i\)</span> in <span class="math inline">\(a_i\)</span> indicates that we allow the nonlinear activation to vary on different channels. When <span class="math inline">\(a_i = 0\)</span>, it becomes ReLU. When <span class="math inline">\(a_i\)</span> is small and fixed value, we have <code>LReLU</code>. When <span class="math inline">\(a_i\)</span> is a learnable parameter, we call this <code>Parametric ReLU (PReLU)</code>.</p>
<p><img src='/images/ML/kaiming_init.png' width="600"></p>
<span id="more"></span>
<p>We can also rewrite the above formula as:</p>
<p><span class="math display">\[f(y_i) = \max (0, y_i) + a_i \min (0, y_i)\]</span></p>
<p>The <code>PReLU</code> only introduces one parameter per channel for each layer which is negligible when considering the total number of weights. At the same time, if we do not consider channels, we can use one <span class="math inline">\(a\)</span> for all channels in same layer, the parameter number reduces to 1.</p>
<h3 id="optimization">Optimization</h3>
<p><code>PReLU</code> can be trained using backpropagation and optimized simultaneously with other layers. For single layer:</p>
<p><span class="math display">\[\frac{\partial L}{\partial a_i} = \sum_{y_i} \frac{\partial L}{\partial f(y_i)} \frac{\partial f(y_i)}{\partial a_i}\]</span></p>
<p>Where <span class="math inline">\(L\)</span> is the objective function, <span class="math inline">\(f(y_1), ...., f(y_N)\)</span> are PReLU transformations. The gradient of PReLU transformation on single activation is given by:</p>
<p><span class="math display">\[
\frac{\partial f(y_i)}{\partial a_i} =
\begin{cases}
0, \quad &amp; y_i &gt; 0\\
y_i, \quad &amp; y_i \leq 0\\
\end{cases}
\]</span></p>
<p>For channel shared version (single <span class="math inline">\(a\)</span> for all channels in a layer):</p>
<p><span class="math display">\[\frac{\partial L}{\partial a} = \sum_{i} \sum_{y_i} \frac{\partial L}{\partial f(y_i)} \frac{\partial f(y_i)}{\partial a}\]</span></p>
<p>When, we can use any gradient based methods to update the parameter (e.g Momentum in this case):</p>
<p><span class="math display">\[a_i = \mu a_i + \epsilon \frac{\partial L}{\partial a_i}\]</span></p>
<h2 id="initialization-of-filter-weights-for-rectifiers">Initialization of Filter Weights for Rectifiers</h2>
<p>The main idea of the derivation is to investigate the variance of the response in each layer.</p>
<h3 id="forward-propagation-case">Forward Propagation Case</h3>
<p>For a conv layer, a response is:</p>
<p><span class="math display">\[\mathbf{y_l} = W_{l} \mathbf{x}_l + \mathbf{b}_l\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\mathbf{x}_l\)</span> is a <span class="math inline">\(k^2c \times 1\)</span> vector that represents co-located <span class="math inline">\(k \times k\)</span> pixels in <span class="math inline">\(c\)</span> input channels (This is a joint of input pixels) and <span class="math inline">\(k\)</span> is the kernel size (the volume after applying one kernel on the input feature maps).</li>
<li><span class="math inline">\(n = k^2c\)</span></li>
<li><span class="math inline">\(W_l\)</span> is a <span class="math inline">\(d \times n\)</span> weight matrix represents the weights in each kernel flattened in to row in <span class="math inline">\(W\)</span>.</li>
<li><span class="math inline">\(\mathbf{b}\)</span> is a <span class="math inline">\(b \times 1\)</span> bias vector, one dimension for each kernel.</li>
<li><span class="math inline">\(\mathbf{y}\)</span> is the response at a pixel of the output feature map (volume <span class="math inline">\(1 \times 1 \times d\)</span>).</li>
<li><span class="math inline">\(l\)</span> is used to index layers.</li>
<li><span class="math inline">\(\mathbf{x}_l = f(\mathbf{y}_{l - 1})\)</span> where <span class="math inline">\(f\)</span> is the activation function.</li>
<li><span class="math inline">\(c_l = d_{l - 1}\)</span> (input feature maps number is the same as kernel numbers in the previous layer).</li>
</ul>
<p><strong>The general idea in words is:</strong></p>
<p><span class="math inline">\(1 \times 1 \times d\)</span> volumn of the output feature map after applying <span class="math inline">\(d\)</span> kernels on the corresponding <span class="math inline">\(k \times k\)</span> region of <span class="math inline">\(c\)</span> input feature maps.</p>
<p><br></p>
<p>We Assume:</p>
<ol type="1">
<li><p>The initialized elements in <span class="math inline">\(W_l\)</span> be mutually independent random variables draw from the same distribution.</p></li>
<li><p>The elements in <span class="math inline">\(\mathbf{x}_l\)</span> are also mutually independent random variables draw from the same distribution.</p></li>
<li><p>(1) and (2) are independent.</p></li>
</ol>
<p>Thus, we have:</p>
<p><span class="math display">\[Var(y_l) = Var(\mathbf{w}_l \cdot \mathbf{x}_l) = n_l Var(w_l x_l)\]</span></p>
<p>Where <span class="math inline">\(y_l, \; \mathbf{w}_l\)</span> are rows of <span class="math inline">\(\mathbf{y}, \; W_l\)</span> respectively, <span class="math inline">\(w_l, \; x_l\)</span> are elements of <span class="math inline">\(\mathbf{w}_l, \; \mathbf{x}_l\)</span></p>
<p>If <span class="math inline">\(w_l\)</span> has zero mean, then:</p>
<span class="math display">\[\begin{aligned}
Var(y_l) &amp;= n_l Var(w_l x_l) \\
&amp;= n_l Var(w_l) Var(x_l) + Var(w_l)E^{2}[x_l]\\
&amp;= n_l Var(w_l) [Var(x_l) + E^{2}[x_l]]\\
&amp;= n_l Var(w_l) E[x^{2}_l]
\end{aligned}\]</span>
<p>Note here, in Xavier Initialization, we have <span class="math inline">\(E[x_l] = 0\)</span> because random variables with zero means after tanh function have zero mean. For ReLU activation, we have <span class="math inline">\(x_l = max(0, y_{l - 1})\)</span> and thus it does not have zero mean.</p>
<p>If we let <span class="math inline">\(w_{l - 1}\)</span> have a symmetric distribution around zero (<span class="math inline">\(E[w_{l-1}] = 0\)</span>) and <span class="math inline">\(b_{l - 1} = 0\)</span>, then:</p>
<p><span class="math display">\[E[y_{l - 1}] = E[w_l]E[x_l] = 0\]</span></p>
<p>Thus, <span class="math inline">\(y_{l - 1}\)</span> has zero mean and it has a symmetric distribution around zero. This lead to:</p>
<span class="math display">\[\begin{aligned}
E_{Y}[x^2_{l}] &amp;= \int^{\infty}_{\infty} \max(0, y_{l - 1})^2 p_Y (y) dy\\
&amp;= \int^{\infty}_{0} y_{l - 1}^2 p_Y (y) dy\\
&amp;= \frac{1}{2} E[y_{l-1}^2]
\end{aligned}\]</span>
<p>Since <span class="math inline">\(\frac{1}{2} Var(y_{l - 1}) = \frac{1}{2} E[y_{l-1}^2] - \frac{1}{2} E^2[y_{l-1}] = \frac{1}{2} E[y_{l-1}^2]\)</span>:</p>
<p><span class="math display">\[\implies E_{Y}[x^2_{l}] = \frac{1}{2} Var(y_{l - 1})\]</span></p>
<p>Then, putting by this result to previous equation we have:</p>
<p><span class="math display">\[Var(y_l) = n_l Var(w_l) E[x^{2}_l] = \frac{n_l}{2} Var(y_{l - 1}) Var(w_l)\]</span></p>
<p>With <span class="math inline">\(L\)</span> layers put together, we have:</p>
<span class="math display">\[\begin{aligned}
Var(y_L) &amp;= \frac{n_L}{2} Var(y_{L - 1}) Var(w_L)\\
&amp;= \frac{n_L}{2} \frac{n_{L - 1}}{2} Var(w_L) Var(w_{L-1}) Var(y_{L - 2})\\
&amp;= Var(y_1) \prod^{L}_{l=2} \frac{n_l}{2} Var(w_{l})
\end{aligned}\]</span>
<p>The <strong>key</strong> is the product above, we can see that the variance of current activate depends on product of previous weights, this lead to exploding or vanishing of variance of initial activation. <strong>A proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially</strong>.</p>
<p>So we expect the above product to take a proper scalar 1, a sufficient condition is:</p>
<p><span class="math display">\[\frac{1}{2} n_{l} Var(w_l) = 1, \; \forall l\]</span></p>
<p>This leads to the algorithm for forward case:</p>
<ol type="1">
<li><p>Initialize <span class="math inline">\(w_l\)</span> from a zero-mean Gaussian distribution (<span class="math inline">\(w_l\)</span> is assumed to be symmetric and with mean 0) whose standard deviation is <span class="math inline">\(\sqrt{\frac{2}{n_l}}\)</span>.</p></li>
<li><p>Initialize <span class="math inline">\(\mathbf{b} = 0\)</span>.</p></li>
<li><p>For the first layer, we initialize the variance to be <span class="math inline">\(\frac{1}{n_1}\)</span>.</p></li>
</ol>
<h3 id="backward-propagation-case">Backward Propagation Case</h3>
<p>For back-propagation, the gradient of a conv layer is computed by:</p>
<p><span class="math display">\[\Delta \mathbf{x}_l =  [\hat{W}_l  \Delta \mathbf{y}_l]_{c \times 1}\]</span></p>
<p>Where <span class="math inline">\(\frac{\partial L}{\partial \mathbf{x}} \triangleq \Delta \mathbf{x}\)</span> and <span class="math inline">\(\frac{\partial L}{\partial \mathbf{y}} \triangleq \Delta \mathbf{y}\)</span>. Notice here, <span class="math inline">\(\Delta \mathbf{y}, \Delta \mathbf{x}\)</span> represents the gradient for single pixel, so each of them has shape <span class="math inline">\(d_i \times 1\)</span>. Recall that, backward pass of a conv layer is also a convolution process with <span class="math inline">\(W^l\)</span> flipped 180 degrees and applying it on gradient map of <span class="math inline">\(\mathbf{y}^l\)</span>, so <span class="math inline">\(\Delta \mathbf{y}_l\)</span> is defined similar to <span class="math inline">\(\mathbf{x}_l\)</span> in the forward pass. It is the <span class="math inline">\(k \times k \times d\)</span> co-located pixels (gradients) in the output feature maps. <span class="math inline">\(\hat{W}_l\)</span> in this case is the original kernel flipped 180 degree.</p>
<p>Notations:</p>
<ol type="1">
<li><span class="math inline">\(\Delta \mathbf{y}_l\)</span> represents <span class="math inline">\(k \times k\)</span> pixels in <span class="math inline">\(d\)</span> dimension channels and is reshaped into a <span class="math inline">\(k^2d \times 1\)</span> vector.</li>
<li><span class="math inline">\(\hat{n} = k^2 d\)</span>, notice here <span class="math inline">\(\hat{n} \neq n\)</span>.</li>
<li><span class="math inline">\(\hat{W}_l\)</span> is a <span class="math inline">\(c \times \hat{n}\)</span> matrix where the filters are rearranged in the way of back-propagation (flipped 180 degree). <span class="math inline">\(W\)</span> and <span class="math inline">\(\hat{W}\)</span> can be reshaped from each other.</li>
<li><span class="math inline">\(\Delta \mathbf{x}\)</span> is a <span class="math inline">\(c \times 1\)</span> vector representing the gradient at a pixel of the input to this layer.</li>
</ol>
<p>Assumptions:</p>
<ol type="1">
<li>We assume <span class="math inline">\(w_l\)</span> is initialized by a symmetric distribution around zero (<span class="math inline">\(w_l, \hat{w}_l\)</span> are from matrix but different representation so their distribution are the same).</li>
<li>We assume <span class="math inline">\(w_l\)</span>, <span class="math inline">\(\Delta y_l\)</span> are independent of each other <span class="math inline">\(\implies E[\Delta x_l] = \hat{n}_lE[w_l]E[\Delta y_l] = 0\)</span> for all <span class="math inline">\(l\)</span></li>
<li>We assume that <span class="math inline">\(f^{\prime} (y_l)\)</span> and <span class="math inline">\(\Delta x_{l + 1}\)</span> are independent.</li>
</ol>
<p><br></p>
<p>Recall that <span class="math inline">\(\Delta \mathbf{y}_l = \Delta \mathbf{x}^T_{l + 1} \frac{\partial \mathbf{x}_{l + 1}}{\partial \mathbf{y}_l}\)</span> and <span class="math inline">\(\frac{\partial \mathbf{x}_{l + 1}}{\partial \mathbf{y}_l}\)</span> is a diagonal Jacobian matrix with <span class="math inline">\(f^{\prime} (y_l)\)</span> on the diagonal, so each element of <span class="math inline">\(\mathbf{y}_l\)</span> will have gradient <span class="math inline">\(\Delta y_l = f^{\prime} (y_l) \Delta x_{l + 1}\)</span>.</p>
<p>For the ReLU case, <span class="math inline">\(f^{\prime} (y_l)\)</span> is defined as:</p>
<p><span class="math display">\[
\frac{\partial x_{l + 1}}{\partial y_l} =
\begin{cases}
1, \quad &amp; y_l \geq 0\\
0, \quad &amp; y_l &lt; 0\\
\end{cases}
\]</span></p>
<p>and their probabilities are equal.</p>
<p>Since assumption 3, we have:</p>
<p><span class="math display">\[E[\Delta y_l] = E[f^{\prime} (y_l)]E[\Delta x_{l+1}] = \frac{E[\Delta x_{l+1}]}{2} = 0\]</span></p>
<p>and also:</p>
<p><span class="math display">\[E[(\Delta y_l)^2] = Var(\Delta y_l) = Var(f^{\prime} (y_l)\Delta x_{l+1}) = \frac{Var[\Delta x_{l+1}]}{2}\]</span></p>
<p>Then the variance of gradient w.r.t to the input to layer <span class="math inline">\(l\)</span> is:</p>
<span class="math display">\[\begin{aligned}
Var[\Delta x_l] &amp;= \hat{n}_l(Var(w_l)Var(\Delta y_l) + Var(w_l)E^2[\Delta y_l] + Var(\Delta y_l)E^2[w_l])\\
&amp;= \hat{n}_lVar(w_l)Var(\Delta y_l)\\
&amp;= \frac{\hat{n}}{2} Var(w_l) Var(\Delta x_{l + 1})
\end{aligned}\]</span>
<p>With <span class="math inline">\(L\)</span> layers together, we have:</p>
<p><span class="math display">\[Var(\Delta x_2) = Var(\Delta x_{L + 1}])\prod^{L}_{l=2} \frac{\hat{n}_l}{2} Var(w_l)\]</span></p>
<p>To make <span class="math inline">\(Var(\Delta x_2) = Var(\Delta x_{L + 1})\)</span>, we need to have:</p>
<p><span class="math display">\[\prod^{L}_{l=2} \frac{\hat{n}_l}{2} Var(w_l) = 1\]</span></p>
<p>Thus a sufficient condition that the gradient is not exponentially large or small:</p>
<p><span class="math display">\[\frac{\hat{n}}{2} Var(w_l) = 1\]</span></p>
<p>This results in a zero-mean Gaussian distribution whose std is <span class="math inline">\(\sqrt{\frac{2}{\hat{n}}}\)</span></p>
<h2 id="discussions">Discussions</h2>
<ol type="1">
<li><p>It is sufficient to use either forward result or backward result alone, because they only differ by a constant which is not diminishing number in common network designs. This means that if the initialization properly scales the backward signal, then this is also the case for the forward signal.</p></li>
<li><p>If the forward/backward signal is inappropriately scaled by a factor <span class="math inline">\(\beta\)</span> in each layer, then the final propagated signal will be rescaled by a factor of <span class="math inline">\(\beta^L\)</span> after <span class="math inline">\(L\)</span> layers, where <span class="math inline">\(L\)</span> can represent some or all layers. When <span class="math inline">\(L\)</span> is large, if <span class="math inline">\(\beta &gt; 1\)</span>, this leads to extremely amplified signals. If <span class="math inline">\(\beta &lt; 1\)</span>, this leads to diminishing signal.</p></li>
</ol>

    </div>

    
    
    
      


    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/06/22/alex-net/" rel="prev" title="Alex Net">
                  <i class="fa fa-chevron-left"></i> Alex Net
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/06/23/td3/" rel="next" title="td3">
                  td3 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div><script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhu, Zhaoyang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Symbols count total">805k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">12:12</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.3" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{&quot;enable&quot;:true,&quot;app_id&quot;:&quot;gPOc95wWW2Hwp2pkVSAAz28m-MdYXbMMI&quot;,&quot;app_key&quot;:&quot;7Sf65xCHXfEEdBvu29UHSYdV&quot;,&quot;server_url&quot;:&quot;https:&#x2F;&#x2F;swag1ong.github.io&quot;,&quot;security&quot;:true}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{&quot;enable&quot;:true,&quot;repo&quot;:&quot;swag1ong&#x2F;swag1ong.github.io&quot;,&quot;issue_term&quot;:&quot;pathname&quot;,&quot;theme&quot;:&quot;github-light&quot;,&quot;pathname&quot;:&quot;pathname&quot;}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
