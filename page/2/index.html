<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="1xbHmVEsS5Fta14x1d1FXkHFr93LRr1pMxipdYVcNI4">
  <meta name="baidu-site-verification" content="code-Srzz2vTuzA">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;swag1ong.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;Searching...&quot;,&quot;empty&quot;:&quot;We didn&#39;t find any results for the search: ${query}&quot;,&quot;hits_time&quot;:&quot;${hits} results found in ${time} ms&quot;,&quot;hits&quot;:&quot;${hits} results found&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:3,&quot;unescape&quot;:false,&quot;preload&quot;:true}}</script>
<meta property="og:type" content="website">
<meta property="og:title" content="GoGoGogo!">
<meta property="og:url" content="https://swag1ong.github.io/page/2/index.html">
<meta property="og:site_name" content="GoGoGogo!">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhu, Zhaoyang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://swag1ong.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:true,&quot;isPost&quot;:false,&quot;lang&quot;:&quot;en&quot;,&quot;comments&quot;:&quot;&quot;,&quot;permalink&quot;:&quot;&quot;,&quot;path&quot;:&quot;page&#x2F;2&#x2F;index.html&quot;,&quot;title&quot;:&quot;&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>GoGoGogo!</title><script src="/js/config.js"></script>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?852b005027c5efa81663f6f5c4c5b7fd"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">GoGoGogo!</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-igloo fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">97</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">16</span></a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">25</span></a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-address-card fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhu, Zhaoyang"
      src="/images/others/favicon.jpeg">
  <p class="site-author-name" itemprop="name">Zhu, Zhaoyang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">97</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/swag1ong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;swag1ong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyang.zhu@mail.utoronto.ca" title="E-Mail → mailto:zhaoyang.zhu@mail.utoronto.ca" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zhuzhaoyangzzyu0616" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zhuzhaoyangzzyu0616" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/12/25/transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/25/transformer/" class="post-title-link" itemprop="url">Transformer</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-12-25 18:08:16 / Modified: 18:19:07" itemprop="dateCreated datePublished" datetime="2021-12-25T18:08:16+08:00">2021-12-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/" itemprop="url" rel="index"><span itemprop="name">DL</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/CNN/" itemprop="url" rel="index"><span itemprop="name">CNN</span></a>
        </span>
    </span>

  
    <span id="/2021/12/25/transformer/" class="post-meta-item leancloud_visitors" data-flag-title="Transformer" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>3.8k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>3 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="attention-is-all-you-need">Attention is all you need</h1>
<p><img src="/images/ML/transformer_2.jpg" align="center" width=600></p>
<p><img src="/images/ML/transformer_4.jpg" align="center" width=600></p>
<h2 id="structure">Structure</h2>
<h3 id="encoder">Encoder</h3>
<p>The encoder consists of 6 identical layers. Each layer consists of two sub-layers: 1. Multihead self-attention 2. Feed forward fully connected network</p>
<p>residual connection (skip connection) are employed around each of the two sub-layers followed by layer normalization. The output of each sub-layer is:</p>
<p><span class="math display">\[\text{LayerNorm}(x + \text{Sublayer}(x))\]</span></p>
<p>Where <span class="math inline">\(\text{Sublayer}(x)\)</span> is the function implemented by the sub-layer itself. All sub-layers and embedding layers have output of dimension <span class="math inline">\(d_{model} = 512\)</span></p>
<h3 id="decoder">Decoder</h3>
<p>The decoder consists of 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack as key, value and the previous decoder output as query. The first self-attention sub-layer is modified to mask the future during training, this ensures that the predictions for position <span class="math inline">\(i\)</span> can depend only on the known outputs at positions less than <span class="math inline">\(i\)</span>.</p>
<h2 id="self-attention">Self Attention</h2>
<p>Given input <span class="math inline">\(X \in \mathbb{R}^{n_b \times n_s \times d_{model}}\)</span> and trainable parameters <span class="math inline">\(W_q \in \mathbb{R}^{d_{model} \times d_{k}}, W_k \in \mathbb{R}^{d_{model} \times d_{k}}, W_v \in \mathbb{R}^{d_{model} \times d_{v}}\)</span>, matrices query <span class="math inline">\(Q \in \mathbb{R}^{n_b \times n_s \times d_k}\)</span>, key <span class="math inline">\(K \in \mathbb{R}^{n_b \times n_s \times d_k}\)</span>, value <span class="math inline">\(V \in \mathbb{R}^{n_b \times n_s \times d_v}\)</span> are defined as:</p>
<p><span class="math display">\[Q = XW_q, \;\; K = XW_k, \;\; V = XW_v\]</span></p>
<h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3>
<p><img src="/images/ML/transformer_1.jpg"></p>
<p><span class="math display">\[\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}}) V\]</span></p>
<p>The scale is used to prevent the large magnitude of the dot product so that the gradient of softmax vanishes.</p>
<h3 id="multi-head-self-attention">Multi-head Self Attention</h3>
<p><img src="/images/ML/transformer_3.jpg"></p>
<p>Instead of using a single attention function, <span class="math inline">\(h\)</span> of parallel attention functions with different <span class="math inline">\(W^q_i, W^k_i, W^v_i\)</span> are used and concatenated into single self attention matrix. The final self attention matrix is then multiplied by a weight matrix <span class="math inline">\(W_o \in \mathbb{R}^{hd_v \times d_{model}}\)</span>:</p>
<p><span class="math display">\[\text{MultiHead} (Q, K, V) = \text{Concat} (\text{head}_1, ...., \text{head}_h) W_o\]</span></p>
<p>Where</p>
<p><span class="math display">\[\text{head}_i = \text{Attention}(QW^Q_i, KW^k_i, VW^V_i)\]</span></p>
<p><strong>In this paper <span class="math inline">\(h = 8, d_{model} = 512, d_{model} / h = d_k = d_v = 64\)</span> and in a multi-head self-attention layer, all the keys, values and queries come from same place which is the output of the previous layer in the encoder. (i.e <span class="math inline">\(Q = K = V = X\)</span> where <span class="math inline">\(X\)</span> is the output from previous layer)</strong></p>
<h3 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</h3>
<p>In addition to attention sub-layers, each of the layers in the encoder and decoder contains a fully connected feed-forward network. This consists of two linear transformations with a ReLU activation in between:</p>
<p><span class="math display">\[\text{FFN}(\mathbf{x}) = max(0, \mathbf{x} W_1 + \mathbf{b}_1) W_2 + \mathbf{b}_2\]</span></p>
<p>The parameters are different for different layers. The dimensionality of input and output is <span class="math inline">\(d_{model} = 512\)</span>, and the inner-layer has dimensionality <span class="math inline">\(d_{ff} = 2048, W_1 \in \mathbb{R}^{502 \times 2048}, W_2 \in \mathbf{2048 \times 502}\)</span></p>
<h3 id="positional-encoding">Positional Encoding</h3>
<p>Since the model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. Thus, we add <strong>positional encodings</strong> to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension <span class="math inline">\(d_{model}\)</span> as the embeddings, so that the two can be summed:</p>
<p><span class="math display">\[PE_{(pos, 2i)} = \sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\]</span> <span class="math display">\[PE_{(pos, 2i+1)} = \cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\]</span></p>
<p>Where <span class="math inline">\(i\)</span> is the dimension, <span class="math inline">\(pos\)</span> is the position. For example <span class="math inline">\(PE(1) = [\sin(\frac{1}{10000^{\frac{0}{d_{model}}}}), \cos(\frac{1}{10000^{\frac{2}{d_{model}}}}), \sin(\frac{1}{10000^{\frac{4}{d_{model}}}}) ....]\)</span></p>
<p>It has several properties:</p>
<ol type="1">
<li>For each time-step, it outputs a unique encoding.</li>
<li>The distance between two time-steps is consistent across sentences with different lengths.</li>
<li>Deterministic</li>
<li><span class="math inline">\(PE_{pos+k}\)</span> can be represented linearly using <span class="math inline">\(PE_{pos}\)</span>, so it generalizes easily to unseen length sequences.</li>
</ol>
<h2 id="ref">Ref</h2>
<p>https://jalammar.github.io/illustrated-transformer/</p>
<p>https://theaisummer.com/self-attention/</p>
<p>https://zhuanlan.zhihu.com/p/98641990</p>
<p>https://kazemnejad.com/blog/transformer_architecture_positional_encoding/#the-intuition</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/12/25/tcn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/25/tcn/" class="post-title-link" itemprop="url">TCN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-12-25 18:07:52 / Modified: 18:17:46" itemprop="dateCreated datePublished" datetime="2021-12-25T18:07:52+08:00">2021-12-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/" itemprop="url" rel="index"><span itemprop="name">DL</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/CNN/" itemprop="url" rel="index"><span itemprop="name">CNN</span></a>
        </span>
    </span>

  
    <span id="/2021/12/25/tcn/" class="post-meta-item leancloud_visitors" data-flag-title="TCN" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>2.2k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>2 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="temporal-convolutional-networks">Temporal Convolutional Networks</h1>
<p>Characteristics of TCN:</p>
<ol type="1">
<li>The convolutions in the architecture are causal.</li>
<li>The architecture can take a sequence of any length and map it to an output sequence of same length.</li>
<li>The ability of long term memories using a combination of very deep networks (with residual layers) and dilated convolutions.</li>
</ol>
<h2 id="fully-convolutional-network">Fully Convolutional Network</h2>
<p>Fully convolutional networks replace the fully connected layers with convolution layers. The TCN uses a <span class="math inline">\(1D\)</span> fully-convolutional network architecture, where each hidden layer is the same length as the input layer and zero padding of length (kernel - 1) in the front of the sequence is added to keep subsequent layers the same length as previous ones.</p>
<h2 id="causal-convolutions">Causal Convolutions</h2>
<p>To achieve the causality, the TCN uses causal convolutions, convolutions where an output at time <span class="math inline">\(t\)</span> is convolved only with elements from time <span class="math inline">\(t\)</span> and earlier in the previous layer. Thus:</p>
<p><span class="math display">\[\text{TCN} = 1D\text{FCN} + \text{causal convolutions}\]</span></p>
<p>The major disadvantage of this design is that we need an extremely deep network or very large filters to achieve a long effective history size.</p>
<h2 id="dilated-convolutions">Dilated Convolutions</h2>
<p>The solution is to introduce dilated convolutions:</p>
<p><span class="math display">\[F_d(\mathbf{x}, s) = \sum^{k-1}_{i=0} f(i) \cdot \mathbf{x}_{s - d\cdot i}\]</span></p>
<p>Where <span class="math inline">\(f(i)\)</span> is the <span class="math inline">\(i\)</span>th component of the 1D filter with length <span class="math inline">\(k - 1, i=0, ...., k-1\)</span> and <span class="math inline">\(d\)</span> is the dilation factor, <span class="math inline">\(k\)</span> is the filter size, and <span class="math inline">\(s - d \cdot i\)</span> accounts for the direction of the past. Using large dilation enables an output at the top level to represent a wider range of inputs, thus effectively expanding the receptive field of a ConvNet. Thus, we can choose to: 1. Larger filter size <span class="math inline">\(k\)</span>. 2. Choose larger Dilation factor <span class="math inline">\(d\)</span>. Usually it is increased exponentially with the depth of the network <span class="math inline">\(d = 2^i\)</span> at level <span class="math inline">\(i\)</span> of the network. This ensures that there is some filter that hits each input within the effective history, while also allowing for an extremely large effective using deep networks.</p>
<p><img src="/images/ML/tcn_1.jpg"></p>
<h2 id="residual-connections">Residual Connections</h2>
<p>To allow faster learning , a residual block is introduced with weight norm and dropout inbetween to replace the convolution layer:</p>
<p><img src="/images/ML/tcn_2.jpg"></p>
<p>In TCN, the input and output of the residual block could have different widths (channels), to account for discrepant input-output widths, we use an additional <span class="math inline">\(1 \times 1\)</span> convolution to ensure that elementwise addition will work.</p>
<h2 id="advantages-of-tcn">Advantages of TCN</h2>
<ol type="1">
<li><strong>Parallelism</strong>:</li>
<li><strong>Flexible Receptive Field Size</strong></li>
<li><strong>Stable Gradients</strong></li>
<li><strong>Low Memory Requirement for Training</strong></li>
<li><strong>Variable length inputs</strong></li>
</ol>
<h2 id="disadvantages-of-tcn">Disadvantages of TCN</h2>
<ol type="1">
<li><strong>Data storage during evaluation</strong>:</li>
<li><strong>Potential parameter change for a transfer of domain</strong>:</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/11/19/time-series-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/11/19/time-series-2/" class="post-title-link" itemprop="url">Time Series (2)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-11-19 13:10:49" itemprop="dateCreated datePublished" datetime="2021-11-19T13:10:49+08:00">2021-11-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-11-29 15:08:03" itemprop="dateModified" datetime="2021-11-29T15:08:03+08:00">2021-11-29</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Background/" itemprop="url" rel="index"><span itemprop="name">Background</span></a>
        </span>
    </span>

  
    <span id="/2021/11/19/time-series-2/" class="post-meta-item leancloud_visitors" data-flag-title="Time Series (2)" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>22k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>20 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="time-series-arima">Time Series (ARIMA)</h1>
<h2 id="autoregressive-moving-average-models-arma">Autoregressive Moving Average Models (ARMA)</h2>
<h3 id="autoregressive-models">Autoregressive Models</h3>
<p>Autoregressive models are based on the idea that the current value of the series <span class="math inline">\(X_t\)</span> can be explained as a function of <span class="math inline">\(p\)</span> past values, <span class="math inline">\(X_{t-1}, X_{t-2}, ..., X_{t-p}\)</span> where <span class="math inline">\(p\)</span> determines the number of steps into the past needed to forecast the current value.</p>
<p>An autoregressive model of order <span class="math inline">\(p\)</span>, abbreviated <span class="math inline">\(AR(p)\)</span> with <span class="math inline">\(E[X_t] = 0\)</span>, is of the form:</p>
<p><span class="math display">\[X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + ... + \phi_p X_{t-p} + W_t\]</span></p>
<p>Where <span class="math inline">\(X_t\)</span> is stationary, <span class="math inline">\(\phi_1, ..., \phi_p \neq 0\)</span> are constants, <span class="math inline">\(W_t\)</span> is a Gaussian white noise series with mean zero and variance <span class="math inline">\(\sigma^2_w\)</span>. <strong>We assume above equation <span class="math inline">\(X_t\)</span> has mean zero</strong>, if it has non zero mean <span class="math inline">\(\mu\)</span>, we can replace it by:</p>
<p><span class="math display">\[X_t - \mu = \phi(X_{t-1} - \mu) + \phi_2(X_{t-2} - \mu) + ... + \phi_p (X_{t-p} - \mu) + W_t\]</span> <span class="math display">\[\implies X_t = \alpha + \phi_1 X_{t-1} + \phi_2 X_{t-2} + ... + \phi_p X_{t-p} + W_t\]</span></p>
<p>Where <span class="math inline">\(\alpha = \mu(1 - \phi_1 - ... - \phi_p)\)</span>.</p>
<p>We can also use the backshift operator to rewrite the zero mean <span class="math inline">\(AR(p)\)</span> process as:</p>
<p><span class="math display">\[(1 - \phi_1 B - \phi_2 B^2 - ... - \phi_p B^p) X_t = W_t\]</span></p>
<p>or using <strong>autoregressive operator</strong>:</p>
<p><span class="math display">\[\phi(B)X_t = W_t\]</span></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/11/19/time-series-2/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/11/15/time-series-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/11/15/time-series-1/" class="post-title-link" itemprop="url">Time Series (1)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-11-15 13:10:49" itemprop="dateCreated datePublished" datetime="2021-11-15T13:10:49+08:00">2021-11-15</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-11-19 15:19:43" itemprop="dateModified" datetime="2021-11-19T15:19:43+08:00">2021-11-19</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Background/" itemprop="url" rel="index"><span itemprop="name">Background</span></a>
        </span>
    </span>

  
    <span id="/2021/11/15/time-series-1/" class="post-meta-item leancloud_visitors" data-flag-title="Time Series (1)" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>11 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="time-series-introduction">Time Series (Introduction)</h1>
<h2 id="characteristics-of-time-series">Characteristics of Time Series</h2>
<p>The primary objective of time series analysis is to develop mathematical models that provide plausible descriptions for sample data with time correlations. In order to provide a statistical setting for describing the character of data that seemingly fluctuate in a random fashion over time, we assume a time series can be defined as a collection of random variables indexed according to the order they are obtained in time. In general, a collection of random variables <span class="math inline">\(\{X_t\}\)</span> indexed by <span class="math inline">\(t\)</span> is referred to as a stochastic process. In this text, <span class="math inline">\(t\)</span> will typically be discrete and vary over the integers.</p>
<p><strong>Example of Series:</strong></p>
<blockquote>
<p><strong>White Noise</strong>: A collection of uncorrelated, independent and identically distributed random variables <span class="math inline">\(W_t\)</span> with mean 0 and finite variance <span class="math inline">\(\sigma^2_w\)</span>. A particular useful white noise is Gaussian white noise, that is <span class="math inline">\(W_t \overset{i.i.d}{\sim} N(0, \sigma^2_w)\)</span> <img src="/images/RL/background/ts_1_3_1.png" width="600"></p>
</blockquote>
<blockquote>
<p><strong>Moving Average</strong>: We might replace the white noise series <span class="math inline">\(W_t\)</span> by a moving average that smooths the series: <span class="math display">\[V_t = \frac{1}{3} (W_{t-1} + W_{t} + W_{t+1})\]</span> This introduces a smoother version of white noise series, reflecting the fact that the slower oscillations are more apparent and some of the faster oscillations are taken out. <img src="/images/RL/background/ts_1_3_2.png" width="600"></p>
</blockquote>
<blockquote>
<p><strong>Autoregressions</strong>: Suppose we consider the white noise series <span class="math inline">\(W_t\)</span> as input and calculate the output using the second-order equation: <span class="math display">\[X_t = X_{t-1} - 0.9 X_{t-2} + W_t\]</span> For <span class="math inline">\(t=1, ..., 500\)</span>. We can see the periodic behavior of the series. <img src="/images/RL/background/ts_1_3_3.png" width="600"></p>
</blockquote>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/11/15/time-series-1/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/09/16/hmm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/09/16/hmm/" class="post-title-link" itemprop="url">Sequential Data</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-09-16 14:25:15" itemprop="dateCreated datePublished" datetime="2021-09-16T14:25:15+08:00">2021-09-16</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-09-23 14:04:21" itemprop="dateModified" datetime="2021-09-23T14:04:21+08:00">2021-09-23</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/09/16/hmm/" class="post-meta-item leancloud_visitors" data-flag-title="Sequential Data" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>8.6k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>8 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="sequential-data">Sequential Data</h1>
<p>In some cases, we have i.i.d assumptions to allow use to express the likelihood function as the product over all data points of the probability distribution evaluated at each data point. However, in some cases namely sequential data, we may not have i.i.d samples.</p>
<h2 id="markov-models">Markov Models</h2>
<p>To express the sequential dependence of the samples, we can relax the i.i.d assumption and one of the simplest ways to do this is to consider a <code>Markov Model</code>. First of all, without loss of generality, we can use the product rule to express the joint distribution for a sequence of observations in the form:</p>
<p><span class="math display">\[P(X_1, ...., X_n) = \prod^N_{n=1} P(X_n | X_1, ..., X_{n-1})\]</span></p>
<p>If we assume that each of the conditional distributions on the right-hand side is independent of all previous observations except the most recent, we obtain the <code>first-order Markov Chain</code>, which can depicted as graphical model:</p>
<p><img src='/images/ML/hmm_1.png' width="600"></p>
<p>The joint distribution for a sequence of <span class="math inline">\(N\)</span> observations under this model is given by:</p>
<p><span class="math display">\[P(X_1, ..., X_N) = P(X_1) \prod^{N}_{n=2} P(X_n | X_{n-1})\]</span></p>
<p>Thus, if we use such a model to predict the next observation in a sequence, the distribution of predictions will depend only on the value of the immediately preceding observation and will be independent of all earlier observations. In most applications of such models, the conditional distributions <span class="math inline">\(P(X_n | X_{n-1})\)</span> that define the model will be constrained to be equal, corresponding to a assumption of a stationary time series. The model is then known as the <code>homogeneous Markov Chain</code> (All conditional distribution share the same parameters).</p>
<p>However, first-order Markov model is still restrictive. For many sequential observations, we anticipate that the trends in the data over several successive observations will provide important information in predicting the next value. One way to allow earlier observations to have an influence is to move to higher-order Markov chains, the <code>second-order Markov chain</code> is given by:</p>
<p><span class="math display">\[P(X_1, ..., X_N) = P(X_1) P(X_2 | X_1) \prod^{N}_{n=3} P(X_n | X_{n-1}, X_{n-2})\]</span></p>
<p><img src='/images/ML/hmm_2.png' width="600"></p>
<p>Suppose we wish to build a model for sequences that is not limited by the Markov assumption to any order and yet that can be specified using a limited number of free parameters. We can achieve this by introducing additional latent variables to permit a rich class of models to be constructed out of simple components, as we did with mixture distributions.</p>
<p>For each observation <span class="math inline">\(\mathbf{X}_n\)</span>, we introduce a corresponding latent random vector <span class="math inline">\(\mathbf{Z}_n\)</span> which may be of different type or dimensionality to the observed variable. We now assume that it is the <strong>latent variables that form a Markov chain</strong>, giving rise to the graphical structure known as <code>state space model</code>. It satisfies the key conditional independence properties that <span class="math inline">\(\mathbf{Z}_{n-1}, \mathbf{Z}_{n+1}\)</span> are independent given <span class="math inline">\(\mathbf{Z}_n\)</span> so that:</p>
<p><span class="math display">\[\mathbf{Z}_{n+1} \perp \!\!\! \perp \mathbf{Z}_{n-1} \;|\; \mathbf{Z}_n\]</span></p>
<p>The joint distribution of the model is:</p>
<p><span class="math display">\[P(\mathbf{X}_1, ...., \mathbf{X}_{N}, \mathbf{Z}_1, ...., \mathbf{Z}_{N}) = P(\mathbf{Z}_1) \prod^N_{n=1} P(\mathbf{X}_n | \mathbf{Z}_n) \prod^{N}_{n=2} P(\mathbf{Z}_n | \mathbf{Z}_{n-1})\]</span></p>
<p><img src='/images/ML/hmm_3.png' width="600"></p>
<p>Using the d-separation criterion, we see that there is always a path connecting any two observed variables <span class="math inline">\(\mathbf{X}_n\)</span> and <span class="math inline">\(\mathbf{X}_{n+1}\)</span> via the latent variable, so the predictive distribution <span class="math inline">\(P(\mathbf{X}_{n+1} | \mathbf{X}_{1} , ..., \mathbf{X}_{n})\)</span> does not have any conditional dependence properties so it depends on all previous variables.</p>
<p>There are two important models for sequential data:</p>
<ol type="1">
<li><strong>Hidden Markov Model</strong>: If the latent variables are discrete.</li>
<li><strong>Linear Dynamical System</strong>: If the latent variables, observed variables are Gaussian with a linear-Gaussian dependence of the conditional distributions on their parents.</li>
</ol>
<h2 id="hidden-markov-models">Hidden Markov Models</h2>
<p>The hidden markov model can be viewed as specific instance of the state space model with discrete latent variables. It can also be viewed as an extension of a mixture model in which the choice of mixture component for each observation is not selected independently but depends on the choice of component for the previous observation.</p>
<p>It is convenient to use 1-of-<span class="math inline">\(K\)</span> coding scheme for the latent variables <span class="math inline">\(\mathbf{Z}_n\)</span>. We now allow the probability distribution of <span class="math inline">\(\mathbf{Z}_n\)</span> to depend on the state of previous latent variable <span class="math inline">\(\mathbf{Z}_{n-1}\)</span> through a conditional probability distribution <span class="math inline">\(P(\mathbf{Z}_{n} | \mathbf{Z}_{n-1})\)</span>. Since, we want to express the dependency of each element of <span class="math inline">\(\mathbf{Z}_n\)</span> on each element of <span class="math inline">\(\mathbf{Z}_{n-1}\)</span>, we introduce a <span class="math inline">\(K \times K\)</span> matrix of probabilities that we denote by <span class="math inline">\(\mathbf{A}\)</span>, the element of which are known as transition probabilities:</p>
<p><span class="math display">\[A_{jk} = P(Z_{nk} = 1 | Z_{n-1, j}) = 1\]</span></p>
<p>And because they are probabilities:</p>
<p><span class="math display">\[\sum^K_{k=1} A_{jk} = 1\]</span></p>
<p><span class="math display">\[0 \geq A_{jk} \leq 1\]</span></p>
<p><img src='/images/ML/hmm_4.png' width="600"></p>
<p>We can write the conditional distribution explicitly in the form:</p>
<p><span class="math display">\[P(\mathbf{Z}_{n} | \mathbf{Z}_{n-1}, \mathbf{A}) = \prod^K_{k=1}\prod^{K}_{j=1} A_{jk}^{Z_{nk}Z_{n-1, \;j}}\]</span></p>
<p>The initial latent node <span class="math inline">\(\mathbf{Z}_1\)</span> is special in that it does not have a parent node, and so it has a marginal distribution <span class="math inline">\(P(\mathbf{Z}_1)\)</span> represented by a vector of probabilities that represents the initial probabilities <span class="math inline">\(\boldsymbol{\pi}\)</span>:</p>
<p><span class="math display">\[\pi_k = P(Z_{1k} = 1)\]</span> <span class="math display">\[\sum^{K}_{k=1} \pi_k = 1\]</span> <span class="math display">\[P(\mathbf{Z}_1 | \boldsymbol{\pi}) = \prod^K_{k=1} \pi_k^{Z_{1k}}\]</span></p>
<p>Lastly, the specification of the probabilistic model is completed by defining the conditional distribution of observed variables <span class="math inline">\(P(\mathbf{X}_n | \mathbf{Z}_n, \boldsymbol{\phi})\)</span>, where <span class="math inline">\(\boldsymbol{\phi} = \{\boldsymbol{\phi}_1, ...., \boldsymbol{\phi}_K\}\)</span> is a set of parameters governing the distribution. These distributions are called <code>emissino distribution</code> and might be given by Gaussian if the elements of <span class="math inline">\(\mathbf{X}\)</span> are continues random variables or by conditioanl probability tables if <span class="math inline">\(\mathbf{X}\)</span> are discrete. Since the distribution depends on the values of <span class="math inline">\(\mathbf{Z}\)</span> which has <span class="math inline">\(K\)</span> possible states, We can define the emission distribution as:</p>
<p><span class="math display">\[P(\mathbf{X}_n | \mathbf{Z}_n, \boldsymbol{\phi}) = \prod^K_{k=1} P(\mathbf{X}_n | \boldsymbol{\phi}_k)^{Z_{nk}}\]</span></p>
<p>In this case, we focus on <strong>Homogeneous</strong> models for which all the conditional distributions governing the latent variables share the same parameters <span class="math inline">\(\mathbf{A}\)</span> and similarly all of the emission distributions share the same parameters <span class="math inline">\(\boldsymbol{\phi}\)</span>, so the joint distribution is:</p>
<p><span class="math display">\[P(\mathbf{X}_1, ...., \mathbf{X}_{N}, \mathbf{Z}_1, ...., \mathbf{Z}_{N} | \boldsymbol{\theta}) = P(\mathbf{D}, \mathbf{H} | \boldsymbol{\theta}) = P(\mathbf{Z}_1 | \boldsymbol{\pi}) \prod^N_{n=1} P(\mathbf{X}_n | \mathbf{Z}_n, \boldsymbol{\phi}) \prod^{N}_{n=2} P(\mathbf{Z}_n | \mathbf{Z}_{n-1}, \mathbf{A})\]</span></p>
<p>Where <span class="math inline">\(\boldsymbol{\theta} = \{\boldsymbol{\pi}, \boldsymbol{\phi}, \boldsymbol{A}\}\)</span></p>
<h3 id="maximum-likelihood-for-hmm">Maximum Likelihood for HMM</h3>
<p>If we have observed a dataset <span class="math display">\[\mathbf{D} = \{\mathbf{x}_1, ...., \mathbf{x}_N\}\]</span>, we can determine the parameters of an HMM using maximum likelihood method. The likelihood function is obtained from the joint distribution by marginalizing over the latent variables:</p>
<p><span class="math display">\[L(\boldsymbol{\theta} ;\; \mathbf{D}) = \sum_{\mathbf{H}} P(\mathbf{D}, \mathbf{H} | \; \boldsymbol{\theta}) = P(\mathbf{D} | \; \boldsymbol{\theta}) = \prod^N_{n=1} P(\mathbf{x}_n |\; \boldsymbol{\theta})\]</span></p>
<p>This is similar to the mixture distribution with latent variable in EM, we have a summation inside the log for log likelihood which is much difficult to work with, direct maximization of the log likelihood function will therefore lead to complex expressions with no closed-form solutions. Thus, one way to solve the problem is to use <strong>EM algorithm</strong> to find an efficient framework for maximizing the likelihood function in HMM.</p>
<p>The EM algorithm starts with some initial selection for the model parameters, which we denote by <span class="math inline">\(\boldsymbol{\theta}^{old}\)</span>:</p>
<ol type="1">
<li><strong>E step</strong>:
<ul>
<li>We take these parameter values and find the posterior distribution of the latent variables: <span class="math display">\[P(\mathbf{H} | \mathbf{X}, \boldsymbol{\theta}^{old})\]</span></li>
<li>We then evaluate the expectation of complete log likelihood function over the posterior distribution of the latent variables as a function of new parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>: <span class="math display">\[Q(\boldsymbol{\theta}, \boldsymbol{\theta}^{old}) = E_{\mathbf{H} | \mathbf{D}, \boldsymbol{\theta}^{old}}[\ln P(\mathbf{D}, \mathbf{H} | \boldsymbol{\theta}) | \mathbf{D}, \boldsymbol{\theta}^{old}]\]</span></li>
<li>We can rewrite the log likelihood as:
<span class="math display">\[\begin{aligned}
  Q(\boldsymbol{\theta}, \boldsymbol{\theta}^{old}) &amp;= \sum_{\mathbf{H}} P(\mathbf{H} | \mathbf{D}, \boldsymbol{\theta}^{old}) \ln P(\mathbf{D}, \mathbf{H} | \boldsymbol{\theta})\\
  &amp;= \sum^{K}_{k=1}P(Z_{1k} = 1 | \mathbf{D}, \boldsymbol{\theta}^{old}) \ln P(Z_{1k} = 1| \boldsymbol{\pi}) + \sum^N_{n=2}\sum^{K}_{j=1}\sum^{K}_{k=1} P(Z_{nk}, Z_{n-1, j} | \mathbf{D}, \boldsymbol{\theta}^{old}) \ln P(Z_{nk} | Z_{n-1, j}) + \sum^{N}_{n=1}\sum^{K}_{k=1} P(Z_{nk} = 1 | \mathbf{D}, \boldsymbol{\theta}^{old}) \ln P(\mathbf{X}_n| \boldsymbol{\phi}_k, Z_{nk}=1)\\
  &amp;= \sum^{K}_{k=1} \gamma(Z_{nk})\ln \pi_k + \sum^N_{n=2}\sum^{K}_{j=1}\sum^{K}_{k=1} \xi(Z_{nk}, Z_{n-1, j})\ln A_{jk} + \sum^{N}_{n=1}\sum^{K}_{k=1} \gamma(Z_{nk}) \ln P(\mathbf{X}_n| \boldsymbol{\phi}_k, Z_{nk}=1)
  \end{aligned}\]</span>
Where <span class="math inline">\(\gamma (Z_{nk}) = P(Z_{1k} = 1 | \mathbf{D}, \boldsymbol{\theta}^{old})\)</span> and <span class="math inline">\(\xi(Z_{nk}, Z_{n-1, j}) = P(Z_{nk}, Z_{n-1, j} | \mathbf{D}, \boldsymbol{\theta}^{old}) \ln P(Z_{nk} | Z_{n-1, j})\)</span></li>
<li><strong>Our goal is to evaluate these posterior probabilities <span class="math inline">\(\gamma, \xi\)</span> efficiently</strong></li>
</ul></li>
<li><strong>M step</strong>:
<ul>
<li>We maximize <span class="math inline">\(Q(\boldsymbol{\theta}, \boldsymbol{\theta}^{old})\)</span> w.r.t the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> in which we treat posterior probabilities as constant:</li>
</ul></li>
</ol>
<h1 id="ref">Ref</h1>
<p>PRML Chapter 13</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/09/16/approximate-infer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/09/16/approximate-infer/" class="post-title-link" itemprop="url">approximate_infer</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-09-16 13:55:20 / Modified: 14:22:58" itemprop="dateCreated datePublished" datetime="2021-09-16T13:55:20+08:00">2021-09-16</time>
    </span>

  
    <span id="/2021/09/16/approximate-infer/" class="post-meta-item leancloud_visitors" data-flag-title="approximate_infer" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>20</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>1 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="variational-inference">Variational Inference</h1>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/09/09/graphical-models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/09/09/graphical-models/" class="post-title-link" itemprop="url">Graphical Models</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-09-09 13:17:03" itemprop="dateCreated datePublished" datetime="2021-09-09T13:17:03+08:00">2021-09-09</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-09-13 13:56:59" itemprop="dateModified" datetime="2021-09-13T13:56:59+08:00">2021-09-13</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/09/09/graphical-models/" class="post-meta-item leancloud_visitors" data-flag-title="Graphical Models" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>11 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="graphical-models">Graphical Models</h1>
<p>We can represent complicated probabilistic models using diagrammatic representations of probability distributions called <code>probabilistic graphical models</code>. These offer several useful properties:</p>
<ol type="1">
<li>They provide a simple way to visualize the structure of a probabilistic model and can be used to design and motivate new models.</li>
<li>Insights into the properties of the model, including conditional independence properties can be obtained by inspection of the graph.</li>
<li>Complex computations, required to perform inference and learning in sophisticated models, can be expressed in terms of graphical manipulations, in which underlying mathematical expressions are carried along implicitly.</li>
</ol>
<p><br></p>
<p>A probabilistic graphical model consists of:</p>
<ol type="1">
<li><strong>Nodes</strong>: each random variable (or group of random variables) is represented as a node in the graph</li>
<li><strong>Edges (links)</strong>: links express probabilistic relationship between these random variables.
<ul>
<li><strong>Directed graphical models</strong>: in which the edges of the graphs have a particular directionality indicated by arrows (Bayesian networks). Directed graphs are useful for expressing causal relationships between random variables.</li>
<li><strong>Undirected graphical models</strong>: in which the edges of the graph do not carry arrows and have no directional significance (Markov random fields). Undirected graphs are better suited to expressing soft constraints between random variables.</li>
</ul></li>
</ol>
<p>The graph then captures the way in which the joint distribution over all of the random variables can be decomposed into a product of factors each depending only on a subset of the variables.</p>
<h2 id="bayesian-networks">Bayesian Networks</h2>
<p>Consider first an arbitrary joint distribution defined by <span class="math inline">\(P(\mathbf{Z})\)</span> over random vector <span class="math inline">\(\mathbf{Z} = &lt;A, B, C&gt;\)</span>, by product rule, we have:</p>
<p><span class="math display">\[P(\mathbf{Z}) = P(C| A, B) P(A, B) = P(C | A, B) P(B | A) P(A)\]</span></p>
<p>We now represent the right-hand side in terms of a simgple graphical model as follows:</p>
<ol type="1">
<li>First, we introduce a node for each of the random variables <span class="math inline">\(A, B, C\)</span> and associate each node with the corresponding conditional distribution on the right-hand side.</li>
<li>Then, for each conditional distribution we add directed links to the graph from the nodes to the variables on which the distribution is conditioned.</li>
</ol>
<p><img src='/images/ML/gm_1.png' width="600"></p>
<p>If there is a link going from a node <span class="math inline">\(A\)</span> to a node <span class="math inline">\(B\)</span>, then we say that node <span class="math inline">\(A\)</span> is parent of node <span class="math inline">\(B\)</span> and <span class="math inline">\(B\)</span> is the child of node <span class="math inline">\(A\)</span> (change ordering of the decomposition will change the graph).</p>
<p>We can extend the idea to joint distribution of <span class="math inline">\(K\)</span> random variables given by <span class="math inline">\(P(X_1, ...., X_K)\)</span>. By repeated application of the product rule of the probability, this joint distribution can be written as a product of conditional distributions:</p>
<p><span class="math display">\[P(X_1, ...., X_K) = P(X_K | X_{K-1}, ..., X_{1}) ... P(X_2 | X_1) P(X_1)\]</span></p>
<p>We can generate a graph similar to three-variable case, each node having incoming links from all lower numbered nodes. We say this graph is <code>fully connected</code> because there is a link between every pair of nodes. However, it is the <strong>absence</strong> (not fully connected) of links in the graph that conveys interesting information about the properties of the class of distributions that the graph represents.</p>
<p><img src='/images/ML/gm_2.png' width="600"></p>
<p><br></p>
<p>We can now state in general terms the relationship between a given directed graph and the corresponding distribution over the variables. Thus, for a graph with K nodes <span class="math inline">\(\mathbf{X} = &lt;X_1, ...., X_K&gt;\)</span>, the joint distribution is given by:</p>
<p><span class="math display">\[P(\mathbf{X}) = \prod^K_{k=1} P(X_k | \text{Parent}(X_k))\]</span></p>
<p>Where <span class="math inline">\(\text{Parent}(X_k)\)</span> denotes the set of parents of <span class="math inline">\(X_k\)</span>.</p>
<p>Notice that, the directed graphs that we are considering are subject to an important restriction namely that there must be <strong>no</strong> directed cycles, that is, we are working with <code>directed acyclic graphs</code> or DAGs.</p>
<h3 id="example-generative-models">Example: Generative Models</h3>
<p>There are many situations in which we wish to draw samples from a given probability distribution. One technique which is particularly relevant to graphical models is called <code>ancestral sampling</code>.</p>
<p>Consider a joint distribution <span class="math inline">\(P(\mathbf{X}), \mathbf{X} = &lt;X_1, ...., X_K&gt;\)</span> that factorizes into a DAG. We shall suppose that the variables have been ordered from <span class="math inline">\(X_1\)</span> to <span class="math inline">\(X_K\)</span>, in other words each node has a higher index than any of its parents. Our goal is to draw samples <span class="math inline">\(\hat{X}_1, ..., \hat{X}_K\)</span> from the joint distribution.</p>
<p>To do this, we start from <span class="math inline">\(X_1\)</span>, and draw sample <span class="math inline">\(\hat{X}_1\)</span> from the distribution <span class="math inline">\(P(X_1)\)</span>. We then work through each of the nodes in order, so that for node <span class="math inline">\(n\)</span> we draw a sample from the conditional distribution <span class="math inline">\(P(X_n | \text{Parent}(X_n))\)</span>, in which the parent variables have been set to their sampled values.</p>
<p>To obtain a sample from some marginal distribution corresponding to a subset of the random variables, we simply take the sampled values for the required nodes and discard the rest. For example, to draw a sample from the distribution <span class="math inline">\(P(X_2, X_4)\)</span>, we simply sample from the full joint distribution and then retain the values <span class="math inline">\(\hat{X}_2, \hat{X}_4\)</span> and discard the remaining values.</p>
<p>For practical applications of probabilistic models, it will typically be the higher-numbered variables corresponding to terminal nodes of the graph that represent the observations, with lower-numbered nodes corresponding to latent variables. The primary role of the latent variables is to allow a complicated distribution over the observed variables to tbe represented in terms of a model constructed from simpler conditional distributions.</p>
<blockquote>
<blockquote>
<p>Consider an object recognition task in which each observed data point corresponds to an image of on of the objects (vector of pixels). In this case, we can have latent variables be position and orientation of the object. Given a particular observed image, our goal is to find the posterior distribution over objects in which we integrate over all possible positions and orientations. <img src='/images/ML/gm_3.png' width="600"> Given object, position, orientation, we can sample from the conditional distribution of image and generate pixels.</p>
</blockquote>
</blockquote>
<p>The graphical model captures causal process by which the observed data was generated. For this reason, such models are often called <code>generative models</code>.</p>
<h2 id="conditional-independence">Conditional Independence</h2>
<p>An important concept for probability distributions over multiple variables is that of <strong>conditional independence</strong>. Consider three random variables <span class="math inline">\(A, B, C\)</span> and suppose that the conditional distribution of <span class="math inline">\(A\)</span>, given <span class="math inline">\(B, C\)</span> is such that it does not depend on the value of <span class="math inline">\(B\)</span>, so that:</p>
<p><span class="math display">\[P(A | B, C) = P(A | C)\]</span></p>
<p>Then:</p>
<p><span class="math display">\[P(A, B | C) = P(A | B, C) P(B | C) = P(A | C) P (B | C)\]</span></p>
<p>Thus, we can see that <span class="math inline">\(A, B\)</span> are statistically independent given <span class="math inline">\(C, \; \forall C\)</span>. Note that this definition of conditional independence will require the above equation holds for all values fo <span class="math inline">\(C\)</span> and not just for some values. The shorthand notation for conditional independence is:</p>
<p><span class="math display">\[A \perp \!\!\! \perp B \;|\; C\]</span></p>
<p>An important and elegant feature of graphical models is that conditional independence properties of the joint distribution can be read directly from the graph without having to perform any analytical manipulations. The general framework for achieving this is called <code>d-seperation</code> (d stands for directed).</p>
<h3 id="three-example-graphs">Three example graphs</h3>
<p>We start by illustrating the key concepts of d-separation by three motivating examples.</p>
<ol type="1">
<li><blockquote>
<p><span class="math inline">\(P(A, B, C) = P(A | C) P(B | C) P(C)\)</span> <img src='/images/ML/gm_4.png' width="600"> <span class="math inline">\(A, B\)</span> are generally <strong>not</strong> statistically independent. However, we can easily see that <span class="math inline">\(A, B\)</span> are conditionally independent given <span class="math inline">\(C\)</span>: <span class="math display">\[P(A, B | C) = \frac{P(A, B, C)}{P(C)} = P(A | C) P (B | C)\]</span> <img src='/images/ML/gm_5.png' width="600"></p>
</blockquote></li>
</ol>
<p>We can provide a simple graphical interpretation of this result by considering the path from node <span class="math inline">\(A\)</span> to node <span class="math inline">\(B\)</span> via <span class="math inline">\(C\)</span>. The node <span class="math inline">\(C\)</span> is said to be <strong>tail-to-tail</strong> with respect to this path because the node is connected to the tails of the two arrows. However, when we condition on node <span class="math inline">\(C\)</span> (observed <span class="math inline">\(C\)</span>), the conditional node blocks the path from <span class="math inline">\(A\)</span> to <span class="math inline">\(B\)</span> so causes then to become conditionally independent.</p>
<ol start="2" type="1">
<li><blockquote>
<p><span class="math inline">\(P(A, B, C) = P(B | C) P(C | A) P (A)\)</span> <img src='/images/ML/gm_6.png' width="600"> <span class="math inline">\(A, B\)</span> are generally <strong>not</strong> statistically independent. However, we can easily see that <span class="math inline">\(A, B\)</span> are conditionally independent given <span class="math inline">\(C\)</span> by: <span class="math display">\[P(A, B | C) = \frac{P(A, B, C)}{P(C)} = P(A | C) P (B | C)\]</span></p>
</blockquote></li>
</ol>
<p>We can provide a simple graphical interpretation of this result by considering the path from node <span class="math inline">\(A\)</span> to node <span class="math inline">\(B\)</span> via <span class="math inline">\(C\)</span>. The node <span class="math inline">\(C\)</span> is said to be <strong>head-to-tail</strong> with respect to this path because the node is connected to the head and tail of the two arrows. However, when we condition on node <span class="math inline">\(C\)</span> (observed <span class="math inline">\(C\)</span>), the conditional node blocks the path from <span class="math inline">\(A\)</span> to <span class="math inline">\(B\)</span> so causes then to become conditionally independent.</p>
<ol start="3" type="1">
<li><blockquote>
<p><span class="math inline">\(P(A, B, C) = P(A)P(B)P(C | A, B)\)</span> <img src='/images/ML/gm_7.png' width="600"> We can easily see that <span class="math inline">\(A, B\)</span> are <strong>not</strong> conditionally independent. However, we can see that <span class="math inline">\(A, B\)</span> are statistically independent: <span class="math inline">\(P(A, B) = \sum_{C} P(A)P(B)P(C | A, B) = P(A)P(B)\)</span></p>
</blockquote></li>
</ol>
<p>Thus, our third example has the opposite behaviour from the first two. The node <span class="math inline">\(C\)</span> is said to be <strong>head-to-head</strong> with respect to this path because the node is connected to the heads of the two arrows. When the node <span class="math inline">\(C\)</span> is not given (unobserved), it blocks the path so <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is independent, however, when the node <span class="math inline">\(C\)</span> is given, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> becomes dependent.</p>
<p>There is one more relationship associate with third example. First we say that node <span class="math inline">\(Y\)</span> is a <strong>descendant</strong> of node <span class="math inline">\(X\)</span> if there is a path from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span> in which each step of the path follows the directions of the arrows. Then it can be shown that a <strong>head to head</strong> path will become unblocked if either the node or any of its descendants is observed.</p>
<h3 id="d-separation">D-separation</h3>
<p>Consider a general directed graph in which <span class="math inline">\(A, B, C\)</span> are arbitrary sets of nodes. We wish to ascertain whether a particular conditional independence statement <span class="math inline">\(A \perp \!\!\! \perp B \;|\; C\)</span> is implied by a given directed acyclic graph. To do so, we consider all possible paths from any node in <span class="math inline">\(A\)</span> to any node in <span class="math inline">\(B\)</span>. Any such path is said to be <strong>blocked</strong> if it includes a node such that either:</p>
<ol type="1">
<li>The arrows on the path meet either head-to-tail or tail-to-tail at the node, and the node is in the set <span class="math inline">\(C\)</span>.</li>
<li>The arrows meet head-to-head at the node, and neither the node, nor any of its descendants, is in the set <span class="math inline">\(C\)</span>.</li>
</ol>
<p>If <strong>all</strong> paths are blocked, then <span class="math inline">\(A\)</span> is said to be <code>d-separated</code> from <span class="math inline">\(B\)</span> by <span class="math inline">\(C\)</span>, and the joint distribution over all of the variables in the graph will satisfy <span class="math inline">\(A \perp \!\!\! \perp B \;|\; C\)</span>.</p>
<blockquote>
<p>Consider the problem of finding the posterior distribution for the mean of an univariate Gaussian distribution. This can be represented by the directed graph in which the joint distribution is defined by a prior <span class="math inline">\(P(\mu)\)</span> and <span class="math inline">\(P(\mathbf{X} | \mu)\)</span> to form the posterior distribution: <span class="math display">\[P(\mu | \mathbf{X}) = P(\mu) P(\mathbf{X} | \mu) \]</span> <img src='/images/ML/gm_8.png' width="600"> In practice, we observe <span class="math inline">\(D = \{X_1, ...., X_N\}\)</span> with conditional distribution <span class="math inline">\(P(X_1 | \mu) , ...., P(X_N | \mu)\)</span> respectively, and our goal is to infer <span class="math inline">\(\mu\)</span>. Using d-separation, we note that there is a unique path from any <span class="math inline">\(X_i\)</span> to any other <span class="math inline">\(X_{j\neq i}\)</span> and that this path is tail-to-tail with respect to the observed node <span class="math inline">\(\mu\)</span>. Every such path is blocked and so the observations <span class="math inline">\(D=\{X_1, ..., X_N\}\)</span> are independent given <span class="math inline">\(\mu\)</span>: <span class="math display">\[P(\mathbf{X} | \mu) = \prod^N_{i=1} P(X_i | \mu)\]</span> However, if we do not conditional on <span class="math inline">\(\mu\)</span>, the data samples are not independent: <span class="math display">\[P(\mathbf{X}) = \int_{\mu} P(\mathbf{X} | \mu) P(\mu) \neq \prod^N_{i=1} P(X_i)\]</span></p>
</blockquote>
<h2 id="markov-random-fields">Markov Random Fields</h2>
<p>Directed Graphical models specify a factorization of the joint distribution over a set of variables into a product of local conditional distributions. They also defined a set of conditional independence properties that must be satisfied by any distribution that factorizes according to the graph. A <code>Markove random field</code> has:</p>
<ol type="1">
<li>A set of nodes each of which corresponds to a random variable or group of random variables</li>
<li>A set of links each of which connects a pair of nodes. The links are <strong>undirected</strong> that is they do not carry arrows.</li>
</ol>
<h3 id="conditional-independence-properties">Conditional Independence Properties</h3>
<p>Testing for conditional independence in undirected graph is simpler than in directed graph. Let <span class="math inline">\(A, B, C\)</span> be three sets of nodes and we consider the conditional independence property <span class="math inline">\(A \perp \!\!\! \perp B \;|\; C\)</span>. To test whether this property is satisfied by a probability distribution defined by the graph:</p>
<ul>
<li>Consider all possible paths that connect nodes in set <span class="math inline">\(A\)</span> to nodes in set <span class="math inline">\(B\)</span>. If all such paths pass through one or more nodes in set <span class="math inline">\(C\)</span>, then <strong>all</strong> such paths are <strong>blocked</strong> and so the conditional independence properties holds. If there is <strong>at least one</strong> such path that is not blocked, then there will exist at least some distributions corresponding to the graph that do not satisfy this conditional independence relation.</li>
</ul>
<p><img src='/images/ML/gm_9.png' width="600"></p>
<h3 id="factorization-properties">Factorization Properties</h3>
<p>We now express the joint distribution <span class="math inline">\(P(\mathbf{X})\)</span> as a product of functions defined over set of random variables that are local to the graph.</p>
<p>If we consider two nodes <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> that are not connected by a link, then these variables must be conditionally independent given all other nodes in the graph, because there is no direct path between the two nodes and all other paths are blocked:</p>
<p><span class="math display">\[P(X_i, X_j | \mathbf{X}_{k\notin \{i, j\}}) = P(X_i | \mathbf{X}_{k\notin \{i, j\}}) P(X_j | \mathbf{X}_{k\notin \{i, j\}})\]</span></p>
<p><br></p>
<p>A <code>clique</code> is a subset of nodes in a graph such that there exists a link between all pairs of nodes in the subset. In other words, the nodes in the set are fully connected. Furthermore, a <code>maximal clique</code> is a clique such that it is not possible to include any other nodes from the graph in the set without it ceasing to be a clique.</p>
<p><img src='/images/ML/gm_10.png' width="600"></p>
<p><br></p>
<p>We can therefore define the factors in the decomposition of the joint distribution to be functions of the variables in the cliques. In fact, we can consider functions of the maximal cliques, without loss of generality because other cliques must be subsets of maximal cliques.</p>
<p>Let <span class="math inline">\(C\)</span> be a clique and the set of random variables in that clique by <span class="math inline">\(\mathbf{X}_C\)</span>. Then the joint distribution is written as a product of <code>potential functions</code> <span class="math inline">\(\psi_C(\mathbf{x}_C) \geq 0\)</span> over the maximal cliques of the graph:</p>
<p><span class="math display">\[P(\mathbf{X}) = \frac{1}{Z} \prod_{C} \psi_{C} (\mathbf{X}_C)\]</span></p>
<p>Here the quantity <span class="math inline">\(Z\)</span> is called <code>partition function</code> which is used for normalization to ensure the result is a proper joint distribution:</p>
<p><span class="math display">\[Z = \sum_{X} \prod_{C} \psi_{C} (\mathbf{X}_C)\]</span></p>
<p>In directed graph, we have the links to be conditional distribution, in undirected graph, we do not restrict the choice of potential functions.</p>
<h1 id="ref">Ref</h1>
<p>PRML chapter 8</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/09/06/lgb/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/09/06/lgb/" class="post-title-link" itemprop="url">LGBM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-09-06 14:37:43" itemprop="dateCreated datePublished" datetime="2021-09-06T14:37:43+08:00">2021-09-06</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-09-08 20:48:22" itemprop="dateModified" datetime="2021-09-08T20:48:22+08:00">2021-09-08</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/09/06/lgb/" class="post-meta-item leancloud_visitors" data-flag-title="LGBM" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>4.6k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>4 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="lightgbm-a-highly-efficient-gradient-boosting-decision-tree">LightGBM: A Highly Efficient Gradient Boosting Decision Tree</h1>
<h2 id="background">Background</h2>
<h3 id="gbdt">GBDT</h3>
<p>GBDT is an ensemble model of decision trees, which are trained in sequence. In each iteration, GBDT learns the decision trees by fitting the negative gradients (also known as residual errors).</p>
<p>The main cost in GBDT lies:</p>
<ol type="1">
<li>Learning the decision trees (<strong>finding the best split points</strong>) when there is large number of features and samples.
<ul>
<li><strong>Histogram based algorithm</strong> buckets continuous feature values into discrete bins and uses these bins to construct feature histograms during training. It finds the best split points based on the feature histogram, the criterion is calculated at the interval boundaries. It costs <span class="math inline">\(O(N \times M)\)</span> for histogram building and <span class="math inline">\(O(\text{Number of bins} \times M)\)</span> for split point finding. Since number of bins is much smaller than number of data points, histogram building will dominate the computational complexity. <img src='/images/ML/lgb_1.png' width="600"> <img src='/images/ML/lgb_2.png' width="600"></li>
<li><strong>Pre-sorted algorithm</strong> sorts the values of each numeric attribute, and evaluates the criterion at each possible split point to find the splitting point with the minimum criterion. The sorting requires <span class="math inline">\(O(n\log (n))\)</span>.</li>
</ul></li>
<li><strong>Number of Samples</strong>:
<ul>
<li>Down sampling the data instance (i.e weights, random subsets), most of the algorithms are based on adaboost which has weights but not GBDT which does not have weights natively. Random subsets hurt the performance.</li>
</ul></li>
<li><strong>Number of Features</strong>:
<ul>
<li>PCA to remove weak correlated features (depends on assumption that features contain significant redundancy which might not always be true in practice)</li>
</ul></li>
</ol>
<h3 id="gradient-based-one-side-sampling">Gradient-based One-Side Sampling</h3>
<p>In AdaBoost, the sample weight serves as a good indicator for the importance of data instances. In GBDT, gradient for each data instance provides us with useful information for data sampling. That is, if an instance is associated with a small gradient, the training error for this instance is small and it is already well-trained. A straightforward idea is to discard those data instances with small gradients. However, <strong>the data distribution will be changed by doing so</strong>. To avoid this problem, GOSS keeps all the instances with large gradients and performs random sampling on the instances with samll gradients.</p>
<p>In order to compensate the influence to the data distribution, when computing the information gain, GOSS introduces a constant multiplier for the data instances with samll gradients. Specifically, GOSS:</p>
<ol type="1">
<li>Firstly sorts the data instances according to the absolute value of their gradients.</li>
<li>Selects the top <span class="math inline">\(a \times 100%\)</span> instances.</li>
<li>Then it randomly samples <span class="math inline">\(b \times 100%\)</span> from the rest of the data.</li>
<li>Amplifies the sampled data with small gradients by a constant <span class="math inline">\(\frac{1 - a}{b}\)</span> when calculating the criterion to normalize the sum of the gradients.</li>
</ol>
<p><img src='/images/ML/lgb_3.png' width="600"></p>
<h3 id="exclusive-feature-bundling">Exclusive Feature Bundling</h3>
<p>High-dimensional data are usually very sparse. The sparsity of the feature space provides us a possibility of designing a nearly lossless approach to reduce the number of features. Specifically, in a sparse feature space, many features are mutually exclusive (they never take nonzero values simultaneously), we can safely bundle these features into a single feature. In this way, the complexity of histogram building changes from <span class="math inline">\(O(N \times M)\)</span> to <span class="math inline">\(O(N \times \text{Number of bundles})\)</span>. Then we can significantly speed up the training of GBDT without hurting the accuracy.</p>
<h4 id="which-features-to-bundle">Which Features to Bundle</h4>
<p>Partitioning features into smallest number of exclusive bundles is NP-hard, thus it is impossible to find an exact solution within polynomial time. Thus, a greedy algorithm which can produce reasonable good results are being used. Furthermore, we can allow a small fraction of conflicts which is controlled by <span class="math inline">\(\gamma\)</span> (there are usually quite a few features, although not 100% mutually exclusive, also rarely take nonzero values simultaneously) to have an even smaller number of feature bundles and further improve the computational efficiency. For small <span class="math inline">\(\gamma\)</span>, we will be able to achieve a good balance between accuracy and efficiency.</p>
<p>Intuitively, it:</p>
<ol type="1">
<li>Builds a graph with features as vertices and weighted edges as total conflicts between features (Edge only occurs when two features are not mutually exclusive).</li>
<li>Sort the features by their degrees (Number of edges)</li>
<li>Check each feature in the ordered list and either append it to existing list (if total conflicts between feature and bundle less than <span class="math inline">\(\gamma\)</span>) or create a new bundle.</li>
</ol>
<p><img src='/images/ML/lgb_4.png' width="600"></p>
<p><br></p>
<p>The time complexity of algorithm 3 is <span class="math inline">\(O(M^2)\)</span> and it is only processed once before training. This complexity is acceptable when the number of features is not very large.</p>
<h4 id="how-to-construct-the-bundle">How to Construct the Bundle</h4>
<p>The key is to ensure that the values of the original features can be identified from the feature bundles. This can be done by adding offsets to the original values of the features:</p>
<ol type="1">
<li>Calculate the offset to be added to every feature in feature bundle.</li>
<li>Iterate over every data instance and feature.</li>
<li>Initialize the new bucket as zero for instances where all features are zero.</li>
<li>Calculate the new bucket for every non zero instance of a feature by adding respective offset to original bucket of that feature</li>
</ol>
<p><img src='/images/ML/lgb_5.png' width="600"> <img src='/images/ML/lgb_6.png' width="600"></p>
<h1 id="ref">Ref</h1>
<p>https://www.researchgate.net/publication/351133481_Comparison_of_Gradient_Boosting_Decision_Tree_Algorithms_for_CPU_Performance</p>
<p>https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf</p>
<p>https://robotenique.github.io/posts/gbm-histogram/</p>
<p>https://www.aaai.org/Papers/KDD/1998/KDD98-001.pdf</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/08/20/topology/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/20/topology/" class="post-title-link" itemprop="url">topology</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-08-20 21:58:54" itemprop="dateCreated datePublished" datetime="2021-08-20T21:58:54+08:00">2021-08-20</time>
    </span>

  
    <span id="/2021/08/20/topology/" class="post-meta-item leancloud_visitors" data-flag-title="topology" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>0</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>1 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/08/09/roc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/09/roc/" class="post-title-link" itemprop="url">ROC</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-08-09 16:46:50 / Modified: 17:27:36" itemprop="dateCreated datePublished" datetime="2021-08-09T16:46:50+08:00">2021-08-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/08/09/roc/" class="post-meta-item leancloud_visitors" data-flag-title="ROC" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>2.4k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>2 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="roc-curve">ROC Curve</h1>
<p><code>ROC Curve</code> is a graph showing the performance of a classification model at all classification thresholds (e.g for logistic regression the threshold is default 0.5, we can adjust this threshold to 0.8 and assign examples to classes using this threhold). This curve plots two parameters:</p>
<ul>
<li>True Positive Rate (<span class="math inline">\(\text{sensitivity} = \frac{\text{True Positive}}{\text{True Positive + False Negative}}\)</span>)</li>
<li>False Positive Rate (<span class="math inline">\(1 - \text{specificity} = 1 -\frac{\text{True Negative}}{\text{True Negative + False Positive}} = \frac{\text{False Positive}}{\text{True Negative + False Positive}}\)</span>)</li>
</ul>
<p>An ROC curve plots TRP vs FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive (e.g threshold 0.1 for logistic regression), thus increasing both FP and TP.</p>
<p><img src='/images/ML/roc_1.png' width="600"></p>
<p>To compute the points an ROC curve, we could evaluate a classification model many times with different threshold and repeat this for all thresholds, but this is inefficient. Fortunately, there's an effective algorithm that can provide this information called <code>AUC</code>.</p>
<h2 id="auc">AUC</h2>
<p>AUC stands for <strong>Area under ROC curve</strong>. It measures the entire two-dimensional area underneath the entire ROC curve. Since <span class="math inline">\(TPR \in [0, 1]\)</span> and <span class="math inline">\(FPR \in [0, 1]\)</span>, the AUC can be interpreted as probability. It is the probability that the model ranks a random positive example more highly than a random negative example. Thus, by rearranging the predictions from left to right, AUC is the probability that a random positive example is positioned to the right of a random negative example:</p>
<p><img src='/images/ML/roc_2.png' width="600"></p>
<p>A model whose predictions are 100% wrong will have AUC 0, a model whose predictions are 100% correct has an AUC of 1.</p>
<p><br></p>
<h3 id="properties">Properties</h3>
<p>AUC is desirable for the following two reasons:</p>
<ul>
<li><strong>Scale-invariant</strong>: It measures hwo well predictions are ranked, rather than their absolute values.</li>
<li><strong>Classification-threshold-invariant</strong>: It measures the quality of the model's predictions irrespective of what classification threshold is chosen. (when we draw ROC, we are varying the threshold, the only thing matters to AUC is the predicted values (not class) and how true positive class samples ranked against negative class samples w.r.t their predicted values)</li>
</ul>
<p>However, AUC is not desirable when:</p>
<ol type="1">
<li><strong>Sometimes probabilities matters</strong>, AUC does not tell you how good is a predicted probability.</li>
<li><strong>Sometimes classification threshold matters</strong>, In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn't a useful metric for this type of optimization.</li>
</ol>
<h1 id="ref">Ref</h1>
<p>https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>
<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhu, Zhaoyang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Symbols count total">690k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">10:27</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.3" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{&quot;enable&quot;:true,&quot;app_id&quot;:&quot;gPOc95wWW2Hwp2pkVSAAz28m-MdYXbMMI&quot;,&quot;app_key&quot;:&quot;7Sf65xCHXfEEdBvu29UHSYdV&quot;,&quot;server_url&quot;:&quot;https:&#x2F;&#x2F;swag1ong.github.io&quot;,&quot;security&quot;:true}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{&quot;enable&quot;:true,&quot;repo&quot;:&quot;swag1ong&#x2F;swag1ong.github.io&quot;,&quot;issue_term&quot;:&quot;pathname&quot;,&quot;theme&quot;:&quot;github-light&quot;,&quot;pathname&quot;:&quot;pathname&quot;}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
