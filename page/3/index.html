<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="1xbHmVEsS5Fta14x1d1FXkHFr93LRr1pMxipdYVcNI4">
  <meta name="baidu-site-verification" content="code-Srzz2vTuzA">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;swag1ong.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;Searching...&quot;,&quot;empty&quot;:&quot;We didn&#39;t find any results for the search: ${query}&quot;,&quot;hits_time&quot;:&quot;${hits} results found in ${time} ms&quot;,&quot;hits&quot;:&quot;${hits} results found&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:3,&quot;unescape&quot;:false,&quot;preload&quot;:true}}</script>
<meta property="og:type" content="website">
<meta property="og:title" content="GoGoGogo!">
<meta property="og:url" content="https://swag1ong.github.io/page/3/index.html">
<meta property="og:site_name" content="GoGoGogo!">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhu, Zhaoyang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://swag1ong.github.io/page/3/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:true,&quot;isPost&quot;:false,&quot;lang&quot;:&quot;en&quot;,&quot;comments&quot;:&quot;&quot;,&quot;permalink&quot;:&quot;&quot;,&quot;path&quot;:&quot;page&#x2F;3&#x2F;index.html&quot;,&quot;title&quot;:&quot;&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>GoGoGogo!</title><script src="/js/config.js"></script>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?852b005027c5efa81663f6f5c4c5b7fd"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">GoGoGogo!</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-igloo fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">109</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">16</span></a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">25</span></a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-address-card fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhu, Zhaoyang"
      src="/images/others/favicon.jpeg">
  <p class="site-author-name" itemprop="name">Zhu, Zhaoyang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">109</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/swag1ong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;swag1ong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyang.zhu@mail.utoronto.ca" title="E-Mail → mailto:zhaoyang.zhu@mail.utoronto.ca" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zhuzhaoyangzzyu0616" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zhuzhaoyangzzyu0616" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2022/01/01/real-analysis-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/01/real-analysis-3/" class="post-title-link" itemprop="url">Real Analysis (3)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-01-01 10:33:24" itemprop="dateCreated datePublished" datetime="2022-01-01T10:33:24+08:00">2022-01-01</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2022-03-06 11:19:30" itemprop="dateModified" datetime="2022-03-06T11:19:30+08:00">2022-03-06</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Background/" itemprop="url" rel="index"><span itemprop="name">Background</span></a>
        </span>
    </span>

  
    <span id="/2022/01/01/real-analysis-3/" class="post-meta-item leancloud_visitors" data-flag-title="Real Analysis (3)" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>14k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>13 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="real-analysis-3">Real Analysis (3)</h1>
<h2 id="basic-topology-of-mathbbr">Basic Topology of <span class="math inline">\(\mathbb{R}\)</span></h2>
<h3 id="open-and-closed-sets">Open and Closed Sets</h3>
<h4 id="definition-3.2.1-open">Definition 3.2.1: Open</h4>
<p>A set <span class="math inline">\(O \subseteq \mathbb{R}\)</span> is <strong>open</strong> if for all points <span class="math inline">\(a \in O\)</span> there exists an <span class="math inline">\(\epsilon-neighborhood V_{\epsilon} (a) \subseteq O\)</span>.</p>
<blockquote>
<p><span class="math inline">\(\mathbb{R}\)</span> is open <span class="math inline">\(\emptyset\)</span> is open</p>
</blockquote>
<h4 id="theorem-3.2.3">Theorem 3.2.3</h4>
<ol type="1">
<li>The union of an arbitrary collection of open sets is open.</li>
<li>The intersection of a finite collection of open sets is open.</li>
</ol>
<h4 id="definition-3.2.4-limit-point">Definition 3.2.4: Limit Point</h4>
<p>A point <span class="math inline">\(x\)</span> is a <strong>limit point</strong> of a set <span class="math inline">\(A\)</span> if every <span class="math inline">\(\epsilon-neighborhood V_{\epsilon} (x)\)</span> of <span class="math inline">\(x\)</span> intersects the set <span class="math inline">\(A\)</span> at some point other than <span class="math inline">\(x\)</span>. Limit points may not be in <span class="math inline">\(A\)</span>, consider the end points of open sets.</p>
<h4 id="theorem-3.2.5">Theorem 3.2.5</h4>
<p>A point <span class="math inline">\(x\)</span> is a <strong>limit point</strong> of a set <span class="math inline">\(A\)</span> if and only if <span class="math inline">\(x = \lim_{n \rightarrow \infty} a_n\)</span> for some sequence <span class="math inline">\((a_n)\)</span> contained in <span class="math inline">\(A\)</span> satisfying <span class="math inline">\(a_n \neq x, \; \forall n \in \mathbb{N}\)</span></p>
<h4 id="definition-3.2.6-isolated-point">Definition 3.2.6: Isolated Point</h4>
<p>A point <span class="math inline">\(a \in A\)</span> is an <strong>isolated point</strong> of <span class="math inline">\(A\)</span> if it is not a limit point of <span class="math inline">\(A\)</span>. Isolated point is always in <span class="math inline">\(A\)</span>.</p>
<h4 id="definition-3.2.7-closed-set">Definition 3.2.7: Closed Set</h4>
<p>A set <span class="math inline">\(F \subseteq \mathbb{R}\)</span> is <strong>closed</strong> if it contains all its limit points. Topologically speaking, a closed set is one where convergent sequences within the set have limits that are also in the set.</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/01/01/real-analysis-3/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2022/01/01/real-analysis-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/01/real-analysis-2/" class="post-title-link" itemprop="url">Real Analysis (2)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-01-01 10:31:28" itemprop="dateCreated datePublished" datetime="2022-01-01T10:31:28+08:00">2022-01-01</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2022-03-06 11:19:37" itemprop="dateModified" datetime="2022-03-06T11:19:37+08:00">2022-03-06</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Background/" itemprop="url" rel="index"><span itemprop="name">Background</span></a>
        </span>
    </span>

  
    <span id="/2022/01/01/real-analysis-2/" class="post-meta-item leancloud_visitors" data-flag-title="Real Analysis (2)" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>7.8k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>7 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="real-analysis-2">Real Analysis (2)</h1>
<h2 id="sequence">Sequence</h2>
<h3 id="the-limit-of-sequence">The Limit of Sequence</h3>
<h4 id="definition-2.2.1-definition-of-sequence">Definition 2.2.1: Definition of Sequence</h4>
<p>A <strong>sequence</strong> is a function whose domain is <span class="math inline">\(\mathbb{N}\)</span>. Given a function <span class="math inline">\(f: \mathbb{N} \rightarrow \mathbb{R}\)</span>, <span class="math inline">\(f(n)\)</span> is just the <span class="math inline">\(n\)</span>th term on the list.</p>
<blockquote>
<p><span class="math inline">\((1, \frac{1}{2}, \frac{1}{3}...)\)</span> is a sequence: <span class="math display">\[f(n) = \{n \in \mathbb{N}: \frac{1}{n}\}\]</span></p>
</blockquote>
<h4 id="definition-2.2.3-convergence-of-a-sequence">Definition 2.2.3: Convergence of a Sequence</h4>
<p>A sequence <span class="math inline">\((a_n)\)</span> converges to a real number <span class="math inline">\(a\)</span> if, for every positive number <span class="math inline">\(\epsilon\)</span>, there exists an <span class="math inline">\(N \in \mathbb{N}\)</span> such that whenever <span class="math inline">\(n \geq \mathbb{N}\)</span>, it follows that <span class="math inline">\(|a_n - a| &lt; \epsilon\)</span>.</p>
<p><span class="math display">\[\lim_{n \rightarrow \infty} a_n = a\]</span></p>
<h4 id="definition-2.2.4">Definition 2.2.4</h4>
<p>Given a real number <span class="math inline">\(a \in \mathbb{R}\)</span> and a positive number <span class="math inline">\(\epsilon &gt; 0\)</span>, the set:</p>
<p><span class="math display">\[V_{\epsilon} (a) = \{x \in \mathbb{R}: |x - a| &lt; \epsilon\}\]</span></p>
<p>is called the <span class="math inline">\(\boldsymbol{\epsilon-neighborhood}\)</span> of <span class="math inline">\(a\)</span>. In other words, <span class="math inline">\(V_{\epsilon} (a)\)</span> is an interval, centered at <span class="math inline">\(a\)</span>, with radius <span class="math inline">\(\epsilon\)</span>.</p>
<h4 id="definition-2.2.3b">Definition 2.2.3B</h4>
<p>A sequence <span class="math inline">\((a_n)\)</span> converges to <span class="math inline">\(a\)</span> if, given any <span class="math inline">\(\epsilon-neighborhood V_{\epsilon}(a)\)</span> of <span class="math inline">\(a\)</span>, there exists a point in the sequence after which all of the terms are in <span class="math inline">\(V_{\epsilon} (a)\)</span>.</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/01/01/real-analysis-2/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/12/25/real-analysis-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/25/real-analysis-1/" class="post-title-link" itemprop="url">Real Analysis (1)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-25 18:14:23" itemprop="dateCreated datePublished" datetime="2021-12-25T18:14:23+08:00">2021-12-25</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2022-03-06 11:19:41" itemprop="dateModified" datetime="2022-03-06T11:19:41+08:00">2022-03-06</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Background/" itemprop="url" rel="index"><span itemprop="name">Background</span></a>
        </span>
    </span>

  
    <span id="/2021/12/25/real-analysis-1/" class="post-meta-item leancloud_visitors" data-flag-title="Real Analysis (1)" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>9.1k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>8 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="real-analysis-1">Real Analysis (1)</h1>
<h2 id="preliminaries">Preliminaries</h2>
<h3 id="sets">Sets</h3>
<p>If an element <span class="math inline">\(x\)</span> is in a set <span class="math inline">\(A\)</span>, we write:</p>
<p><span class="math display">\[x \in A\]</span></p>
<p>and say that <span class="math inline">\(x\)</span> is a <strong>member</strong> of <span class="math inline">\(A\)</span>, or that <span class="math inline">\(x\)</span> belongs to <span class="math inline">\(A\)</span>. If <span class="math inline">\(x\)</span> is not in <span class="math inline">\(A\)</span>, we write:</p>
<p><span class="math display">\[x \notin A\]</span></p>
<p>If every element of a set <span class="math inline">\(A\)</span> also belongs to a set <span class="math inline">\(B\)</span>, we say that <span class="math inline">\(A\)</span> is a <strong>subset</strong> of <span class="math inline">\(B\)</span> and write:</p>
<p><span class="math display">\[A \subseteq B\]</span></p>
<p>We say that <span class="math inline">\(A\)</span> is a <strong>proper subset</strong> of <span class="math inline">\(B\)</span> if <span class="math inline">\(A \subset B\)</span>, but there is at least one element of <span class="math inline">\(B\)</span> that is not in <span class="math inline">\(A\)</span>. In this case we write:</p>
<p><span class="math display">\[A \subset B\]</span></p>
<h4 id="definition-1.11-equal-sets">Definition 1.11: Equal Sets</h4>
<p>Two sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are said to be <strong>equal</strong>, we write <span class="math inline">\(A = B\)</span> if they contain the same elements. Thus, to prove that sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are equal, we must show that:</p>
<p><span class="math display">\[A \subseteq B, \;\; B \subseteq A\]</span></p>
<p>A set is normally defined by either listing its elements explicitly, or by specifying a property that determines the elements of the set. If <span class="math inline">\(P\)</span> denotes a property that is meaningful and unambiguous for elements of a set <span class="math inline">\(S\)</span>, then we write:</p>
<p><span class="math display">\[\{x \in S; P(x)\}\]</span></p>
<p>for the set of all elements <span class="math inline">\(x\)</span> in <span class="math inline">\(S\)</span> for which the property <span class="math inline">\(P\)</span> is true.</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/12/25/real-analysis-1/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/12/25/transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/25/transformer/" class="post-title-link" itemprop="url">Transformer</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-12-25 18:08:16 / Modified: 18:19:07" itemprop="dateCreated datePublished" datetime="2021-12-25T18:08:16+08:00">2021-12-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/" itemprop="url" rel="index"><span itemprop="name">DL</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/CNN/" itemprop="url" rel="index"><span itemprop="name">CNN</span></a>
        </span>
    </span>

  
    <span id="/2021/12/25/transformer/" class="post-meta-item leancloud_visitors" data-flag-title="Transformer" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>3.8k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>3 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="attention-is-all-you-need">Attention is all you need</h1>
<p><img src="/images/ML/transformer_2.jpg" align="center" width=600></p>
<p><img src="/images/ML/transformer_4.jpg" align="center" width=600></p>
<h2 id="structure">Structure</h2>
<h3 id="encoder">Encoder</h3>
<p>The encoder consists of 6 identical layers. Each layer consists of two sub-layers: 1. Multihead self-attention 2. Feed forward fully connected network</p>
<p>residual connection (skip connection) are employed around each of the two sub-layers followed by layer normalization. The output of each sub-layer is:</p>
<p><span class="math display">\[\text{LayerNorm}(x + \text{Sublayer}(x))\]</span></p>
<p>Where <span class="math inline">\(\text{Sublayer}(x)\)</span> is the function implemented by the sub-layer itself. All sub-layers and embedding layers have output of dimension <span class="math inline">\(d_{model} = 512\)</span></p>
<h3 id="decoder">Decoder</h3>
<p>The decoder consists of 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack as key, value and the previous decoder output as query. The first self-attention sub-layer is modified to mask the future during training, this ensures that the predictions for position <span class="math inline">\(i\)</span> can depend only on the known outputs at positions less than <span class="math inline">\(i\)</span>.</p>
<h2 id="self-attention">Self Attention</h2>
<p>Given input <span class="math inline">\(X \in \mathbb{R}^{n_b \times n_s \times d_{model}}\)</span> and trainable parameters <span class="math inline">\(W_q \in \mathbb{R}^{d_{model} \times d_{k}}, W_k \in \mathbb{R}^{d_{model} \times d_{k}}, W_v \in \mathbb{R}^{d_{model} \times d_{v}}\)</span>, matrices query <span class="math inline">\(Q \in \mathbb{R}^{n_b \times n_s \times d_k}\)</span>, key <span class="math inline">\(K \in \mathbb{R}^{n_b \times n_s \times d_k}\)</span>, value <span class="math inline">\(V \in \mathbb{R}^{n_b \times n_s \times d_v}\)</span> are defined as:</p>
<p><span class="math display">\[Q = XW_q, \;\; K = XW_k, \;\; V = XW_v\]</span></p>
<h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3>
<p><img src="/images/ML/transformer_1.jpg"></p>
<p><span class="math display">\[\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}}) V\]</span></p>
<p>The scale is used to prevent the large magnitude of the dot product so that the gradient of softmax vanishes.</p>
<h3 id="multi-head-self-attention">Multi-head Self Attention</h3>
<p><img src="/images/ML/transformer_3.jpg"></p>
<p>Instead of using a single attention function, <span class="math inline">\(h\)</span> of parallel attention functions with different <span class="math inline">\(W^q_i, W^k_i, W^v_i\)</span> are used and concatenated into single self attention matrix. The final self attention matrix is then multiplied by a weight matrix <span class="math inline">\(W_o \in \mathbb{R}^{hd_v \times d_{model}}\)</span>:</p>
<p><span class="math display">\[\text{MultiHead} (Q, K, V) = \text{Concat} (\text{head}_1, ...., \text{head}_h) W_o\]</span></p>
<p>Where</p>
<p><span class="math display">\[\text{head}_i = \text{Attention}(QW^Q_i, KW^k_i, VW^V_i)\]</span></p>
<p><strong>In this paper <span class="math inline">\(h = 8, d_{model} = 512, d_{model} / h = d_k = d_v = 64\)</span> and in a multi-head self-attention layer, all the keys, values and queries come from same place which is the output of the previous layer in the encoder. (i.e <span class="math inline">\(Q = K = V = X\)</span> where <span class="math inline">\(X\)</span> is the output from previous layer)</strong></p>
<h3 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</h3>
<p>In addition to attention sub-layers, each of the layers in the encoder and decoder contains a fully connected feed-forward network. This consists of two linear transformations with a ReLU activation in between:</p>
<p><span class="math display">\[\text{FFN}(\mathbf{x}) = max(0, \mathbf{x} W_1 + \mathbf{b}_1) W_2 + \mathbf{b}_2\]</span></p>
<p>The parameters are different for different layers. The dimensionality of input and output is <span class="math inline">\(d_{model} = 512\)</span>, and the inner-layer has dimensionality <span class="math inline">\(d_{ff} = 2048, W_1 \in \mathbb{R}^{502 \times 2048}, W_2 \in \mathbf{2048 \times 502}\)</span></p>
<h3 id="positional-encoding">Positional Encoding</h3>
<p>Since the model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. Thus, we add <strong>positional encodings</strong> to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension <span class="math inline">\(d_{model}\)</span> as the embeddings, so that the two can be summed:</p>
<p><span class="math display">\[PE_{(pos, 2i)} = \sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\]</span> <span class="math display">\[PE_{(pos, 2i+1)} = \cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\]</span></p>
<p>Where <span class="math inline">\(i\)</span> is the dimension, <span class="math inline">\(pos\)</span> is the position. For example <span class="math inline">\(PE(1) = [\sin(\frac{1}{10000^{\frac{0}{d_{model}}}}), \cos(\frac{1}{10000^{\frac{2}{d_{model}}}}), \sin(\frac{1}{10000^{\frac{4}{d_{model}}}}) ....]\)</span></p>
<p>It has several properties:</p>
<ol type="1">
<li>For each time-step, it outputs a unique encoding.</li>
<li>The distance between two time-steps is consistent across sentences with different lengths.</li>
<li>Deterministic</li>
<li><span class="math inline">\(PE_{pos+k}\)</span> can be represented linearly using <span class="math inline">\(PE_{pos}\)</span>, so it generalizes easily to unseen length sequences.</li>
</ol>
<h2 id="ref">Ref</h2>
<p>https://jalammar.github.io/illustrated-transformer/</p>
<p>https://theaisummer.com/self-attention/</p>
<p>https://zhuanlan.zhihu.com/p/98641990</p>
<p>https://kazemnejad.com/blog/transformer_architecture_positional_encoding/#the-intuition</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/12/25/tcn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/25/tcn/" class="post-title-link" itemprop="url">TCN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-12-25 18:07:52 / Modified: 18:17:46" itemprop="dateCreated datePublished" datetime="2021-12-25T18:07:52+08:00">2021-12-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/" itemprop="url" rel="index"><span itemprop="name">DL</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/CNN/" itemprop="url" rel="index"><span itemprop="name">CNN</span></a>
        </span>
    </span>

  
    <span id="/2021/12/25/tcn/" class="post-meta-item leancloud_visitors" data-flag-title="TCN" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>2.2k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>2 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="temporal-convolutional-networks">Temporal Convolutional Networks</h1>
<p>Characteristics of TCN:</p>
<ol type="1">
<li>The convolutions in the architecture are causal.</li>
<li>The architecture can take a sequence of any length and map it to an output sequence of same length.</li>
<li>The ability of long term memories using a combination of very deep networks (with residual layers) and dilated convolutions.</li>
</ol>
<h2 id="fully-convolutional-network">Fully Convolutional Network</h2>
<p>Fully convolutional networks replace the fully connected layers with convolution layers. The TCN uses a <span class="math inline">\(1D\)</span> fully-convolutional network architecture, where each hidden layer is the same length as the input layer and zero padding of length (kernel - 1) in the front of the sequence is added to keep subsequent layers the same length as previous ones.</p>
<h2 id="causal-convolutions">Causal Convolutions</h2>
<p>To achieve the causality, the TCN uses causal convolutions, convolutions where an output at time <span class="math inline">\(t\)</span> is convolved only with elements from time <span class="math inline">\(t\)</span> and earlier in the previous layer. Thus:</p>
<p><span class="math display">\[\text{TCN} = 1D\text{FCN} + \text{causal convolutions}\]</span></p>
<p>The major disadvantage of this design is that we need an extremely deep network or very large filters to achieve a long effective history size.</p>
<h2 id="dilated-convolutions">Dilated Convolutions</h2>
<p>The solution is to introduce dilated convolutions:</p>
<p><span class="math display">\[F_d(\mathbf{x}, s) = \sum^{k-1}_{i=0} f(i) \cdot \mathbf{x}_{s - d\cdot i}\]</span></p>
<p>Where <span class="math inline">\(f(i)\)</span> is the <span class="math inline">\(i\)</span>th component of the 1D filter with length <span class="math inline">\(k - 1, i=0, ...., k-1\)</span> and <span class="math inline">\(d\)</span> is the dilation factor, <span class="math inline">\(k\)</span> is the filter size, and <span class="math inline">\(s - d \cdot i\)</span> accounts for the direction of the past. Using large dilation enables an output at the top level to represent a wider range of inputs, thus effectively expanding the receptive field of a ConvNet. Thus, we can choose to: 1. Larger filter size <span class="math inline">\(k\)</span>. 2. Choose larger Dilation factor <span class="math inline">\(d\)</span>. Usually it is increased exponentially with the depth of the network <span class="math inline">\(d = 2^i\)</span> at level <span class="math inline">\(i\)</span> of the network. This ensures that there is some filter that hits each input within the effective history, while also allowing for an extremely large effective using deep networks.</p>
<p><img src="/images/ML/tcn_1.jpg"></p>
<h2 id="residual-connections">Residual Connections</h2>
<p>To allow faster learning , a residual block is introduced with weight norm and dropout inbetween to replace the convolution layer:</p>
<p><img src="/images/ML/tcn_2.jpg"></p>
<p>In TCN, the input and output of the residual block could have different widths (channels), to account for discrepant input-output widths, we use an additional <span class="math inline">\(1 \times 1\)</span> convolution to ensure that elementwise addition will work.</p>
<h2 id="advantages-of-tcn">Advantages of TCN</h2>
<ol type="1">
<li><strong>Parallelism</strong>:</li>
<li><strong>Flexible Receptive Field Size</strong></li>
<li><strong>Stable Gradients</strong></li>
<li><strong>Low Memory Requirement for Training</strong></li>
<li><strong>Variable length inputs</strong></li>
</ol>
<h2 id="disadvantages-of-tcn">Disadvantages of TCN</h2>
<ol type="1">
<li><strong>Data storage during evaluation</strong>:</li>
<li><strong>Potential parameter change for a transfer of domain</strong>:</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/11/19/time-series-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/11/19/time-series-2/" class="post-title-link" itemprop="url">Time Series (2)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-11-19 13:10:49" itemprop="dateCreated datePublished" datetime="2021-11-19T13:10:49+08:00">2021-11-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-11-29 15:08:03" itemprop="dateModified" datetime="2021-11-29T15:08:03+08:00">2021-11-29</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Background/" itemprop="url" rel="index"><span itemprop="name">Background</span></a>
        </span>
    </span>

  
    <span id="/2021/11/19/time-series-2/" class="post-meta-item leancloud_visitors" data-flag-title="Time Series (2)" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>22k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>20 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="time-series-arima">Time Series (ARIMA)</h1>
<h2 id="autoregressive-moving-average-models-arma">Autoregressive Moving Average Models (ARMA)</h2>
<h3 id="autoregressive-models">Autoregressive Models</h3>
<p>Autoregressive models are based on the idea that the current value of the series <span class="math inline">\(X_t\)</span> can be explained as a function of <span class="math inline">\(p\)</span> past values, <span class="math inline">\(X_{t-1}, X_{t-2}, ..., X_{t-p}\)</span> where <span class="math inline">\(p\)</span> determines the number of steps into the past needed to forecast the current value.</p>
<p>An autoregressive model of order <span class="math inline">\(p\)</span>, abbreviated <span class="math inline">\(AR(p)\)</span> with <span class="math inline">\(E[X_t] = 0\)</span>, is of the form:</p>
<p><span class="math display">\[X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + ... + \phi_p X_{t-p} + W_t\]</span></p>
<p>Where <span class="math inline">\(X_t\)</span> is stationary, <span class="math inline">\(\phi_1, ..., \phi_p \neq 0\)</span> are constants, <span class="math inline">\(W_t\)</span> is a Gaussian white noise series with mean zero and variance <span class="math inline">\(\sigma^2_w\)</span>. <strong>We assume above equation <span class="math inline">\(X_t\)</span> has mean zero</strong>, if it has non zero mean <span class="math inline">\(\mu\)</span>, we can replace it by:</p>
<p><span class="math display">\[X_t - \mu = \phi(X_{t-1} - \mu) + \phi_2(X_{t-2} - \mu) + ... + \phi_p (X_{t-p} - \mu) + W_t\]</span> <span class="math display">\[\implies X_t = \alpha + \phi_1 X_{t-1} + \phi_2 X_{t-2} + ... + \phi_p X_{t-p} + W_t\]</span></p>
<p>Where <span class="math inline">\(\alpha = \mu(1 - \phi_1 - ... - \phi_p)\)</span>.</p>
<p>We can also use the backshift operator to rewrite the zero mean <span class="math inline">\(AR(p)\)</span> process as:</p>
<p><span class="math display">\[(1 - \phi_1 B - \phi_2 B^2 - ... - \phi_p B^p) X_t = W_t\]</span></p>
<p>or using <strong>autoregressive operator</strong>:</p>
<p><span class="math display">\[\phi(B)X_t = W_t\]</span></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/11/19/time-series-2/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/11/15/time-series-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/11/15/time-series-1/" class="post-title-link" itemprop="url">Time Series (1)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-11-15 13:10:49" itemprop="dateCreated datePublished" datetime="2021-11-15T13:10:49+08:00">2021-11-15</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-11-19 15:19:43" itemprop="dateModified" datetime="2021-11-19T15:19:43+08:00">2021-11-19</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Background/" itemprop="url" rel="index"><span itemprop="name">Background</span></a>
        </span>
    </span>

  
    <span id="/2021/11/15/time-series-1/" class="post-meta-item leancloud_visitors" data-flag-title="Time Series (1)" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>11 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="time-series-introduction">Time Series (Introduction)</h1>
<h2 id="characteristics-of-time-series">Characteristics of Time Series</h2>
<p>The primary objective of time series analysis is to develop mathematical models that provide plausible descriptions for sample data with time correlations. In order to provide a statistical setting for describing the character of data that seemingly fluctuate in a random fashion over time, we assume a time series can be defined as a collection of random variables indexed according to the order they are obtained in time. In general, a collection of random variables <span class="math inline">\(\{X_t\}\)</span> indexed by <span class="math inline">\(t\)</span> is referred to as a stochastic process. In this text, <span class="math inline">\(t\)</span> will typically be discrete and vary over the integers.</p>
<p><strong>Example of Series:</strong></p>
<blockquote>
<p><strong>White Noise</strong>: A collection of uncorrelated, independent and identically distributed random variables <span class="math inline">\(W_t\)</span> with mean 0 and finite variance <span class="math inline">\(\sigma^2_w\)</span>. A particular useful white noise is Gaussian white noise, that is <span class="math inline">\(W_t \overset{i.i.d}{\sim} N(0, \sigma^2_w)\)</span> <img src="/images/RL/background/ts_1_3_1.png" width="600"></p>
</blockquote>
<blockquote>
<p><strong>Moving Average</strong>: We might replace the white noise series <span class="math inline">\(W_t\)</span> by a moving average that smooths the series: <span class="math display">\[V_t = \frac{1}{3} (W_{t-1} + W_{t} + W_{t+1})\]</span> This introduces a smoother version of white noise series, reflecting the fact that the slower oscillations are more apparent and some of the faster oscillations are taken out. <img src="/images/RL/background/ts_1_3_2.png" width="600"></p>
</blockquote>
<blockquote>
<p><strong>Autoregressions</strong>: Suppose we consider the white noise series <span class="math inline">\(W_t\)</span> as input and calculate the output using the second-order equation: <span class="math display">\[X_t = X_{t-1} - 0.9 X_{t-2} + W_t\]</span> For <span class="math inline">\(t=1, ..., 500\)</span>. We can see the periodic behavior of the series. <img src="/images/RL/background/ts_1_3_3.png" width="600"></p>
</blockquote>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/11/15/time-series-1/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/09/16/hmm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/09/16/hmm/" class="post-title-link" itemprop="url">Sequential Data</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-09-16 14:25:15" itemprop="dateCreated datePublished" datetime="2021-09-16T14:25:15+08:00">2021-09-16</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-09-23 14:04:21" itemprop="dateModified" datetime="2021-09-23T14:04:21+08:00">2021-09-23</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/09/16/hmm/" class="post-meta-item leancloud_visitors" data-flag-title="Sequential Data" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>8.6k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>8 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="sequential-data">Sequential Data</h1>
<p>In some cases, we have i.i.d assumptions to allow use to express the likelihood function as the product over all data points of the probability distribution evaluated at each data point. However, in some cases namely sequential data, we may not have i.i.d samples.</p>
<h2 id="markov-models">Markov Models</h2>
<p>To express the sequential dependence of the samples, we can relax the i.i.d assumption and one of the simplest ways to do this is to consider a <code>Markov Model</code>. First of all, without loss of generality, we can use the product rule to express the joint distribution for a sequence of observations in the form:</p>
<p><span class="math display">\[P(X_1, ...., X_n) = \prod^N_{n=1} P(X_n | X_1, ..., X_{n-1})\]</span></p>
<p>If we assume that each of the conditional distributions on the right-hand side is independent of all previous observations except the most recent, we obtain the <code>first-order Markov Chain</code>, which can depicted as graphical model:</p>
<p><img src='/images/ML/hmm_1.png' width="600"></p>
<p>The joint distribution for a sequence of <span class="math inline">\(N\)</span> observations under this model is given by:</p>
<p><span class="math display">\[P(X_1, ..., X_N) = P(X_1) \prod^{N}_{n=2} P(X_n | X_{n-1})\]</span></p>
<p>Thus, if we use such a model to predict the next observation in a sequence, the distribution of predictions will depend only on the value of the immediately preceding observation and will be independent of all earlier observations. In most applications of such models, the conditional distributions <span class="math inline">\(P(X_n | X_{n-1})\)</span> that define the model will be constrained to be equal, corresponding to a assumption of a stationary time series. The model is then known as the <code>homogeneous Markov Chain</code> (All conditional distribution share the same parameters).</p>
<p>However, first-order Markov model is still restrictive. For many sequential observations, we anticipate that the trends in the data over several successive observations will provide important information in predicting the next value. One way to allow earlier observations to have an influence is to move to higher-order Markov chains, the <code>second-order Markov chain</code> is given by:</p>
<p><span class="math display">\[P(X_1, ..., X_N) = P(X_1) P(X_2 | X_1) \prod^{N}_{n=3} P(X_n | X_{n-1}, X_{n-2})\]</span></p>
<p><img src='/images/ML/hmm_2.png' width="600"></p>
<p>Suppose we wish to build a model for sequences that is not limited by the Markov assumption to any order and yet that can be specified using a limited number of free parameters. We can achieve this by introducing additional latent variables to permit a rich class of models to be constructed out of simple components, as we did with mixture distributions.</p>
<p>For each observation <span class="math inline">\(\mathbf{X}_n\)</span>, we introduce a corresponding latent random vector <span class="math inline">\(\mathbf{Z}_n\)</span> which may be of different type or dimensionality to the observed variable. We now assume that it is the <strong>latent variables that form a Markov chain</strong>, giving rise to the graphical structure known as <code>state space model</code>. It satisfies the key conditional independence properties that <span class="math inline">\(\mathbf{Z}_{n-1}, \mathbf{Z}_{n+1}\)</span> are independent given <span class="math inline">\(\mathbf{Z}_n\)</span> so that:</p>
<p><span class="math display">\[\mathbf{Z}_{n+1} \perp \!\!\! \perp \mathbf{Z}_{n-1} \;|\; \mathbf{Z}_n\]</span></p>
<p>The joint distribution of the model is:</p>
<p><span class="math display">\[P(\mathbf{X}_1, ...., \mathbf{X}_{N}, \mathbf{Z}_1, ...., \mathbf{Z}_{N}) = P(\mathbf{Z}_1) \prod^N_{n=1} P(\mathbf{X}_n | \mathbf{Z}_n) \prod^{N}_{n=2} P(\mathbf{Z}_n | \mathbf{Z}_{n-1})\]</span></p>
<p><img src='/images/ML/hmm_3.png' width="600"></p>
<p>Using the d-separation criterion, we see that there is always a path connecting any two observed variables <span class="math inline">\(\mathbf{X}_n\)</span> and <span class="math inline">\(\mathbf{X}_{n+1}\)</span> via the latent variable, so the predictive distribution <span class="math inline">\(P(\mathbf{X}_{n+1} | \mathbf{X}_{1} , ..., \mathbf{X}_{n})\)</span> does not have any conditional dependence properties so it depends on all previous variables.</p>
<p>There are two important models for sequential data:</p>
<ol type="1">
<li><strong>Hidden Markov Model</strong>: If the latent variables are discrete.</li>
<li><strong>Linear Dynamical System</strong>: If the latent variables, observed variables are Gaussian with a linear-Gaussian dependence of the conditional distributions on their parents.</li>
</ol>
<h2 id="hidden-markov-models">Hidden Markov Models</h2>
<p>The hidden markov model can be viewed as specific instance of the state space model with discrete latent variables. It can also be viewed as an extension of a mixture model in which the choice of mixture component for each observation is not selected independently but depends on the choice of component for the previous observation.</p>
<p>It is convenient to use 1-of-<span class="math inline">\(K\)</span> coding scheme for the latent variables <span class="math inline">\(\mathbf{Z}_n\)</span>. We now allow the probability distribution of <span class="math inline">\(\mathbf{Z}_n\)</span> to depend on the state of previous latent variable <span class="math inline">\(\mathbf{Z}_{n-1}\)</span> through a conditional probability distribution <span class="math inline">\(P(\mathbf{Z}_{n} | \mathbf{Z}_{n-1})\)</span>. Since, we want to express the dependency of each element of <span class="math inline">\(\mathbf{Z}_n\)</span> on each element of <span class="math inline">\(\mathbf{Z}_{n-1}\)</span>, we introduce a <span class="math inline">\(K \times K\)</span> matrix of probabilities that we denote by <span class="math inline">\(\mathbf{A}\)</span>, the element of which are known as transition probabilities:</p>
<p><span class="math display">\[A_{jk} = P(Z_{nk} = 1 | Z_{n-1, j}) = 1\]</span></p>
<p>And because they are probabilities:</p>
<p><span class="math display">\[\sum^K_{k=1} A_{jk} = 1\]</span></p>
<p><span class="math display">\[0 \geq A_{jk} \leq 1\]</span></p>
<p><img src='/images/ML/hmm_4.png' width="600"></p>
<p>We can write the conditional distribution explicitly in the form:</p>
<p><span class="math display">\[P(\mathbf{Z}_{n} | \mathbf{Z}_{n-1}, \mathbf{A}) = \prod^K_{k=1}\prod^{K}_{j=1} A_{jk}^{Z_{nk}Z_{n-1, \;j}}\]</span></p>
<p>The initial latent node <span class="math inline">\(\mathbf{Z}_1\)</span> is special in that it does not have a parent node, and so it has a marginal distribution <span class="math inline">\(P(\mathbf{Z}_1)\)</span> represented by a vector of probabilities that represents the initial probabilities <span class="math inline">\(\boldsymbol{\pi}\)</span>:</p>
<p><span class="math display">\[\pi_k = P(Z_{1k} = 1)\]</span> <span class="math display">\[\sum^{K}_{k=1} \pi_k = 1\]</span> <span class="math display">\[P(\mathbf{Z}_1 | \boldsymbol{\pi}) = \prod^K_{k=1} \pi_k^{Z_{1k}}\]</span></p>
<p>Lastly, the specification of the probabilistic model is completed by defining the conditional distribution of observed variables <span class="math inline">\(P(\mathbf{X}_n | \mathbf{Z}_n, \boldsymbol{\phi})\)</span>, where <span class="math inline">\(\boldsymbol{\phi} = \{\boldsymbol{\phi}_1, ...., \boldsymbol{\phi}_K\}\)</span> is a set of parameters governing the distribution. These distributions are called <code>emissino distribution</code> and might be given by Gaussian if the elements of <span class="math inline">\(\mathbf{X}\)</span> are continues random variables or by conditioanl probability tables if <span class="math inline">\(\mathbf{X}\)</span> are discrete. Since the distribution depends on the values of <span class="math inline">\(\mathbf{Z}\)</span> which has <span class="math inline">\(K\)</span> possible states, We can define the emission distribution as:</p>
<p><span class="math display">\[P(\mathbf{X}_n | \mathbf{Z}_n, \boldsymbol{\phi}) = \prod^K_{k=1} P(\mathbf{X}_n | \boldsymbol{\phi}_k)^{Z_{nk}}\]</span></p>
<p>In this case, we focus on <strong>Homogeneous</strong> models for which all the conditional distributions governing the latent variables share the same parameters <span class="math inline">\(\mathbf{A}\)</span> and similarly all of the emission distributions share the same parameters <span class="math inline">\(\boldsymbol{\phi}\)</span>, so the joint distribution is:</p>
<p><span class="math display">\[P(\mathbf{X}_1, ...., \mathbf{X}_{N}, \mathbf{Z}_1, ...., \mathbf{Z}_{N} | \boldsymbol{\theta}) = P(\mathbf{D}, \mathbf{H} | \boldsymbol{\theta}) = P(\mathbf{Z}_1 | \boldsymbol{\pi}) \prod^N_{n=1} P(\mathbf{X}_n | \mathbf{Z}_n, \boldsymbol{\phi}) \prod^{N}_{n=2} P(\mathbf{Z}_n | \mathbf{Z}_{n-1}, \mathbf{A})\]</span></p>
<p>Where <span class="math inline">\(\boldsymbol{\theta} = \{\boldsymbol{\pi}, \boldsymbol{\phi}, \boldsymbol{A}\}\)</span></p>
<h3 id="maximum-likelihood-for-hmm">Maximum Likelihood for HMM</h3>
<p>If we have observed a dataset <span class="math display">\[\mathbf{D} = \{\mathbf{x}_1, ...., \mathbf{x}_N\}\]</span>, we can determine the parameters of an HMM using maximum likelihood method. The likelihood function is obtained from the joint distribution by marginalizing over the latent variables:</p>
<p><span class="math display">\[L(\boldsymbol{\theta} ;\; \mathbf{D}) = \sum_{\mathbf{H}} P(\mathbf{D}, \mathbf{H} | \; \boldsymbol{\theta}) = P(\mathbf{D} | \; \boldsymbol{\theta}) = \prod^N_{n=1} P(\mathbf{x}_n |\; \boldsymbol{\theta})\]</span></p>
<p>This is similar to the mixture distribution with latent variable in EM, we have a summation inside the log for log likelihood which is much difficult to work with, direct maximization of the log likelihood function will therefore lead to complex expressions with no closed-form solutions. Thus, one way to solve the problem is to use <strong>EM algorithm</strong> to find an efficient framework for maximizing the likelihood function in HMM.</p>
<p>The EM algorithm starts with some initial selection for the model parameters, which we denote by <span class="math inline">\(\boldsymbol{\theta}^{old}\)</span>:</p>
<ol type="1">
<li><strong>E step</strong>:
<ul>
<li>We take these parameter values and find the posterior distribution of the latent variables: <span class="math display">\[P(\mathbf{H} | \mathbf{X}, \boldsymbol{\theta}^{old})\]</span></li>
<li>We then evaluate the expectation of complete log likelihood function over the posterior distribution of the latent variables as a function of new parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>: <span class="math display">\[Q(\boldsymbol{\theta}, \boldsymbol{\theta}^{old}) = E_{\mathbf{H} | \mathbf{D}, \boldsymbol{\theta}^{old}}[\ln P(\mathbf{D}, \mathbf{H} | \boldsymbol{\theta}) | \mathbf{D}, \boldsymbol{\theta}^{old}]\]</span></li>
<li>We can rewrite the log likelihood as:
<span class="math display">\[\begin{aligned}
  Q(\boldsymbol{\theta}, \boldsymbol{\theta}^{old}) &amp;= \sum_{\mathbf{H}} P(\mathbf{H} | \mathbf{D}, \boldsymbol{\theta}^{old}) \ln P(\mathbf{D}, \mathbf{H} | \boldsymbol{\theta})\\
  &amp;= \sum^{K}_{k=1}P(Z_{1k} = 1 | \mathbf{D}, \boldsymbol{\theta}^{old}) \ln P(Z_{1k} = 1| \boldsymbol{\pi}) + \sum^N_{n=2}\sum^{K}_{j=1}\sum^{K}_{k=1} P(Z_{nk}, Z_{n-1, j} | \mathbf{D}, \boldsymbol{\theta}^{old}) \ln P(Z_{nk} | Z_{n-1, j}) + \sum^{N}_{n=1}\sum^{K}_{k=1} P(Z_{nk} = 1 | \mathbf{D}, \boldsymbol{\theta}^{old}) \ln P(\mathbf{X}_n| \boldsymbol{\phi}_k, Z_{nk}=1)\\
  &amp;= \sum^{K}_{k=1} \gamma(Z_{nk})\ln \pi_k + \sum^N_{n=2}\sum^{K}_{j=1}\sum^{K}_{k=1} \xi(Z_{nk}, Z_{n-1, j})\ln A_{jk} + \sum^{N}_{n=1}\sum^{K}_{k=1} \gamma(Z_{nk}) \ln P(\mathbf{X}_n| \boldsymbol{\phi}_k, Z_{nk}=1)
  \end{aligned}\]</span>
Where <span class="math inline">\(\gamma (Z_{nk}) = P(Z_{1k} = 1 | \mathbf{D}, \boldsymbol{\theta}^{old})\)</span> and <span class="math inline">\(\xi(Z_{nk}, Z_{n-1, j}) = P(Z_{nk}, Z_{n-1, j} | \mathbf{D}, \boldsymbol{\theta}^{old}) \ln P(Z_{nk} | Z_{n-1, j})\)</span></li>
<li><strong>Our goal is to evaluate these posterior probabilities <span class="math inline">\(\gamma, \xi\)</span> efficiently</strong></li>
</ul></li>
<li><strong>M step</strong>:
<ul>
<li>We maximize <span class="math inline">\(Q(\boldsymbol{\theta}, \boldsymbol{\theta}^{old})\)</span> w.r.t the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> in which we treat posterior probabilities as constant:</li>
</ul></li>
</ol>
<h1 id="ref">Ref</h1>
<p>PRML Chapter 13</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/09/16/approximate-infer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/09/16/approximate-infer/" class="post-title-link" itemprop="url">approximate_infer</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-09-16 13:55:20 / Modified: 14:22:58" itemprop="dateCreated datePublished" datetime="2021-09-16T13:55:20+08:00">2021-09-16</time>
    </span>

  
    <span id="/2021/09/16/approximate-infer/" class="post-meta-item leancloud_visitors" data-flag-title="approximate_infer" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>20</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>1 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="variational-inference">Variational Inference</h1>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/09/09/graphical-models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/09/09/graphical-models/" class="post-title-link" itemprop="url">Graphical Models</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-09-09 13:17:03" itemprop="dateCreated datePublished" datetime="2021-09-09T13:17:03+08:00">2021-09-09</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2022-07-04 22:04:09" itemprop="dateModified" datetime="2022-07-04T22:04:09+08:00">2022-07-04</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/09/09/graphical-models/" class="post-meta-item leancloud_visitors" data-flag-title="Graphical Models" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>20k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>18 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="graphical-models">Graphical Models</h1>
<h2 id="conditional-independence">Conditional Independence</h2>
<h3 id="conditional-independence-of-random-variable">Conditional Independence of Random Variable</h3>
<p>Let <span class="math inline">\(\mathbf{X}, \mathbf{Y}, \mathbf{Z}\)</span> be a set of random variables. We say that <span class="math inline">\(\mathbf{X}\)</span> is conditionally independent of <span class="math inline">\(\mathbf{Y}\)</span> given <span class="math inline">\(\mathbf{Z}\)</span> in a distribution <span class="math inline">\(P\)</span> if <span class="math inline">\(P\)</span> satisfies <span class="math inline">\(P(\mathbf{X} = x \perp \mathbf{Y}=y | \mathbf{Z}=z)\)</span> for all values of <span class="math inline">\(x, y, z \in (Val(\mathbf{X}), Val(\mathbf{Y}), Val(\mathbf{Z}))\)</span>. If the set <span class="math inline">\(\mathbf{Z}\)</span> is empty, we write <span class="math inline">\((\mathbf{X} \perp \mathbf{Y})\)</span> and say that <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span> are marginally independent.</p>
<p>The distribution <span class="math inline">\(P\)</span> satisfies <span class="math inline">\((\mathbf{X} \perp \mathbf{Y} | \mathbf{Z})\)</span> IFF <span class="math inline">\(P(\mathbf{X}, \mathbf{Y} | \mathbf{Z}) = P(\mathbf{X}| \mathbf{Z}) P(\mathbf{Y} | \mathbf{Z})\)</span></p>
<ul>
<li><strong>Symmetry</strong>: <span class="math display">\[(\mathbf{X} \perp \mathbf{Y} | \mathbf{Z}) \implies (\mathbf{Y} \perp \mathbf{X} | \mathbf{Z})\]</span></li>
<li><strong>Decomposition</strong>: <span class="math display">\[(\mathbf{X} \perp \mathbf{Y}, \mathbf{W} | \mathbf{Z}) \implies (\mathbf{X} \perp \mathbf{Y} | \mathbf{Z}), \;(\mathbf{X} \perp \mathbf{W} | \mathbf{Z})\]</span></li>
<li><strong>Weak Union</strong>: <span class="math display">\[(\mathbf{X} \perp \mathbf{Y}, \mathbf{W} | \mathbf{Z}) \implies (\mathbf{X} \perp \mathbf{Y} | \mathbf{Z}, \mathbf{W}), \; (\mathbf{X} \perp \mathbf{W} | \mathbf{Z}, \mathbf{Y})\]</span></li>
<li><strong>Contraction</strong>: <span class="math display">\[(\mathbf{X} \perp \mathbf{W} | \mathbf{Z}, \mathbf{Y}) \cap (\mathbf{X} \perp \mathbf{Y} | \mathbf{Z}) \implies (\mathbf{X} \perp \mathbf{Y}, \mathbf{W}| \mathbf{Z})\]</span></li>
</ul>
<p>We can represent complicated probabilistic models using diagrammatic representations of probability distributions called <code>probabilistic graphical models</code>. These offer several useful properties:</p>
<ol type="1">
<li>They provide a simple way to visualize the structure of a probabilistic model and can be used to design and motivate new models.</li>
<li>Insights into the properties of the model, including conditional independence properties can be obtained by inspection of the graph.</li>
<li>Complex computations, required to perform inference and learning in sophisticated models, can be expressed in terms of graphical manipulations, in which underlying mathematical expressions are carried along implicitly.</li>
</ol>
<p><br></p>
<p>A probabilistic graphical model consists of:</p>
<ol type="1">
<li><strong>Nodes</strong>: each random variable (or group of random variables) is represented as a node in the graph</li>
<li><strong>Edges (links)</strong>: links express probabilistic relationship between these random variables, the edges encode our intuition about the way the world works.
<ul>
<li><strong>Directed graphical models</strong>: in which the edges of the graphs have a particular directionality indicated by arrows (Bayesian networks). Directed graphs are useful for expressing causal relationships between random variables.</li>
<li><strong>Undirected graphical models</strong>: in which the edges of the graph do not carry arrows and have no directional significance (Markov random fields). Undirected graphs are better suited to expressing soft constraints between random variables.</li>
</ul></li>
</ol>
<p>The graph then captures the way in which the joint distribution over all of the random variables can be decomposed into a product of factors each depending only on a subset of the variables.</p>
<h2 id="bayesian-networks">Bayesian Networks</h2>
<p>The DAG of random variables can be viewed in two very different ways (Also strongly equivalent):</p>
<ul>
<li>As a data structure that provides the skeleton for representing a joint distribution compactly in a factorized way.</li>
<li>As a compact representation for a set of conditional independence assumptions about a distribution.</li>
</ul>
<blockquote>
<p><strong>We can view the graph as encoding a generative sampling process executed by nature, where the value for each variable is selected by nature using a distribution that depends only on its parents. In other words, each variable is a stochastic function of its parents.</strong></p>
</blockquote>
<blockquote>
<p><strong>In general, there are many weak influences that we might choose to model, but if we put in all of them, the network can become very complex, such networks are problematic from a representational perspective.</strong></p>
</blockquote>
<h3 id="bayesian-network-represents-a-joint-distribution-compactly-in-a-factorized-way">Bayesian Network Represents a Joint Distribution Compactly in a Factorized Way</h3>
<p>Consider first an arbitrary joint distribution defined by <span class="math inline">\(P(\mathbf{Z})\)</span> over random vector <span class="math inline">\(\mathbf{Z} = &lt;A, B, C&gt;\)</span>, by product rule, we have:</p>
<p><span class="math display">\[P(\mathbf{Z}) = P(C| A, B) P(A, B) = P(C | A, B) P(B | A) P(A)\]</span></p>
<p>We now represent the right-hand side in terms of a simgple graphical model as follows:</p>
<ol type="1">
<li>First, we introduce a node for each of the random variables <span class="math inline">\(A, B, C\)</span> and associate each node with the corresponding conditional distribution on the right-hand side.</li>
<li>Then, for each conditional distribution we add directed links to the graph from the nodes to the variables on which the distribution is conditioned.</li>
</ol>
<p><img src='/images/ML/gm_1.png' width="600"></p>
<p>If there is a link going from a node <span class="math inline">\(A\)</span> to a node <span class="math inline">\(B\)</span>, then we say that node <span class="math inline">\(A\)</span> is parent of node <span class="math inline">\(B\)</span> and <span class="math inline">\(B\)</span> is the child of node <span class="math inline">\(A\)</span> (change ordering of the decomposition will change the graph).</p>
<p>We can extend the idea to joint distribution of <span class="math inline">\(K\)</span> random variables given by <span class="math inline">\(P(X_1, ...., X_K)\)</span>. By repeated application of the product rule of the probability, this joint distribution can be written as a product of conditional distributions:</p>
<p><span class="math display">\[P(X_1, ...., X_K) = P(X_K | X_{K-1}, ..., X_{1}) ... P(X_2 | X_1) P(X_1)\]</span></p>
<p>We can generate a graph similar to three-variable case, each node having incoming links from all lower numbered nodes. We say this graph is <code>fully connected</code> because there is a link between every pair of nodes. However, it is the <strong>absence</strong> (not fully connected) of links in the graph that conveys interesting information about the properties of the class of distributions that the graph represents.</p>
<p><img src='/images/ML/gm_2.png' width="600"></p>
<p><br></p>
<p>We can now state in general terms the relationship between a given directed graph and the corresponding distribution over the variables. Thus, for a graph with K nodes <span class="math inline">\(\mathbf{X} = &lt;X_1, ...., X_K&gt;\)</span>, the joint distribution is given by:</p>
<p><span class="math display">\[P(\mathbf{X}) = \prod^K_{k=1} P(X_k | \text{Parent}(X_k))\]</span></p>
<p>Where <span class="math inline">\(\text{Parent}(X_k)\)</span> denotes the set of parents of <span class="math inline">\(X_k\)</span>.</p>
<p>Notice that, the directed graphs that we are considering are subject to an important restriction namely that there must be <strong>no</strong> directed cycles, that is, we are working with <code>directed acyclic graphs</code> or DAGs.</p>
<h4 id="example-generative-models">Example: Generative Models</h4>
<p>There are many situations in which we wish to draw samples from a given probability distribution. One technique which is particularly relevant to graphical models is called <code>ancestral sampling</code>.</p>
<p>Consider a joint distribution <span class="math inline">\(P(\mathbf{X}), \mathbf{X} = &lt;X_1, ...., X_K&gt;\)</span> that factorizes into a DAG. We shall suppose that the variables have been ordered from <span class="math inline">\(X_1\)</span> to <span class="math inline">\(X_K\)</span>, in other words each node has a higher index than any of its parents. Our goal is to draw samples <span class="math inline">\(\hat{X}_1, ..., \hat{X}_K\)</span> from the joint distribution.</p>
<p>To do this, we start from <span class="math inline">\(X_1\)</span>, and draw sample <span class="math inline">\(\hat{X}_1\)</span> from the distribution <span class="math inline">\(P(X_1)\)</span>. We then work through each of the nodes in order, so that for node <span class="math inline">\(n\)</span> we draw a sample from the conditional distribution <span class="math inline">\(P(X_n | \text{Parent}(X_n))\)</span>, in which the parent variables have been set to their sampled values.</p>
<p>To obtain a sample from some marginal distribution corresponding to a subset of the random variables, we simply take the sampled values for the required nodes and discard the rest. For example, to draw a sample from the distribution <span class="math inline">\(P(X_2, X_4)\)</span>, we simply sample from the full joint distribution and then retain the values <span class="math inline">\(\hat{X}_2, \hat{X}_4\)</span> and discard the remaining values.</p>
<p>For practical applications of probabilistic models, it will typically be the higher-numbered variables corresponding to terminal nodes of the graph that represent the observations, with lower-numbered nodes corresponding to latent variables. The primary role of the latent variables is to allow a complicated distribution over the observed variables to tbe represented in terms of a model constructed from simpler conditional distributions.</p>
<blockquote>
<blockquote>
<p>Consider an object recognition task in which each observed data point corresponds to an image of on of the objects (vector of pixels). In this case, we can have latent variables be position and orientation of the object. Given a particular observed image, our goal is to find the posterior distribution over objects in which we integrate over all possible positions and orientations. <img src='/images/ML/gm_3.png' width="600"> Given object, position, orientation, we can sample from the conditional distribution of image and generate pixels.</p>
</blockquote>
</blockquote>
<p>The graphical model captures causal process by which the observed data was generated. For this reason, such models are often called <code>generative models</code>.</p>
<p><br></p>
<h3 id="independence-in-bayesian-networks">Independence in Bayesian Networks</h3>
<p><strong>Our intuition tells us that the parents of a variable “shield” it from probabilistic influence that is causal in nature. In other words, once I know the value of the parents, no information relating directly or indirectly to its parents or other ancestors can influence my beliefs about it. However, information about its descendants can change my beliefs about it, via an evidential reasoning process.</strong></p>
<h4 id="bayesian-network-semantics">Bayesian Network Semantics</h4>
<h5 id="definition-3.1-bayesian-network-structure">Definition 3.1: Bayesian Network Structure</h5>
<p>A Bayesian network structure <span class="math inline">\(G\)</span> is a directed acyclic graph whose nodes represent random variables <span class="math inline">\(X_1, ...., X_n\)</span>. Let <span class="math inline">\(Pa^{G}_{X_i}\)</span> denote the parents of <span class="math inline">\(X_i\)</span> in <span class="math inline">\(G\)</span>, and <span class="math inline">\(\text{NonDescendants}_{X_i}\)</span> denote the variables in the graph that are not descendants of <span class="math inline">\(X_i\)</span>. Then <span class="math inline">\(G\)</span> encodes the following set of <strong>conditional independence assumptions</strong>, called <strong>local independence</strong>, and denoted by <span class="math inline">\(I_l (G)\)</span>:</p>
<p><span class="math display">\[I_l (G) = \{\text{For each variable $X_i$: ($X_i \perp \text{NonDescendants}_{X_i} | Pa^G_{X_i}$})\}\]</span></p>
<p>In other words, the local independence state that each node <span class="math inline">\(X_i\)</span> is conditionally independent of its non-descendants given its parents.</p>
<p><br></p>
<h3 id="graphs-and-distributions">Graphs and Distributions</h3>
<p>We now show that the previous two definitions of BN are equivalent. That is, <strong>a distribution <span class="math inline">\(P\)</span> satisfies the local independence associated with a graph <span class="math inline">\(G\)</span> IFF <span class="math inline">\(P\)</span> is representable as a set of CPDs associated with the graph <span class="math inline">\(G\)</span>.</strong></p>
<h4 id="i-maps">I-Maps</h4>
<h5 id="definition-3.2-independencies-in-p">Definition 3.2: Independencies in <span class="math inline">\(P\)</span></h5>
<p>Let <span class="math inline">\(P\)</span> be a distribution over <span class="math inline">\(\mathbf{X}\)</span>. We define <span class="math inline">\(I(P)\)</span> to be the set of independence assertions of the form <span class="math inline">\((\mathbf{X} \perp \mathbf{Y} | \mathbf{Z})\)</span></p>
<p><br></p>
<h5 id="definition-3.3-i-map">Definition 3.3: I-Map</h5>
<p>Let <span class="math inline">\(K\)</span> be any graph object associated with a set of independencies <span class="math inline">\(I(K)\)</span>. We say that <span class="math inline">\(K\)</span> is an I-map for a set of independencies <span class="math inline">\(I\)</span> if <span class="math inline">\(I(K) \subseteq I\)</span>.</p>
<p>We now say that <span class="math inline">\(G\)</span> is a I-map for <span class="math inline">\(P\)</span> if <span class="math inline">\(G\)</span> is an I-map for <span class="math inline">\(I(P)\)</span>. That is <span class="math inline">\(I(G) \subseteq I(P)\)</span>.</p>
<p><br></p>
<p>That is, for <span class="math inline">\(G\)</span> to be an I-map of <span class="math inline">\(P\)</span>, it is necessary that <span class="math inline">\(G\)</span> does not mis-lead us regarding independencies in <span class="math inline">\(P\)</span>: any dependence that <span class="math inline">\(G\)</span> asserts must also hold in <span class="math inline">\(P\)</span>. Conversely, <span class="math inline">\(P\)</span> may have additional independencies that are not reflected in <span class="math inline">\(G\)</span>.</p>
<p><br></p>
<h4 id="i-map-to-factorization">I-Map to Factorization</h4>
<p>A BN structure <span class="math inline">\(G\)</span> encodes a set of conditional independence assumptions, every distribution for which <span class="math inline">\(G\)</span> is an I-map must satisfy these assumptions.</p>
<h5 id="definition-3.4-factorization-chain-rule-of-bayesian-network">Definition 3.4: Factorization (Chain Rule of Bayesian Network)</h5>
<p>Let <span class="math inline">\(G\)</span> be a BN graph over the variables <span class="math inline">\(X_1, ...., X_n\)</span>. We say that a distribution <span class="math inline">\(P\)</span> over the same space factorizes according to <span class="math inline">\(G\)</span> if <span class="math inline">\(P\)</span> can be expressed as a product:</p>
<p><span class="math display">\[P(X_1, ...., X_n) = \prod^n_{i=1} P(X_i | Pa^G_{X_i})\]</span></p>
<p><br></p>
<h5 id="definition-3.5-bayesian-network">Definition 3.5: Bayesian Network</h5>
<p>A Bayesian network is a pair <span class="math inline">\(B = (G, P)\)</span> where <span class="math inline">\(P\)</span> factorizes over <span class="math inline">\(G\)</span>, and where <span class="math inline">\(P\)</span> is specified as a set of CPDs associated with <span class="math inline">\(G\)</span>'s nodes. The distribution <span class="math inline">\(P\)</span> is often annotated <span class="math inline">\(P_B\)</span>.</p>
<p><br></p>
<h5 id="theorem-3.1-i-map-factorization">Theorem 3.1: I-Map Factorization</h5>
<p>Let <span class="math inline">\(G\)</span> be a BN structure over a set of random variables <span class="math inline">\(\mathbb{X}\)</span>, and let <span class="math inline">\(P\)</span> be a joint distribution over the same space. If <span class="math inline">\(G\)</span> is a I-map for <span class="math inline">\(P\)</span>, then <span class="math inline">\(P\)</span> factorizes according to <span class="math inline">\(G\)</span> (can be written in the form as in definition 3.4).</p>
<p><br></p>
<h4 id="factorization-to-i-map">Factorization to I-Map</h4>
<h5 id="theorem-3.2-factorization-to-i-map">Theorem 3.2: Factorization to I-Map</h5>
<p>Let <span class="math inline">\(G\)</span> be a BN structure over a set of random variables <span class="math inline">\(\mathbb{X}\)</span> and let <span class="math inline">\(P\)</span> be a joint distribution over the same space. If <span class="math inline">\(P\)</span> factorizes according to <span class="math inline">\(G\)</span>, then <span class="math inline">\(G\)</span> is an I-map for <span class="math inline">\(P\)</span>.</p>
<p><br></p>
<h3 id="independencies-in-graphs">Independencies in Graphs</h3>
<p>Knowing only that a distribution <span class="math inline">\(P\)</span> factorizes over <span class="math inline">\(G\)</span>, we can conclude that it satisfies <span class="math inline">\(I_l (G)\)</span>. Are there other independencies that hold for every distribution <span class="math inline">\(P\)</span> that factorizes over <span class="math inline">\(G\)</span>? Our goal is to understand when we can guarantee that an independence <span class="math inline">\((\mathbf{X} \perp \mathbf{Y} | \mathbf{Z})\)</span> holds in a distribution associated with a BN structure <span class="math inline">\(G\)</span>.</p>
<h4 id="d-separation">D-Separation</h4>
<p>When influence can flow from <span class="math inline">\(X, Y\)</span> via <span class="math inline">\(Z\)</span> (<span class="math inline">\(X, Y\)</span> are correlated), we say that the trail is <strong>active</strong></p>
<ul>
<li><strong>Direct connection</strong>: When <span class="math inline">\(X, Y\)</span> are directly connected via edge. For any network structure <span class="math inline">\(G\)</span> they are <strong>always</strong> correlated regardless of any evidence about any of the other variables in the network.</li>
<li><strong>Indirect connection</strong>: <span class="math inline">\(X, Y\)</span> are not directly connected via edge, but there is a trail between then in the graph via <span class="math inline">\(Z\)</span>.
<ul>
<li><strong>Indirect causal effect</strong>: <span class="math inline">\(X \rightarrow Z \rightarrow Y\)</span>. If we observe <span class="math inline">\(Z\)</span>, then <span class="math inline">\(X, Y\)</span> are conditionally independent, if <span class="math inline">\(Z\)</span> is not observed, <span class="math inline">\(X\)</span> influences <span class="math inline">\(Y\)</span> by first sampling from <span class="math inline">\(P(X)\)</span> then sampling <span class="math inline">\(Z\)</span> from <span class="math inline">\(P(Z | X)\)</span>, so <span class="math inline">\(X, Y\)</span> are not independent. (Active IFF <span class="math inline">\(Z\)</span> is <strong>not</strong> observed)</li>
<li><strong>Indirect evidential effect</strong>: <span class="math inline">\(Y \rightarrow Z \rightarrow X\)</span>, same as indirect causal effect. (Active IFF <span class="math inline">\(Z\)</span> is <strong>not</strong> observed)</li>
<li><strong>Common cause</strong>: <span class="math inline">\(X \leftarrow Z \rightarrow Y\)</span>, same as above, <span class="math inline">\(X\)</span> can influence <span class="math inline">\(Y\)</span> via <span class="math inline">\(Z\)</span> IFF <span class="math inline">\(Z\)</span> is not observed. (Active IFF <span class="math inline">\(Z\)</span> is <strong>not</strong> observed)</li>
<li><strong>Common effect</strong>: <span class="math inline">\(X \rightarrow Z \leftarrow Y\)</span>, if <span class="math inline">\(Z\)</span> is unobserved, then <span class="math inline">\(X, Y\)</span> are independent, if it is observed then they are not. (Active IFF <span class="math inline">\(Z\)</span> or one of <span class="math inline">\(Z\)</span>'s descendants is <strong>observed</strong>)</li>
</ul></li>
</ul>
<p><img src='/images/RL/background/pgm_1.png' width="600"></p>
<p><br></p>
<h5 id="definition-3.6-general-case-one-path-from-x_1-to-x_n">Definition 3.6: General Case (One path from <span class="math inline">\(X_1\)</span> to <span class="math inline">\(X_n\)</span>)</h5>
<p>Let <span class="math inline">\(G\)</span> be a BN structure, and <span class="math inline">\(X_1 \rightleftharpoons .... \rightleftharpoons X_n\)</span> a trail in <span class="math inline">\(G\)</span>. Let <span class="math inline">\(\mathbf{Z}\)</span> be a subset of observed variables. The trail <span class="math inline">\(X_1 \rightleftharpoons .... \rightleftharpoons X_n\)</span> is <strong>active</strong> given <span class="math inline">\(\mathbf{Z}\)</span> if:</p>
<ul>
<li>Whenever we have a <span class="math inline">\(v\)</span>-structure (Common effect), then <span class="math inline">\(X_i\)</span> or one of its descendants are in <span class="math inline">\(\mathbf{Z}\)</span>.</li>
<li>No other node along the trail is in <span class="math inline">\(\mathbf{Z}\)</span>.</li>
</ul>
<p>Note if <span class="math inline">\(X_1\)</span> or <span class="math inline">\(X_n\)</span> are in <span class="math inline">\(\mathbf{Z}\)</span>, the trail is not active.</p>
<p><br></p>
<h5 id="definition-3.7-d-seperation">Definition 3.7: D-seperation</h5>
<p>Let <span class="math inline">\(\mathbf{X}, \mathbf{Y}, \mathbf{Z}\)</span> be three sets of nodes in <span class="math inline">\(G\)</span>. We say that <span class="math inline">\(\mathbf{X}, \mathbf{Y}\)</span> are <strong>d-seperated</strong> given <span class="math inline">\(\mathbf{Z}\)</span>, denoted <span class="math inline">\(d-sep_G(\mathbf{X}; \mathbf{Y} | \mathbf{Z})\)</span>, if there is <strong>no</strong> active trail between any node <span class="math inline">\(X \in \mathbf{Z}\)</span> and <span class="math inline">\(Y \in \mathbf{Y}\)</span> given <span class="math inline">\(\mathbf{Z}\)</span>. We use <span class="math inline">\(I(G)\)</span> to denote the set of independencies that correspond to d-separation:</p>
<p><span class="math display">\[I(G) = \{(\mathbf{X} \perp \mathbf{Y} | \mathbf{Z}) : d-sep_{G} (\mathbf{X}; \mathbf{Y} | \mathbf{Z})\}\]</span></p>
<p>The independencies in <span class="math inline">\(I(G)\)</span> are precisely those that are guaranteed to hold for every distribution over <span class="math inline">\(G\)</span>.</p>
<p><br></p>
<h4 id="soundness-and-completeness">Soundness and Completeness</h4>
<h5 id="theorem-3.3-soundness">Theorem 3.3: Soundness</h5>
<p>If a distribution <span class="math inline">\(P\)</span> factorizes according to <span class="math inline">\(G\)</span>, then <span class="math inline">\(I(G) \subseteq I(P)\)</span>.</p>
<p>In other words, any independence reported by d-separation is satisfied by the underlying distribution.</p>
<p><br></p>
<h5 id="definition-3.8-faithful">Definition 3.8: Faithful</h5>
<p>A distribution <span class="math inline">\(P\)</span> is faithful to <span class="math inline">\(G\)</span> if, whenever <span class="math inline">\((X \perp Y | \mathbf{Z}) \in I(P)\)</span>, then <span class="math inline">\(d-sep_G(X; Y | \mathbf{Z})\)</span>.</p>
<p>In other words, any independence in <span class="math inline">\(P\)</span> is reflected in the d-separation properties of the graph.</p>
<p><br></p>
<h5 id="theorem-3.4-completeness">Theorem 3.4: Completeness</h5>
<p>Let <span class="math inline">\(G\)</span> be a BN structure. If <span class="math inline">\(X, Y\)</span> are not d-separated given <span class="math inline">\(\mathbf{Z}\)</span> in <span class="math inline">\(G\)</span>, then <span class="math inline">\(X, Y\)</span> are dependent given <span class="math inline">\(\mathbf{Z}\)</span> in some distribution <span class="math inline">\(P\)</span> that factorizes over <span class="math inline">\(G\)</span>.</p>
<p><br></p>
<p><strong>THese results state that for almost all parameterizations <span class="math inline">\(P\)</span> of the graph <span class="math inline">\(G\)</span>, the d-separation test precisely characterizes the independencies that hold for <span class="math inline">\(P\)</span>.</strong></p>
<h2 id="conditional-independence-1">Conditional Independence</h2>
<p>An important concept for probability distributions over multiple variables is that of <strong>conditional independence</strong>. Consider three random variables <span class="math inline">\(A, B, C\)</span> and suppose that the conditional distribution of <span class="math inline">\(A\)</span>, given <span class="math inline">\(B, C\)</span> is such that it does not depend on the value of <span class="math inline">\(B\)</span>, so that:</p>
<p><span class="math display">\[P(A | B, C) = P(A | C)\]</span></p>
<p>Then:</p>
<p><span class="math display">\[P(A, B | C) = P(A | B, C) P(B | C) = P(A | C) P (B | C)\]</span></p>
<p>Thus, we can see that <span class="math inline">\(A, B\)</span> are statistically independent given <span class="math inline">\(C, \; \forall C\)</span>. Note that this definition of conditional independence will require the above equation holds for all values fo <span class="math inline">\(C\)</span> and not just for some values. The shorthand notation for conditional independence is:</p>
<p><span class="math display">\[A \perp \!\!\! \perp B \;|\; C\]</span></p>
<p>An important and elegant feature of graphical models is that conditional independence properties of the joint distribution can be read directly from the graph without having to perform any analytical manipulations. The general framework for achieving this is called <code>d-seperation</code> (d stands for directed).</p>
<h3 id="three-example-graphs">Three example graphs</h3>
<p>We start by illustrating the key concepts of d-separation by three motivating examples.</p>
<ol type="1">
<li><blockquote>
<p><span class="math inline">\(P(A, B, C) = P(A | C) P(B | C) P(C)\)</span> <img src='/images/ML/gm_4.png' width="600"> <span class="math inline">\(A, B\)</span> are generally <strong>not</strong> statistically independent. However, we can easily see that <span class="math inline">\(A, B\)</span> are conditionally independent given <span class="math inline">\(C\)</span>: <span class="math display">\[P(A, B | C) = \frac{P(A, B, C)}{P(C)} = P(A | C) P (B | C)\]</span> <img src='/images/ML/gm_5.png' width="600"></p>
</blockquote></li>
</ol>
<p>We can provide a simple graphical interpretation of this result by considering the path from node <span class="math inline">\(A\)</span> to node <span class="math inline">\(B\)</span> via <span class="math inline">\(C\)</span>. The node <span class="math inline">\(C\)</span> is said to be <strong>tail-to-tail</strong> with respect to this path because the node is connected to the tails of the two arrows. However, when we condition on node <span class="math inline">\(C\)</span> (observed <span class="math inline">\(C\)</span>), the conditional node blocks the path from <span class="math inline">\(A\)</span> to <span class="math inline">\(B\)</span> so causes then to become conditionally independent.</p>
<ol start="2" type="1">
<li><blockquote>
<p><span class="math inline">\(P(A, B, C) = P(B | C) P(C | A) P (A)\)</span> <img src='/images/ML/gm_6.png' width="600"> <span class="math inline">\(A, B\)</span> are generally <strong>not</strong> statistically independent. However, we can easily see that <span class="math inline">\(A, B\)</span> are conditionally independent given <span class="math inline">\(C\)</span> by: <span class="math display">\[P(A, B | C) = \frac{P(A, B, C)}{P(C)} = P(A | C) P (B | C)\]</span></p>
</blockquote></li>
</ol>
<p>We can provide a simple graphical interpretation of this result by considering the path from node <span class="math inline">\(A\)</span> to node <span class="math inline">\(B\)</span> via <span class="math inline">\(C\)</span>. The node <span class="math inline">\(C\)</span> is said to be <strong>head-to-tail</strong> with respect to this path because the node is connected to the head and tail of the two arrows. However, when we condition on node <span class="math inline">\(C\)</span> (observed <span class="math inline">\(C\)</span>), the conditional node blocks the path from <span class="math inline">\(A\)</span> to <span class="math inline">\(B\)</span> so causes then to become conditionally independent.</p>
<ol start="3" type="1">
<li><blockquote>
<p><span class="math inline">\(P(A, B, C) = P(A)P(B)P(C | A, B)\)</span> <img src='/images/ML/gm_7.png' width="600"> We can easily see that <span class="math inline">\(A, B\)</span> are <strong>not</strong> conditionally independent. However, we can see that <span class="math inline">\(A, B\)</span> are statistically independent: <span class="math inline">\(P(A, B) = \sum_{C} P(A)P(B)P(C | A, B) = P(A)P(B)\)</span></p>
</blockquote></li>
</ol>
<p>Thus, our third example has the opposite behaviour from the first two. The node <span class="math inline">\(C\)</span> is said to be <strong>head-to-head</strong> with respect to this path because the node is connected to the heads of the two arrows. When the node <span class="math inline">\(C\)</span> is not given (unobserved), it blocks the path so <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is independent, however, when the node <span class="math inline">\(C\)</span> is given, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> becomes dependent.</p>
<p>There is one more relationship associate with third example. First we say that node <span class="math inline">\(Y\)</span> is a <strong>descendant</strong> of node <span class="math inline">\(X\)</span> if there is a path from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span> in which each step of the path follows the directions of the arrows. Then it can be shown that a <strong>head to head</strong> path will become unblocked if either the node or any of its descendants is observed.</p>
<h3 id="d-separation-1">D-separation</h3>
<p>Consider a general directed graph in which <span class="math inline">\(A, B, C\)</span> are arbitrary sets of nodes. We wish to ascertain whether a particular conditional independence statement <span class="math inline">\(A \perp \!\!\! \perp B \;|\; C\)</span> is implied by a given directed acyclic graph. To do so, we consider all possible paths from any node in <span class="math inline">\(A\)</span> to any node in <span class="math inline">\(B\)</span>. Any such path is said to be <strong>blocked</strong> if it includes a node such that either:</p>
<ol type="1">
<li>The arrows on the path meet either head-to-tail or tail-to-tail at the node, and the node is in the set <span class="math inline">\(C\)</span>.</li>
<li>The arrows meet head-to-head at the node, and neither the node, nor any of its descendants, is in the set <span class="math inline">\(C\)</span>.</li>
</ol>
<p>If <strong>all</strong> paths are blocked, then <span class="math inline">\(A\)</span> is said to be <code>d-separated</code> from <span class="math inline">\(B\)</span> by <span class="math inline">\(C\)</span>, and the joint distribution over all of the variables in the graph will satisfy <span class="math inline">\(A \perp \!\!\! \perp B \;|\; C\)</span>.</p>
<blockquote>
<p>Consider the problem of finding the posterior distribution for the mean of an univariate Gaussian distribution. This can be represented by the directed graph in which the joint distribution is defined by a prior <span class="math inline">\(P(\mu)\)</span> and <span class="math inline">\(P(\mathbf{X} | \mu)\)</span> to form the posterior distribution: <span class="math display">\[P(\mu | \mathbf{X}) = P(\mu) P(\mathbf{X} | \mu) \]</span> <img src='/images/ML/gm_8.png' width="600"> In practice, we observe <span class="math inline">\(D = \{X_1, ...., X_N\}\)</span> with conditional distribution <span class="math inline">\(P(X_1 | \mu) , ...., P(X_N | \mu)\)</span> respectively, and our goal is to infer <span class="math inline">\(\mu\)</span>. Using d-separation, we note that there is a unique path from any <span class="math inline">\(X_i\)</span> to any other <span class="math inline">\(X_{j\neq i}\)</span> and that this path is tail-to-tail with respect to the observed node <span class="math inline">\(\mu\)</span>. Every such path is blocked and so the observations <span class="math inline">\(D=\{X_1, ..., X_N\}\)</span> are independent given <span class="math inline">\(\mu\)</span>: <span class="math display">\[P(\mathbf{X} | \mu) = \prod^N_{i=1} P(X_i | \mu)\]</span> However, if we do not conditional on <span class="math inline">\(\mu\)</span>, the data samples are not independent: <span class="math display">\[P(\mathbf{X}) = \int_{\mu} P(\mathbf{X} | \mu) P(\mu) \neq \prod^N_{i=1} P(X_i)\]</span></p>
</blockquote>
<h2 id="markov-random-fields">Markov Random Fields</h2>
<p>Directed Graphical models specify a factorization of the joint distribution over a set of variables into a product of local conditional distributions. They also defined a set of conditional independence properties that must be satisfied by any distribution that factorizes according to the graph. A <code>Markove random field</code> has:</p>
<ol type="1">
<li>A set of nodes each of which corresponds to a random variable or group of random variables</li>
<li>A set of links each of which connects a pair of nodes. The links are <strong>undirected</strong> that is they do not carry arrows.</li>
</ol>
<h3 id="conditional-independence-properties">Conditional Independence Properties</h3>
<p>Testing for conditional independence in undirected graph is simpler than in directed graph. Let <span class="math inline">\(A, B, C\)</span> be three sets of nodes and we consider the conditional independence property <span class="math inline">\(A \perp \!\!\! \perp B \;|\; C\)</span>. To test whether this property is satisfied by a probability distribution defined by the graph:</p>
<ul>
<li>Consider all possible paths that connect nodes in set <span class="math inline">\(A\)</span> to nodes in set <span class="math inline">\(B\)</span>. If all such paths pass through one or more nodes in set <span class="math inline">\(C\)</span>, then <strong>all</strong> such paths are <strong>blocked</strong> and so the conditional independence properties holds. If there is <strong>at least one</strong> such path that is not blocked, then there will exist at least some distributions corresponding to the graph that do not satisfy this conditional independence relation.</li>
</ul>
<p><img src='/images/ML/gm_9.png' width="600"></p>
<h3 id="factorization-properties">Factorization Properties</h3>
<p>We now express the joint distribution <span class="math inline">\(P(\mathbf{X})\)</span> as a product of functions defined over set of random variables that are local to the graph.</p>
<p>If we consider two nodes <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> that are not connected by a link, then these variables must be conditionally independent given all other nodes in the graph, because there is no direct path between the two nodes and all other paths are blocked:</p>
<p><span class="math display">\[P(X_i, X_j | \mathbf{X}_{k\notin \{i, j\}}) = P(X_i | \mathbf{X}_{k\notin \{i, j\}}) P(X_j | \mathbf{X}_{k\notin \{i, j\}})\]</span></p>
<p><br></p>
<p>A <code>clique</code> is a subset of nodes in a graph such that there exists a link between all pairs of nodes in the subset. In other words, the nodes in the set are fully connected. Furthermore, a <code>maximal clique</code> is a clique such that it is not possible to include any other nodes from the graph in the set without it ceasing to be a clique.</p>
<p><img src='/images/ML/gm_10.png' width="600"></p>
<p><br></p>
<p>We can therefore define the factors in the decomposition of the joint distribution to be functions of the variables in the cliques. In fact, we can consider functions of the maximal cliques, without loss of generality because other cliques must be subsets of maximal cliques.</p>
<p>Let <span class="math inline">\(C\)</span> be a clique and the set of random variables in that clique by <span class="math inline">\(\mathbf{X}_C\)</span>. Then the joint distribution is written as a product of <code>potential functions</code> <span class="math inline">\(\psi_C(\mathbf{x}_C) \geq 0\)</span> over the maximal cliques of the graph:</p>
<p><span class="math display">\[P(\mathbf{X}) = \frac{1}{Z} \prod_{C} \psi_{C} (\mathbf{X}_C)\]</span></p>
<p>Here the quantity <span class="math inline">\(Z\)</span> is called <code>partition function</code> which is used for normalization to ensure the result is a proper joint distribution:</p>
<p><span class="math display">\[Z = \sum_{X} \prod_{C} \psi_{C} (\mathbf{X}_C)\]</span></p>
<p>In directed graph, we have the links to be conditional distribution, in undirected graph, we do not restrict the choice of potential functions.</p>
<h1 id="ref">Ref</h1>
<p>PRML chapter 8</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>
<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhu, Zhaoyang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Symbols count total">878k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">13:18</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.3" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{&quot;enable&quot;:true,&quot;app_id&quot;:&quot;gPOc95wWW2Hwp2pkVSAAz28m-MdYXbMMI&quot;,&quot;app_key&quot;:&quot;7Sf65xCHXfEEdBvu29UHSYdV&quot;,&quot;server_url&quot;:&quot;https:&#x2F;&#x2F;swag1ong.github.io&quot;,&quot;security&quot;:true}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{&quot;enable&quot;:true,&quot;repo&quot;:&quot;swag1ong&#x2F;swag1ong.github.io&quot;,&quot;issue_term&quot;:&quot;pathname&quot;,&quot;theme&quot;:&quot;github-light&quot;,&quot;pathname&quot;:&quot;pathname&quot;}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
