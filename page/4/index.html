<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="1xbHmVEsS5Fta14x1d1FXkHFr93LRr1pMxipdYVcNI4">
  <meta name="baidu-site-verification" content="code-Srzz2vTuzA">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;swag1ong.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;Searching...&quot;,&quot;empty&quot;:&quot;We didn&#39;t find any results for the search: ${query}&quot;,&quot;hits_time&quot;:&quot;${hits} results found in ${time} ms&quot;,&quot;hits&quot;:&quot;${hits} results found&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:3,&quot;unescape&quot;:false,&quot;preload&quot;:true}}</script>
<meta property="og:type" content="website">
<meta property="og:title" content="GoGoGogo!">
<meta property="og:url" content="https://swag1ong.github.io/page/4/index.html">
<meta property="og:site_name" content="GoGoGogo!">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhu, Zhaoyang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://swag1ong.github.io/page/4/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:true,&quot;isPost&quot;:false,&quot;lang&quot;:&quot;en&quot;,&quot;comments&quot;:&quot;&quot;,&quot;permalink&quot;:&quot;&quot;,&quot;path&quot;:&quot;page&#x2F;4&#x2F;index.html&quot;,&quot;title&quot;:&quot;&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>GoGoGogo!</title><script src="/js/config.js"></script>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?852b005027c5efa81663f6f5c4c5b7fd"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">GoGoGogo!</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-igloo fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">97</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">16</span></a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">25</span></a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-address-card fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhu, Zhaoyang"
      src="/images/others/favicon.jpeg">
  <p class="site-author-name" itemprop="name">Zhu, Zhaoyang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">97</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/swag1ong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;swag1ong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyang.zhu@mail.utoronto.ca" title="E-Mail → mailto:zhaoyang.zhu@mail.utoronto.ca" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zhuzhaoyangzzyu0616" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zhuzhaoyangzzyu0616" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/19/k-means/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/k-means/" class="post-title-link" itemprop="url">K-Means</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-19 16:32:37" itemprop="dateCreated datePublished" datetime="2021-07-19T16:32:37+08:00">2021-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-08-19 15:11:17" itemprop="dateModified" datetime="2021-08-19T15:11:17+08:00">2021-08-19</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/07/19/k-means/" class="post-meta-item leancloud_visitors" data-flag-title="K-Means" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>3.8k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>3 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="k-means">K-Means</h1>
<p>Considering the problem of identifying groups, or clusters of data points in a multidimensional space.</p>
<p>Suppose we have a dataset <span class="math inline">\(\mathbf{x}_1, ....., \mathbf{x}_N\)</span> consisting of <span class="math inline">\(N\)</span> observations of a random <span class="math inline">\(D\)</span> dimensional euclidean variable <span class="math inline">\(\mathbf{x}\)</span>. <strong>Our goal is to partition these points in to <span class="math inline">\(K\)</span> clusters</strong>.</p>
<p>A cluster <span class="math inline">\(k\)</span> contains:</p>
<ol type="1">
<li><strong>Cluster center</strong>: <span class="math inline">\(\boldsymbol{\mu}_k \in \mathbb{R}^{D}\)</span>.</li>
<li><strong>Assignment indicator variables</strong>: <span class="math inline">\(\mathbf{r}_{n} \in \mathbb{R}^{K}, \; r_{nk} \in \{0, 1\}\)</span>, one for each data point and each dimension <span class="math inline">\(k\)</span> indicates whether the data point <span class="math inline">\(\mathbf{x}_n\)</span> belongs to cluster <span class="math inline">\(k\)</span></li>
</ol>
<p>So we can reformulate our goal to be: <strong>find an assignment of data points to clusters, as well as a set of cluster centers <span class="math inline">\(\{\boldsymbol{\mu}_k\}^{K}_{k=1}\)</span>, such that the sum of squares of the distances of each data point to its closest cluster center is a minimum.</strong> In equation, our objective is:</p>
<p><span class="math display">\[J(\mathbf{x}; \; \{\boldsymbol{\mu}_{k}, \mathbf{r}_{nk}\}) = \sum^{N}_{n=1}\sum^{K}_{k=1} r_{nk} \|\mathbf{x}_n - \boldsymbol{\mu}_k\|^2_2\]</span></p>
<p>Where the distance measure here is the L2 norm.</p>
<p>Thus, we want to find parameters <span class="math inline">\(\{\boldsymbol{\mu}_{k}, \mathbf{r}_{nk}\}\)</span> such that the objective is minimized.</p>
<h2 id="algorithm">Algorithm</h2>
<ol type="1">
<li>Initialize <span class="math inline">\(\boldsymbol{\mu}_k, \; \forall \; k=1, ...., K\)</span></li>
<li>Given <span class="math inline">\(\{\boldsymbol{\mu}_k\}\)</span>, minimize <span class="math inline">\(J\)</span> w.r.t <span class="math inline">\(\{\mathbf{r}_{n}\}\)</span></li>
<li>Given <span class="math inline">\(\{\mathbf{r}_{n}\}\)</span>, minimize <span class="math inline">\(J\)</span> w.r.t <span class="math inline">\(\{\boldsymbol{\mu}_k\}\)</span></li>
<li>Repeat 2, 3 until converges</li>
</ol>
<p><strong>For phase 2</strong>:</p>
<p>Given the cluster centers, it is obvious that if we assign each point to the closest cluster center, we have the minimum objective:</p>
<p><span class="math display">\[
r_{nk}=
\begin{cases}
1, \quad \text{if }\; k = \underset{k}{\arg\min} \|\mathbf{x}_n - \boldsymbol{\mu}_k\|^2_2\\
0, \quad o.w\\
\end{cases}
\]</span></p>
<p><strong>For phase 3</strong>:</p>
<p>Given the assignments, since the objective is convex, we can take gradient and solve for the optimal <span class="math inline">\(\boldsymbol{\mu}_k\)</span>:</p>
<p><span class="math display">\[\frac{\partial J}{\partial \boldsymbol{\mu}_k} = \sum^{N}_{n=1} -2 r_{nk} \mathbf{x}_n + 2\boldsymbol{\mu}_k \sum^{N}_{n=1} r_{nk} = 0\]</span></p>
<p><span class="math display">\[\implies \boldsymbol{\mu}_k \sum^{N}_{n=1} r_{nk} = \sum^{N}_{n=1} r_{nk} \mathbf{x}_n\]</span></p>
<p><span class="math display">\[\implies \boldsymbol{\mu}_k = \frac{\sum^{N}_{n=1} r_{nk} \mathbf{x}_n}{\sum^{N}_{n=1} r_{nk}}\]</span></p>
<p><span class="math inline">\(\sum^{N}_{n=1} r_{nk} \mathbf{x}_n\)</span> is the sum of all points that belongs to cluster <span class="math inline">\(k\)</span> and <span class="math inline">\(\sum^{N}_{n=1} r_{nk}\)</span> is the count of points in cluster <span class="math inline">\(k\)</span>, so the new <span class="math inline">\(\boldsymbol{\mu}_k\)</span> is just the <strong>sample mean</strong> of the points in cluster <span class="math inline">\(k\)</span>.</p>
<h2 id="convergence">Convergence</h2>
<p>Since:</p>
<ol type="1">
<li><span class="math inline">\(\{\boldsymbol{\mu}_{k}, \mathbf{r}_{nk}\}\)</span> can only take finite values (i.e they are derived from finite subsets of data and uniquely defined for each subset).</li>
<li>At each step, we minimize the objective (i.e we either decrease or maintain the objective).</li>
<li>The cost function is bounded below zero.</li>
<li>Ties are broken consistently.</li>
</ol>
<p>Thus, the algorithm can only take a finite number of non-decreasing steps before terminating at a local minimum.</p>
<h2 id="implementation">Implementation</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KMeans</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, k=<span class="number">2</span>, max_iteration=<span class="number">10</span>, init_method=<span class="string">&#x27;random&#x27;</span></span>):</span></span><br><span class="line">        self.k = k</span><br><span class="line">        self.max_iteration = max_iteration</span><br><span class="line">        self.init_method = init_method</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span>(<span class="params">self, x: np.array</span>):</span></span><br><span class="line">        n, d = x.shape</span><br><span class="line">        curr_mu = self._init_mu(x)</span><br><span class="line">        curr_iter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> curr_iter &lt;= self.max_iteration:</span><br><span class="line">            r_matrix = np.zeros((n, self.k))</span><br><span class="line">            <span class="comment"># step one, map each instance to cluster center muk</span></span><br><span class="line">            <span class="keyword">for</span> sample <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                distance = []</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(self.k):</span><br><span class="line">                    distance.append(np.linalg.norm(x[sample] - curr_mu[j]))</span><br><span class="line"></span><br><span class="line">                mu_assigned = np.argmin(distance)</span><br><span class="line">                r_matrix[sample][mu_assigned] = <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">            <span class="comment"># step two calculate new mu_k</span></span><br><span class="line">            prev_mu = curr_mu.copy()</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(self.k):</span><br><span class="line">                curr_mu[j] = np.mean(x[r_matrix[:, j] == <span class="number">1</span>], axis=<span class="number">0</span>)</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># check stopping criteria</span></span><br><span class="line">            <span class="keyword">if</span> np.linalg.norm(prev_mu - curr_mu) == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            curr_iter += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;finished k-means algorithm, on iteration <span class="subst">&#123;curr_iter&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> r_matrix, curr_mu</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_mu</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.init_method == <span class="string">&#x27;random&#x27;</span>:</span><br><span class="line">            col_max = x.<span class="built_in">max</span>(axis=<span class="number">0</span>)</span><br><span class="line">            col_min = x.<span class="built_in">min</span>(axis=<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> (col_max - col_min) * np.random.random_sample((self.k, x.shape[<span class="number">1</span>])) + col_min</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> self.init_method == <span class="string">&#x27;random_points&#x27;</span>:</span><br><span class="line">            random_int = np.random.randint(<span class="number">0</span>, x.shape[<span class="number">0</span>], size=self.k)</span><br><span class="line">            <span class="keyword">return</span> x[random_int]</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/19/pca/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/pca/" class="post-title-link" itemprop="url">PCA</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-19 16:32:26" itemprop="dateCreated datePublished" datetime="2021-07-19T16:32:26+08:00">2021-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-08-26 16:46:58" itemprop="dateModified" datetime="2021-08-26T16:46:58+08:00">2021-08-26</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/07/19/pca/" class="post-meta-item leancloud_visitors" data-flag-title="PCA" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>3.6k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>3 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="principal-component-analysis">Principal Component Analysis</h1>
<p>Let <span class="math inline">\([\mathbf{X}_1 ... \mathbf{X}_N]\)</span> be a <span class="math inline">\(p \times N\)</span> matrix of random sample where <span class="math inline">\(\mathbf{X}_i \in \mathbb{R}^p\)</span>. The sample mean <span class="math inline">\(\mathbf{M}\)</span> of the random sample is defined as:</p>
<p><span class="math display">\[\mathbf{M} = \frac{1}{N} \sum^{N}_{i=1} \mathbf{X}_{i}\]</span></p>
<p>Let <span class="math inline">\(\hat{\mathbf{X}}_k\)</span> be the centered random vector for <span class="math inline">\(k = 1, ...., N\)</span>:</p>
<p><span class="math display">\[\hat{\mathbf{X}}_k = \mathbf{X}_k - \mathbf{M}\]</span></p>
<p>Then we can defined the centered random sample matrix as:</p>
<p><span class="math display">\[B = [\hat{\mathbf{X}}_1 ... \hat{\mathbf{X}}_N]_{[p\times N]}\]</span></p>
<p>The sample covariance matrix <span class="math inline">\(S\)</span> is then:</p>
<p><span class="math display">\[S = [\frac{1}{N} BB^T]_{[p\times p]}\]</span></p>
<p>We can easily show that <span class="math inline">\(S\)</span> is positive semi-definite. Assume that <span class="math inline">\(\mathbf{x} &gt; 0\)</span>:</p>
<span class="math display">\[\begin{aligned}
BB^T \mathbf{x} &amp;= \lambda \mathbf{x}\\
\implies \mathbf{x}^T BB^T \mathbf{x} &amp;= \lambda \mathbf{x}^T \mathbf{x}\\
\implies \frac{\mathbf{x}^T BB^T \mathbf{x}}{\mathbf{x}^T \mathbf{x}} &amp;= \lambda
\end{aligned}\]</span>
<p>Let <span class="math inline">\(\mathbf{A} = B^T\mathbf{x}\)</span>, then:</p>
<p><span class="math display">\[\frac{\mathbf{A}^T \mathbf{A}}{\mathbf{x}^T \mathbf{x}} = \lambda\]</span></p>
<p>Since, <span class="math inline">\(\mathbf{A}^T \mathbf{A} \geq 0\)</span> for any vector <span class="math inline">\(A\)</span>, similar for <span class="math inline">\(\mathbf{x}^T \mathbf{x}\)</span>, then:</p>
<p><span class="math display">\[\lambda \geq 0\]</span></p>
<p>Since, <span class="math inline">\(S\)</span> is symmetric and <span class="math inline">\(\lambda \geq 0\)</span> for all eigenvalues of <span class="math inline">\(S\)</span>, we can conclude that <span class="math inline">\(S\)</span> is <strong>positive semi-definite</strong>.</p>
<p>The <strong>total variance</strong> in the data is:</p>
<p><span class="math display">\[tr(S)\]</span></p>
<h2 id="pca">PCA</h2>
<p><strong>The goal of PCA is to find an orthogonal <span class="math inline">\(p \times p\)</span> matrix <span class="math inline">\(P = [\mathbf{u}_1 .... \mathbf{u}_p]\)</span> that determines a change of variable, <span class="math inline">\(\mathbf{X} = P\mathbf{Y}\)</span> with the property that the features of <span class="math inline">\(\mathbf{Y}\)</span> are uncorrelated and are arranged in order of decreasing variance.</strong></p>
<p>Assume the <span class="math inline">\(\mathbf{X}\)</span> is already being centered, that is <span class="math inline">\(B = [\mathbf{X}_1 .... \mathbf{X}_N]\)</span>, then <span class="math inline">\(\mathbf{Y}\)</span> is also centered since <span class="math inline">\(P \neq 0\)</span>:</p>
<p><span class="math display">\[E[\mathbf{X}] = P\cdot E[\mathbf{Y}] = 0\]</span></p>
<p>Then the sample covariance matrix of <span class="math inline">\(\mathbf{Y}\)</span> is:</p>
<p><span class="math display">\[S_Y = \frac{1}{N-1}[P^{-1}\mathbf{X}_1 .... P^{-1}\mathbf{X}_N] [P^{-1}\mathbf{X}_1 .... P^{-1}\mathbf{X}_N]^T\]</span></p>
<p><span class="math display">\[\implies S_{\mathbf{Y}} = \frac{1}{N-1} P^{T}BB^{T}P = P^{T} S_{\mathbf{X}} P\]</span></p>
<p>So the desired orthogonal matrix is the one that makes <span class="math inline">\(\hat{Cov}[Y_i, Y_j] = 0, \; \forall i \neq j\)</span> (features are uncorrelated), which means that the we want the sample covariance matrix <span class="math inline">\(S_Y\)</span> to be diagonal.</p>
<p><br></p>
<p>Let <span class="math inline">\(D\)</span> be a diagonal matrix with eigenvalues of <span class="math inline">\(S_{\mathbf{X}}\)</span>, <span class="math inline">\(\lambda_1 ,...., \lambda_p\)</span> on the diagonal s.t <span class="math inline">\(\lambda_1 \geq \lambda_2 \geq ..... \lambda_p \geq 0\)</span>, let <span class="math inline">\(P\)</span> be an orthogonal matrix whose columns are the corresponding unit eigenvectors <span class="math inline">\(\mathbf{u}_1, ....., \mathbf{u}_p\)</span> (Symmetric matrices have property that the eigenspaces are mutually orthogonal, in the sense that eigenvectors corresponding to different eigenvalues are orthogonal). Then, we can use <span class="math inline">\(P\)</span> and <span class="math inline">\(\mathbf{X}\)</span> to represent <span class="math inline">\(Y\)</span>, and the sample covariance matrix is:</p>
<p><span class="math display">\[S_{\mathbf{X}} = PDP^T \implies P^{-1} S_{\mathbf{X}} P = D\]</span></p>
<p>Thus, <span class="math inline">\(S_{\mathbf{Y}} = D\)</span>.</p>
<p>Then, the eigenvectors <span class="math inline">\(\mathbf{u}_1, ...., \mathbf{u}_p\)</span> are called <code>Principal Components</code> of the data. The first PC is the eigenvector corresponding to the largest eigenvalue of <span class="math inline">\(S_{\mathbf{X}}\)</span>.</p>
<p>The result transformation is defiend as:</p>
<p><span class="math display">\[P^{T} \mathbf{X} = \mathbf{Y}\]</span></p>
<p><span class="math display">\[
\mathbf{Y} = 
\begin{bmatrix}
\mathbf{u}^T_1 \mathbf{X} \\
.\\
.\\
.\\
\mathbf{u}^T_p \mathbf{X}\\
\end{bmatrix}
\]</span></p>
<h2 id="total-variance">Total Variance</h2>
<p>It can be shown that the PCA transformation does not change the total variance of the data, that is the total variance of the data is the sum of eigenvalues:</p>
<p><span class="math display">\[tr(S_{\mathbf{Y}}) = tr(D)\]</span></p>
<p>This is only true when we are dealing with sample covariance matrix,</p>
<h1 id="implementation">Implementation</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span>(<span class="params">X, k=<span class="number">2</span></span>):</span></span><br><span class="line">    u, s, vt = np.linalg.svd(X, full_matrices=<span class="literal">False</span>)</span><br><span class="line">    xv = u * s</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> xv[:, :k]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span>(<span class="params">X</span>):</span></span><br><span class="line">    n, d = X.shape</span><br><span class="line">    x_trans = X.copy()</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(d):</span><br><span class="line">        <span class="comment"># normalization</span></span><br><span class="line">        x_trans[:, j] = (X[:, j] - X[:, j].mean()) / X[:, j].std()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_trans</span><br></pre></td></tr></table></figure>
<h1 id="ref">Ref</h1>
<p>https://stats.stackexchange.com/questions/266864/why-is-the-sum-of-eigenvalues-of-a-pca-equal-to-the-original-variance-of-the-dat</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/19/svm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/svm/" class="post-title-link" itemprop="url">SVM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-19 16:32:20" itemprop="dateCreated datePublished" datetime="2021-07-19T16:32:20+08:00">2021-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-09-10 20:11:23" itemprop="dateModified" datetime="2021-09-10T20:11:23+08:00">2021-09-10</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/07/19/svm/" class="post-meta-item leancloud_visitors" data-flag-title="SVM" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>10k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>9 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="support-vector-machine">Support Vector Machine</h1>
<p>Given dataset <span class="math inline">\(D = \{(\mathbf{x}_1, y_1) ....., (\mathbf{x}_N, y_N); \; \mathbf{x} \in \mathbb{R}^m, \; y_i \in \{-1, 1\}\}\)</span>. Let <span class="math inline">\(\mathbf{w}^T \mathbf{x} + b = 0\)</span> be a hyperplane, we classify a new point <span class="math inline">\(\mathbf{x}_i\)</span> by</p>
<p><span class="math display">\[
\hat{y}_i (\mathbf{x}_i) =
\begin{cases}
\mathbf{w}^T \mathbf{x}_i + b \geq 0, \quad \;\;\; 1\\
\mathbf{w}^T \mathbf{x}_i + b &lt; 0, \quad -1\\
\end{cases}
\]</span></p>
<p>Suppose that our data is <strong>linear separable</strong>. Then, <span class="math inline">\(\exists\)</span> at least one possible combination of parameters <span class="math inline">\(\{\mathbf{w}, b\}\)</span>, such that:</p>
<p><span class="math display">\[\hat{\gamma}_i = y_i \hat{y}_i(\mathbf{x}_i) &gt; 0, \; \forall i=1, ...., N\]</span></p>
<p>When <span class="math inline">\(y_i = 1\)</span>, a confident classifier would have <span class="math inline">\(\hat{y}_i(\mathbf{x}_i)\)</span> as large as possible. On the other hand, when <span class="math inline">\(y_i = -1\)</span>, the confident classifier would have <span class="math inline">\(\hat{y}_i(\mathbf{x}_i)\)</span> as negative as possible. Thus, we want <span class="math inline">\(\hat{\gamma}_i\)</span> as large as possible, this <span class="math inline">\(\hat{\gamma}_i\)</span> is called <code>functional margin</code> associated with training example for specific set of parameters <span class="math inline">\(\{\mathbf{w}, b\}\)</span>. And <code>functional margin</code> of <span class="math inline">\(\{\mathbf{w}, b\}\)</span> is defined as minimum of these functions margins:</p>
<p><span class="math display">\[\hat{\gamma} = \min_{i=1, ..., N} \hat{\gamma}_i\]</span></p>
<p><strong>In support vector machines the decision boundary is chosen to be the one for which the functional margin (confidence) is maximized.</strong></p>
<h2 id="distance-to-plane">Distance to Plane</h2>
<p>Let <span class="math inline">\(\mathbf{x}_i\)</span> be a sample that has label <span class="math inline">\(y_i = 1\)</span>, thus, it is on the positive side of the hyperplane <span class="math inline">\(\mathbf{w}^T \mathbf{x} + b = 0\)</span>. Define <span class="math inline">\(r\)</span> to be the shortest distance between point <span class="math inline">\(\mathbf{x}_i\)</span> and the hyperplane. Then <span class="math inline">\(r\)</span> is the distance between <span class="math inline">\(\mathbf{x}_i\)</span> to its projection on the hyperplane <span class="math inline">\(\mathbf{x}^{\prime}_i\)</span>.</p>
<p><img src='/images/ML/svm_1.png' width="600"></p>
<p>Since <span class="math inline">\(\mathbf{w}\)</span> is the normal vector that is orthogonal to the plane and <span class="math inline">\(\frac{\mathbf{w}}{\|\mathbf{w}\|_2}\)</span> is the unit vector that represents its direction. We can write the <span class="math inline">\(r\)</span> as:</p>
<p><span class="math display">\[\mathbf{x}_i - \mathbf{x}^{\prime}_i = r \frac{\mathbf{w}}{\|\mathbf{w}\|_2}\]</span></p>
<p><span class="math display">\[\implies r = \frac{\mathbf{x}_i - \mathbf{x}^{\prime}_i}{\frac{\mathbf{w}}{\|\mathbf{w}\|_2}}\]</span></p>
<h2 id="goal">Goal</h2>
<p>The concept of the margin is intuitively simple: it is the distance of the separating hyperplane to the closest examples in the dataset, assuming that our dataset is <strong>linearly separable.</strong> That is:</p>
<span class="math display">\[\begin{aligned}
&amp;\max \quad &amp;&amp; \hat{\gamma}\\
&amp;\;\text{s.t} \quad &amp;&amp;y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq \hat{\gamma} \quad \quad \forall i=1, ...., N
\end{aligned}\]</span>
<p>This optimization problem is unbounded, because one can make the functional margin large by simply scaling the parameters by a constant <span class="math inline">\(c\)</span>, <span class="math inline">\(\{c\mathbf{w}, cb\}\)</span>:</p>
<p><span class="math display">\[y_i (c\mathbf{w}^T \mathbf{x}_i + cb) &gt; y_i (\mathbf{w}^T \mathbf{x}_i + b) = \hat{\gamma}\]</span></p>
<p>This has no effect on the decision plane because:</p>
<p><span class="math display">\[\mathbf{w}^T \mathbf{x}_i + b = c\mathbf{w}^T \mathbf{x}_i + cb = 0\]</span></p>
<p>Thus, we need to transform the optimization problem to <strong>maximize the distance between the samples and decision boundary</strong> instead of maximizing functional margin. Suppose we let all functional margins to be at least 1 (can easily achieve by multiplying parameters by a constant):</p>
<p><span class="math display">\[y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1\]</span></p>
<p><span class="math display">\[\implies \mathbf{w}^T \mathbf{x}_i + b \geq 1\]</span></p>
<p><span class="math display">\[\implies \mathbf{w}^T \mathbf{x}_i + b \leq -1\]</span></p>
<p>Then, for point <span class="math inline">\(\mathbf{x}_i\)</span> on <span class="math inline">\(\mathbf{w}^T \mathbf{x}_i + b = 1\)</span>, we have its distance to the decision plane:</p>
<p><span class="math display">\[\mathbf{w}^T \mathbf{x}_i + b - r \frac{\|\mathbf{w}\|^2_2}{\|\mathbf{w}\|_2} = 0\]</span></p>
<p><span class="math display">\[\implies r = \frac{1}{\|\mathbf{w}\|}\]</span></p>
<p>Then, we can formulate our objective as:</p>
<span class="math display">\[\begin{aligned}
&amp;\max_{\mathbf{w}, b} \quad &amp;&amp; \frac{1}{\|\mathbf{w}\|_2}\\
&amp;\;\text{s.t} \quad &amp;&amp;y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 \quad \quad \forall i=1, ...., N
\end{aligned}\]</span>
<p>Which is equivalent to:</p>
<span class="math display">\[\begin{aligned}
&amp;\min_{\mathbf{w}, b} \quad &amp;&amp; \frac{1}{2}\|\mathbf{w}\|^2_2\\
&amp;\;\text{s.t} \quad &amp;&amp;y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 \quad \quad \forall i=1, ...., N
\end{aligned}\]</span>
<p><br></p>
<p><img src='/images/ML/svm_2.png' width="600"></p>
<h2 id="soft-margin-svm">Soft Margin SVM</h2>
<h3 id="slack-variables">Slack Variables</h3>
<p>Notice, in the above formulation, we have hard constraints on the margins which do not allow misclassification of points. However, in real world, data points are rarely linear separable and there will be outliers in the dataset, we may wish to allow some examples to be on the wrong side of the hyperplane or to have margin less than 1 .</p>
<p><img src='/images/ML/svm_3.png' width="600"></p>
<p>To resolve this problem, we can introduce slack variables one for each data point to relax the hard constraints:</p>
<p><span class="math display">\[y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i\]</span> <span class="math display">\[\quad\xi_i \geq 0, \; \; \forall i=1, ...., N\]</span></p>
<p>To encourage correct classification of the samples, we add <span class="math inline">\(\xi_i\)</span> to the objective:</p>
<span class="math display">\[\begin{aligned}
&amp;\min_{\mathbf{w}, b, \boldsymbol{\xi}} \quad &amp;&amp; \frac{1}{2}\|\mathbf{w}\|^2_2 + C\sum^{N}_{n=1} \xi_i\\
&amp;\;\text{s.t} \quad &amp;&amp;y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i  \quad &amp;&amp;&amp;\forall i=1, ...., N\\
&amp; &amp;&amp; \xi_i \geq 0, &amp;&amp;&amp;\forall i=1, ...., N
\end{aligned}\]</span>
<p>Thus, sample points are now permitted to have margin less than 1, and if an example <span class="math inline">\(\mathbf{x}_i\)</span> has slack variable greater than 0, we would have penalty in the objective function <span class="math inline">\(C\xi_i\)</span>. The parameter <span class="math inline">\(C\)</span> controls the relative weighting between the twin goals of making the <span class="math inline">\(\|\mathbf{w}\|\)</span> small and of ensuring that most examples have functional margin at least 1.</p>
<p><br></p>
<h3 id="dual-problem">Dual Problem</h3>
<p>Using <strong>Lagrange Multiplier</strong>, we can transform the constrained problem into an unconstrained concave problem:</p>
<p><span class="math display">\[\max_{\boldsymbol{\alpha}, \boldsymbol{\eta}}\;\min_{\mathbf{w}, b, \boldsymbol{\xi}} \; \frac{1}{2}\|\mathbf{w}\|^2_2 + C\sum^{N}_{n=1} \xi_i - \sum^N_{i=1} \alpha_i [y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 + \xi_i] - \sum^{N}_{i=1} \eta_i \xi_i\]</span></p>
<p>Where the inner minimization is the dual function and the maximization w.r.t <span class="math inline">\(\alpha\)</span> is called dual problem.</p>
<h3 id="kkt">KKT</h3>
<p>For an unconstrained convex optimization problem, we know we are at global minimum if the gradient is zero. The KKT conditions are the equivalent conditions for the global minimum of a constrained convex optimization problem. <span class="math inline">\(\forall i=1, ...., N\)</span>:</p>
<ol type="1">
<li><p><strong>Stationarity</strong>, If the strong duality holds, <span class="math inline">\((\mathbf{w}^*, \boldsymbol{\alpha}^*)\)</span> is optimal, then <span class="math inline">\(\mathbf{w}^*\)</span> minimizes <span class="math inline">\(L(\mathbf{w}^*, \boldsymbol{\alpha}^*)\)</span> (same for <span class="math inline">\(b^*, \xi^*\)</span> which are formulated as constraints in the dual problem):</p>
<p><span class="math display">\[\nabla_{\mathbf{w}} L(\mathbf{w}^*, \boldsymbol{\alpha}^*) = 0\]</span> <span class="math display">\[\implies \mathbf{w}^* = \sum_{i=1}^{n} \alpha^*_i y_i \mathbf{x}_i\]</span></p></li>
<li><p><strong>Complementary Slackness</strong>: <span class="math display">\[\alpha_i [y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 + \xi_i] = 0\]</span></p></li>
<li><p><strong>Primal Feasibility</strong>: <span class="math display">\[y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 + \xi_i \geq 0\]</span> <span class="math display">\[\eta_i\xi_i = 0\]</span></p></li>
<li><p><strong>Dual Feasibility</strong>: <span class="math display">\[\alpha_i, \eta_i, \xi_i \geq 0\]</span></p></li>
</ol>
<h4 id="solving-dual-problem">Solving Dual Problem</h4>
<p>We now solve for the dual function by fixing <span class="math inline">\(\{\alpha_i\, \eta_i\}\)</span> (satisfying Stationarity condition):</p>
<p><span class="math display">\[\min_{\mathbf{w}, b, \boldsymbol{\xi}} \frac{1}{2}\|\mathbf{w}\|^2_2 + C\sum^{N}_{n=1} \xi_i - \sum^N_{i=1} \alpha_i [y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 + \xi_i] - \sum^{N}_{i=1} \eta_i \xi_i\]</span></p>
<span class="math display">\[\begin{aligned}
&amp; \frac{\partial L(\mathbf{w}, b, \boldsymbol{\xi})}{\partial \mathbf{w}} = \mathbf{w} - \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i = 0 \\
&amp; \implies \mathbf{w}^* = \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i \\
&amp; \frac{\partial L(\mathbf{w},, b, \boldsymbol{\xi})}{\partial b} =\sum_{i=1}^{n} \alpha_i y_i = 0 \\
&amp; \implies \sum_{i=1}^{n} \alpha_i y_i = 0 \\
&amp; \frac{\partial L(\mathbf{w}, b, \boldsymbol{\xi})}{\partial \xi_n} = C - \alpha_n - \eta_n = 0 \\
&amp; \implies \alpha_n = C - \eta_n
\end{aligned}\]</span>
<p>Substitute back to the original equation, we obtain the dual function:</p>
<p><span class="math display">\[g(\boldsymbol{\alpha}) = -\frac{1}{2}\sum_{i=1}^{N}\sum_{k=1}^{N}\alpha_i \alpha_k {\mathbf{x}_i}^T \mathbf{x}_k y_i y_k + \sum_{i=1}^{N} \alpha_i\]</span></p>
<p>Then, we have the dual problem:</p>
<span class="math display">\[\begin{aligned}
&amp; \underset{\boldsymbol{\alpha}}{\text{max}}
&amp; &amp;  g(\boldsymbol{\alpha}) = -\frac{1}{2}\sum_{i=1}^{N}\sum_{k=1}^{N}\alpha_i \alpha_k {\mathbf{x}_i}^T \mathbf{x}_k y_i y_k + \sum_{i=1}^{N} \alpha_i\\
&amp; \text{subject to}
&amp; &amp; 0 \leq \alpha_i \leq C \\
&amp; &amp; &amp; \sum_{i=1}^{n} \alpha_i y_i = 0 \\
\end{aligned}\]</span>
<p>This is a quadratic programming problem that we can solve using quadratic programming.</p>
<h3 id="interpretation">Interpretation</h3>
<p>We could conclude:</p>
<ol type="1">
<li><p>if <span class="math inline">\(0 &lt; \alpha_i &lt; C \implies y_i(w^T x_i + b) = 1 - \xi_i\)</span> Since <span class="math inline">\(\alpha_i = C - \mu_i, \mu_i \geq 0\)</span>, we have <span class="math inline">\(\xi_i =0 \implies\)</span> the points are with <span class="math inline">\(0 &lt; \alpha_i &lt; C\)</span> are on the margin</p></li>
<li><p>if <span class="math inline">\(\alpha_i = C\)</span></p>
<ul>
<li><span class="math inline">\(0 &lt; \xi_i &lt; 1\)</span>: the points are inside the margin on the correct side</li>
<li><span class="math inline">\(\xi_i = 1\)</span>: the points are on the decision boundary</li>
<li><span class="math inline">\(\xi_i &gt; 1\)</span>: the points are inside the wrong side of the margin and misclassified</li>
</ul></li>
<li><p>if <span class="math inline">\(\alpha_i = 0\)</span>, the points are not support vectors, have no affect on the weight.</p></li>
</ol>
<p>After finding the optimal values for <span class="math inline">\(\boldsymbol{\alpha}\)</span>, we obtain optimal <span class="math inline">\(\mathbf{w}^*\)</span> by solving:</p>
<p><span class="math display">\[\mathbf{w}^* = \sum_{i=1}^{n} \alpha^*_i y_i \mathbf{x}_i\]</span></p>
<p>We obtain optimal <span class="math inline">\(b^*\)</span> by realizing that the points on the margins have <span class="math inline">\(0 &lt; \alpha_i &lt; C\)</span>. Let <span class="math inline">\(\mathbf{x}_i\)</span> be one of those points, then:</p>
<p><span class="math display">\[{\mathbf{w^*}}^T \mathbf{x}_i + b = y_i\]</span></p>
<p>Let <span class="math inline">\(M\)</span> be the set of all points that lies exactly on the margin, a more stable solution is obtained by averaging over all points:</p>
<p><span class="math display">\[b^* = \frac{1}{N_m} \sum^{N_m}_{i=1} (y_i - {\mathbf{w^*}}^T\mathbf{x}_i)\]</span></p>
<h2 id="kernel-tricks">Kernel Tricks</h2>
<h1 id="implementation">Implementation</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cvxopt <span class="keyword">import</span> matrix, solvers</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> qpsolvers <span class="keyword">import</span> solve_qp</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVM</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, c=<span class="number">1</span>, kernel=<span class="string">&#x27;linear&#x27;</span></span>):</span></span><br><span class="line">        self.c = c</span><br><span class="line">        self.kernel = kernel</span><br><span class="line">        self.b = <span class="literal">None</span></span><br><span class="line">        self.dual_coef_ = <span class="literal">None</span></span><br><span class="line">        self.decision_matrix = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y</span>):</span></span><br><span class="line">        n, d = X.shape</span><br><span class="line">        y = y.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        yyt = np.matmul(y, y.T)</span><br><span class="line">        P = np.zeros((n, n))</span><br><span class="line">        q = matrix(-np.ones((n, <span class="number">1</span>)))</span><br><span class="line">        a = matrix(y.T, tc=<span class="string">&#x27;d&#x27;</span>)</span><br><span class="line">        b = matrix([<span class="number">0.0</span>])</span><br><span class="line">        G = matrix(np.row_stack([np.diag([-<span class="number">1</span>] * n), np.diag([<span class="number">1</span>] * n)]), tc=<span class="string">&#x27;d&#x27;</span>)</span><br><span class="line">        h = matrix(np.row_stack([np.array([<span class="number">0</span>] * n).reshape(n, <span class="number">1</span>),</span><br><span class="line">                                 np.array([self.c] * n).reshape(n, <span class="number">1</span>)]), tc=<span class="string">&#x27;d&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                P[i][j] = self.apply_kernel(X[i], X[j])</span><br><span class="line"></span><br><span class="line">        P = matrix(P * yyt)</span><br><span class="line">        alpha = np.array(solvers.qp(P, q, G, h , a, b)[<span class="string">&#x27;x&#x27;</span>])</span><br><span class="line">        alpha[alpha &lt; np.mean(alpha) * <span class="number">0.1</span>] = <span class="number">0</span></span><br><span class="line">        temp_x = np.column_stack([X, alpha, y])</span><br><span class="line">        m = temp_x[(temp_x[:, -<span class="number">2</span>] &gt; <span class="number">0</span>) &amp; (temp_x[:, -<span class="number">2</span>] &lt; self.c)]</span><br><span class="line">        N_m = <span class="built_in">len</span>(m)</span><br><span class="line">        self.decision_matrix = m[:, :-<span class="number">2</span>]</span><br><span class="line">        self.b = <span class="number">0</span></span><br><span class="line">        self.dual_coef_ = m[:, -<span class="number">1</span>] * m[:, -<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">## get b</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N_m):</span><br><span class="line">            self.b += m[i, -<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(N_m):</span><br><span class="line">                self.b -= m[j, -<span class="number">2</span>] * m[j, -<span class="number">1</span>] * self.apply_kernel(m[i, :-<span class="number">2</span>], m[j, :-<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        self.b = self.b / N_m</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply_kernel</span>(<span class="params">self, x_1, x_2</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.kernel == <span class="string">&#x27;linear&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> np.dot(x_1, x_2)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decision_function</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        pred_results = np.array([])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">            pred = self.b</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.decision_matrix)):</span><br><span class="line">                pred += self.dual_coef_[j] * self.apply_kernel(X[i], self.decision_matrix[j])</span><br><span class="line"></span><br><span class="line">            pred_results = np.append(pred_results, pred)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> pred_results</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        pred_results = self.decision_function(X)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> np.where(pred_results &gt;= <span class="number">0</span>, <span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="ref">Ref</h1>
<p>https://www.ccs.neu.edu/home/vip/teach/MLcourse/6_SVM_kernels/lecture_notes/svm/svm.pdf</p>
<p>http://www.cs.cmu.edu/~guestrin/Class/10701-S06/Slides/svms-s06.pdf</p>
<p>MML book</p>
<p>Lagrangian Duality for Dummies, David Knowles</p>
<p>PRML Chapter 7</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/19/naive-bayes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/naive-bayes/" class="post-title-link" itemprop="url">Naive Bayes</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-19 16:32:15" itemprop="dateCreated datePublished" datetime="2021-07-19T16:32:15+08:00">2021-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-24 23:06:03" itemprop="dateModified" datetime="2021-07-24T23:06:03+08:00">2021-07-24</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/07/19/naive-bayes/" class="post-meta-item leancloud_visitors" data-flag-title="Naive Bayes" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>7.2k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>7 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="naive-bayes">Naive Bayes</h1>
<p>Suppose our training set consists of data samples <span class="math inline">\(D = \{(\mathbf{x}_1, y_1), ...., (\mathbf{x}_N, y_N)\}, \; \mathbf{x_i} \in \mathbb{R}^d\)</span>, where <span class="math inline">\(D = \{(\mathbf{x}_i, y_i)\}\)</span> are realizations of a random sample that follows unknown joint distribution <span class="math inline">\(P(\mathbf{X}, Y)\)</span>.</p>
<p><strong>Assumptions</strong>:</p>
<ol type="1">
<li><p><strong>Features are conditionally independent (Naive bayes assumption)</strong>: <span class="math display">\[P(\mathbf{X} | Y) = \prod^{d}_{j=1} P(X_j | Y)\]</span></p></li>
<li><p><strong>MLE assumption</strong>: Random sample is identically distributed.</p></li>
<li><p><strong>Positional independence</strong>: The position of features does not matter (used in Multinomial case).</p></li>
</ol>
<p>By applying bayes rule (applying on distribution <span class="math inline">\(P (\cdot)\)</span> to make things general), we have:</p>
<p><span class="math display">\[P(Y | \mathbf{X}) = \frac{P(\mathbf{X}, Y)}{P(\mathbf{X})} = \frac{P(\mathbf{X} | Y) P(Y)}{P(\mathbf{X})}\]</span></p>
<p>By substituting the assumption:</p>
<p><span class="math display">\[P(Y | \mathbf{X}) = \frac{\prod^{d}_{j=1} P(X_j | Y)P(Y)}{P(\mathbf{X})}\]</span></p>
<p>Since the probability distribution <span class="math inline">\(P(\mathbf{X})\)</span> characterised by <span class="math inline">\(F_{\mathbf{X}}(\mathbf{x})\)</span> is constant for any given <span class="math inline">\(\mathbf{x}\)</span>, we can drop it from the equation because it only changes <span class="math inline">\(P(Y | \mathbf{X})\)</span> by a proportion:</p>
<p><span class="math display">\[P(Y | \mathbf{X}) \propto P(Y) \prod^{d}_{j=1} P(X_j | Y)\]</span></p>
<p>Our goal is to find a class <span class="math inline">\(\hat{y}\)</span> that maximize the probability given input <span class="math inline">\(\mathbf{X} = \mathbf{x}\)</span>:</p>
<span class="math display">\[\begin{aligned}
\hat{y} = \underset{y}{\arg\max} \sum^{d}_{j=1} \log P_{X_j|Y}(x_j | y) + \log P_{Y}(y)
\end{aligned}\]</span>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/07/19/naive-bayes/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/19/logistic-regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/logistic-regression/" class="post-title-link" itemprop="url">Logistic Regression</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-19 16:32:07" itemprop="dateCreated datePublished" datetime="2021-07-19T16:32:07+08:00">2021-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-08-09 17:48:42" itemprop="dateModified" datetime="2021-08-09T17:48:42+08:00">2021-08-09</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/07/19/logistic-regression/" class="post-meta-item leancloud_visitors" data-flag-title="Logistic Regression" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>5.4k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>5 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="logistic-regression">Logistic Regression</h1>
<p>Suppose we have training examples <span class="math inline">\(D = \{(\mathbf{x}_1, y_1), ...., (\mathbf{x}_N, y_N); \; \mathbf{x}_i \in \mathbb{R}^d\}\)</span>, our goal is to make decision about the class of new input <span class="math inline">\(\mathbf{x}\)</span>. The logistic regression does this by learning from a training set, a vector of bias and a matrix of weights.</p>
<h2 id="binary-class">Binary-Class</h2>
<p>In binary class problem, our target <span class="math inline">\(Y\)</span> takes values <span class="math inline">\(\{0, 1\}\)</span>. To model the distribution <span class="math inline">\(P(Y | \mathbf{X}; \; \mathbf{w}, b)\)</span>, we apply sigmoid function on the dot product of weights and inputs which transform the output to a value between <span class="math inline">\([0, 1]\)</span> (one criteria for probability):</p>
<p><span class="math display">\[z = \mathbf{x}^T \mathbf{w} + b\]</span></p>
<p><span class="math display">\[y = \sigma(z)\]</span></p>
<p>To make sure that class random variable <span class="math inline">\(Y\)</span>'s conditional pmf sums to 1:</p>
<p><span class="math display">\[P(Y=1 | X=\mathbf{x} ;\; \mathbf{w}, b) = \frac{1}{1 + e^{-z}} = p\]</span></p>
<p><span class="math display">\[P(Y=0 | X=\mathbf{x} ;\; \mathbf{w}, b) = 1 - \frac{1}{1 + e^{-z}} = \frac{e^{-z}}{1 + e^{-z}} = 1 - p\]</span></p>
<p>Then, it is equivalently to express this conditional pmf as Bernoulli pmf:</p>
<p><span class="math display">\[p_{Y|\mathbf{X}} (y | \mathbf{x}; \; \mathbf{w}, b) = p^y + (1 - p)^{1 - y}\]</span></p>
<p>If we have the conditional pmf of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X= \mathbf{x}\)</span>, then we can use simple decision rule to make decisions:</p>
<p><span class="math display">\[
\hat{y} =
\begin{cases}
P(Y=1 | X=\mathbf{x}) &gt; 0.5, \quad 1\\
P(Y=1 | X=\mathbf{x}) \leq 0.5, \quad 0
\end{cases}
\]</span></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/07/19/logistic-regression/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/19/gbdt/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/gbdt/" class="post-title-link" itemprop="url">GBDT</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-19 16:31:40" itemprop="dateCreated datePublished" datetime="2021-07-19T16:31:40+08:00">2021-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-28 11:22:10" itemprop="dateModified" datetime="2021-07-28T11:22:10+08:00">2021-07-28</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/07/19/gbdt/" class="post-meta-item leancloud_visitors" data-flag-title="GBDT" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>4.7k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>4 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="gradient-boosting-decision-trees">Gradient Boosting Decision Trees</h1>
<h2 id="boosting-trees">Boosting Trees</h2>
<p>Regression and classification trees partition the space of all joint predictor variable values into disjoint regions <span class="math inline">\(R_j, \; j=1, 2, ...., J\)</span> as represented by the terminal nodes of the tree. A constant <span class="math inline">\(c_j\)</span> is assigned to each such region and the prediction rule is:</p>
<p><span class="math display">\[\mathbf{x} \in R_j \implies f(\mathbf{x}) = c_j\]</span></p>
<p>Thus, a tree can be formally expressed as:</p>
<p><span class="math display">\[T(\mathbf{x}; \boldsymbol{\theta}) = \sum^{J}_{j=1} c_j I[\mathbf{x} \in R_j]\]</span></p>
<p>with parameters <span class="math inline">\(\theta = \{R_j, c_j\}^J_1\)</span>. The parameters are found by iteratively solving minimizing problem:</p>
<ol type="1">
<li>Given <span class="math inline">\(R_j\)</span>, we solve <span class="math inline">\(\hat{c}_j\)</span> by simply taking the average or majority class.</li>
<li><span class="math inline">\(R_j\)</span> is found by iterating over all possible pairs of feature and splitting point.</li>
</ol>
<p>The boosted tree model is a sum of such trees induced in a forward stagewise manner:</p>
<p><span class="math display">\[f_M (\mathbf{x}) = \sum^{M}_{m=1} T(\mathbf{x}; \; \boldsymbol{\theta}_m)\]</span></p>
<p>At each step, one must solve:</p>
<p><span class="math display">\[\hat{\boldsymbol{\theta}}_m = \underset{\boldsymbol{\theta}_m}{\arg\min} \sum^{N}_{i=1} L(y_i, f_{m-1} (\mathbf{x}_i) + T(\mathbf{x}_i; \; \boldsymbol{\theta}_m))\]</span></p>
<h2 id="gradient-boosting">Gradient Boosting</h2>
<p>In general, it is hard to directly take partial derivatives w.r.t the tree's parameters, thus, we take partial derivatives of tree predictions <span class="math inline">\(f(\mathbf{x}_i)\)</span>.</p>
<p>Fast approximate algorithms for solving the above problem with any differentiable loss criterion can be derived by analogy to numerical optimization. We first start with general case. The loss in using <span class="math inline">\(\mathbf{f}\)</span> to predict <span class="math inline">\(y\)</span> on the training data is:</p>
<p><span class="math display">\[L(\mathbf{f}) = \sum^{N}_{i=1} L(y_i, f(\mathbf{x}_i))\]</span></p>
<p>The goal is to minimize <span class="math inline">\(L(\mathbf{x}_i)\)</span> w.r.t <span class="math inline">\(f\)</span>, where here <span class="math inline">\(f(\mathbf{x})\)</span> is constrained to be a sum of trees <span class="math inline">\(f_M (\mathbf{x})\)</span>.</p>
<p>We first start with general case where <span class="math inline">\(f\)</span> can be any parameters or numbers. In this case, we have <span class="math inline">\(N\)</span> samples, thus, <span class="math inline">\(\mathbf{f} = \{f(\mathbf{x}_1), ...., f(\mathbf{x}_N)\} \in \mathbb{R}^N\)</span>.</p>
<p>Then, the gradient of objective w.r.t <span class="math inline">\(\mathbf{f} = \mathbf{f}_{m-1}\)</span> which is the current model is:</p>
<p><span class="math display">\[\mathbf{g}_m = \nabla_{\mathbf{f}} L(\mathbf{f}) = \; &lt;\frac{\partial L(\mathbf{f})}{\partial f(\mathbf{x}_1)}, ...., \frac{\partial L(\mathbf{f})}{\partial f(\mathbf{x}_N)}&gt;\]</span></p>
<p>Then this gradient points at the direction of <strong>steepest increase</strong>, it tells us how we can change our current predictions to increase our loss. Since we want to minimize the objective, we would like to adjust our current predictions to the direction of <strong>steepest decrease</strong>:</p>
<p><span class="math display">\[\mathbf{h}_m = - \rho_m \mathbf{g}_m\]</span></p>
<p>Where <span class="math inline">\(\rho_m\)</span> is the step size for current model and it is minimizer of:</p>
<p><span class="math display">\[\rho_m = \underset{\rho}{\arg\min} \; L(\mathbf{f}_{m-1} - \rho\mathbf{g}_m)\]</span></p>
<p>The current solution is then updated as</p>
<p><span class="math display">\[\mathbf{f}_{m} = f_{m-1} - \rho_m\mathbf{g}_m\]</span></p>
<p><strong>If fitting the training data (minimizing the loss) is our ultimate goal</strong>, then the above update rule can solve our problem by adding the negative gradient at each iteration. However, our ultimate goal is to generalize to new data, copying and pasting training data exactly is not what we want. One possible solution is to learn the update <span class="math inline">\(- \rho_m\mathbf{g}_m\)</span> by fitting a simple decision tree:</p>
<p><span class="math display">\[\hat{\boldsymbol{\theta}}_m = \underset{\boldsymbol{\theta}}{\arg\min} \sum^{N}_{i=1} (-g_{im} - T(x_i; \; \boldsymbol{\theta}))^2\]</span></p>
<p>That is, we fit a regression tree <span class="math inline">\(T\)</span> to the negative gradient values.</p>
<p><img src='/images/ML/gbdt_1.png' width="600"></p>
<h3 id="algorithm">Algorithm</h3>
<p><img src='/images/ML/gbdt_2.png' width="600"></p>
<ol type="1">
<li>We first start by a constant model (model that predict constants) which is a single terminal node tree.</li>
<li>For all samples new targets are generated to be the negative gradient of the loss function w.r.t the current model prediction <span class="math inline">\(\mathbf{f}_{m-1}\)</span>.</li>
<li>Fit a regression tree to minimize the MSE between new target (negative gradient) and current prediction.</li>
</ol>
<h2 id="discussions">Discussions</h2>
<h3 id="regularization">Regularization</h3>
<p>For numbers of gradient boosting rounds <span class="math inline">\(M\)</span>, the loss can be made arbitrarily small. However, fitting the data too well can lead to overfitting which degrades the risk on future predictions.</p>
<h4 id="shrinkage">Shrinkage</h4>
<p>Controlling the value of <span class="math inline">\(M\)</span> is not the only possible regularization strategy, we can add penalty terms to the loss function that penalize large <span class="math inline">\(M\)</span> or we can weight subsequent trees. The simplest implementation of shrinkage in the context of boosting is to scale the contribution of each tree by a factor of <span class="math inline">\(0 &lt; v &lt; 1\)</span>, when it is added to the current approximation:</p>
<p><span class="math display">\[f_m (\mathbf{x}) = f_{m-1}(\mathbf{x}) + v \sum^{J}_{j=1} c_{jm} I[\mathbf{x} \in R_{jm}]\]</span></p>
<p>The parameter <span class="math inline">\(v\)</span> can be regarded as controlling the <strong>learning rate</strong> of the boosting procedure. Smaller values of <span class="math inline">\(v\)</span> result in larger training error for the same number of iterations <span class="math inline">\(M\)</span> but might have better generalization. Thus, both <span class="math inline">\(v\)</span> and <span class="math inline">\(M\)</span> control prediction risk on the training data. <span class="math inline">\(v\)</span> and <span class="math inline">\(M\)</span> tradeoff each other, therefore in practice, it is best to set <span class="math inline">\(v\)</span> small and control <span class="math inline">\(M\)</span> by early stopping.</p>
<h4 id="subsampling">Subsampling</h4>
<p>We know that bootstrap averaging (bagging) improves the performance of a noisy classifier through averaging (reduce variance). We can exploit the same idea in gradient boosting.</p>
<p>At each iteration, we sample a fraction <span class="math inline">\(\eta\)</span> of the training observations without replacement, and grow the next tree using that subsample. A typical value of <span class="math inline">\(\eta\)</span> is 0.5, although for large sample size <span class="math inline">\(N\)</span>, we can have smaller <span class="math inline">\(\eta\)</span>.</p>
<h1 id="ref">Ref</h1>
<p>https://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/19/decision-trees/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/decision-trees/" class="post-title-link" itemprop="url">Decision Trees</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-19 16:31:33" itemprop="dateCreated datePublished" datetime="2021-07-19T16:31:33+08:00">2021-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-26 16:19:50" itemprop="dateModified" datetime="2021-07-26T16:19:50+08:00">2021-07-26</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/07/19/decision-trees/" class="post-meta-item leancloud_visitors" data-flag-title="Decision Trees" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>9.3k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>8 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="decision-trees-cart">Decision Trees (CART)</h1>
<p>Tree based methods partition the feature space into a set of rectangles, and then fit a simple model (i.e constant) in each one. We focus on CART in this post.</p>
<p>Suppose we have dataset <span class="math inline">\(D = \{(\mathbf{x}_1, y_1) , ...., (\mathbf{x}_N, y_N) ;\; \mathbf{x}_i \in \mathbb{R}^d\}\)</span>. The algorithm needs to automatically decide on the <strong>splitting variables</strong> and <strong>splitting points</strong> and also what shape the tree should have.</p>
<h2 id="regression-trees">Regression Trees</h2>
<p>In this scenario, our response variable <span class="math inline">\(Y\)</span> is continuous. Suppose first that we have a partition into <span class="math inline">\(M\)</span> regions <span class="math inline">\(R_1, ...., R_M\)</span> and we define the <strong>model prediction</strong> as:</p>
<p><span class="math display">\[\hat{y} = \sum^{M}_{m=1} c_m I[\mathbf{x}_i \in R_m]\]</span></p>
<p>By minimizing the mean square loss <span class="math inline">\(\frac{1}{2} \frac{1}{N} \sum^{N}_{i=1} (y_i - \hat{y}_i)^2\)</span>, we have:</p>
<span class="math display">\[\begin{aligned}
\frac{\partial L}{\partial c_m} &amp;= \frac{1}{N}\sum^{N}_{i=1} (y_i -  \sum^{M}_{m=1} c_m I[\mathbf{x}_i \in R_m]) I[\mathbf{x}_i \in R_m]\\
&amp;= \frac{1}{N_m}\sum^{N}_{i=1} (y_i I[\mathbf{x}_i \in R_m]) - c_m\\
\implies \hat{c}_m &amp;= \frac{1}{N_m}\sum^{N}_{i=1} (y_i I[\mathbf{x}_i \in R_m])
\end{aligned}\]</span>
<p>Thus, the best estimate <span class="math inline">\(\hat{c}_m\)</span> in each region is the <strong>average training responses</strong> in that region w.r.t mean square error:</p>
<p><span class="math display">\[\hat{c}_m = \frac{1}{N_m} \sum^{N}_{i=1} y_i I[\mathbf{x}_i \in R_m]\]</span></p>
<p>Where <span class="math inline">\(N_m = \sum^{N}_{i=1} I[\mathbf{x}_i \in R_m]\)</span>, is total training examples in region <span class="math inline">\(R_m\)</span>.</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/07/19/decision-trees/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/19/heaps/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/heaps/" class="post-title-link" itemprop="url">Heaps</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-19 13:31:28" itemprop="dateCreated datePublished" datetime="2021-07-19T13:31:28+08:00">2021-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-24 23:09:20" itemprop="dateModified" datetime="2021-07-24T23:09:20+08:00">2021-07-24</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS-Basics/" itemprop="url" rel="index"><span itemprop="name">CS Basics</span></a>
        </span>
    </span>

  
    <span id="/2021/07/19/heaps/" class="post-meta-item leancloud_visitors" data-flag-title="Heaps" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>5.2k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>5 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="heaps">Heaps</h1>
<p>The <code>binary heap</code> data structure is an array object that we can view as a nearly complete binary tree. Each node of the tree corresponds to an element of the array. The tree is completely filled on all levels except possibly the lowes, which is filled from the left up to a point that is <strong>the elements in the subarray <span class="math inline">\(A[\lfloor \frac{n}{2} \rfloor + 1 ... n]\)</span></strong> are all leaves.</p>
<p>An array <span class="math inline">\(A\)</span> that represents a heap is an object with two attributes:</p>
<ol type="1">
<li><span class="math inline">\(A.length\)</span>: gives the number of elements in the array. <span class="math inline">\(A[1:A.length]\)</span>.</li>
<li><span class="math inline">\(A.heap\_size\)</span>: gives how many elements in the heap are stored within array <span class="math inline">\(A\)</span>. <span class="math inline">\(A[1:A.heap\_size:A.length]\)</span></li>
</ol>
<p>Given the index <span class="math inline">\(i\)</span> of a node, we can easily compute the indices of its parent, left child and right child by:</p>
<ol type="1">
<li><code>parent</code>: <span class="math inline">\(\lfloor \frac{i}{2} \rfloor\)</span> (by shifting right 1 bit <code>i &gt;&gt; 1</code>)</li>
<li><code>left child</code>: <span class="math inline">\(2i\)</span> (by shifting left 1 bit <code>i &lt;&lt; 1</code>)</li>
<li><code>right child</code>: <span class="math inline">\(2i + 1\)</span> (by shifting left 1 bit and add 1 <code>i &lt;&lt; 1 + 1</code>)</li>
</ol>
<p>There are two types of binary heap:</p>
<ol type="1">
<li><code>Max heap</code>: satisfies the <code>max heap property</code> that for every node <span class="math inline">\(i\)</span> other than the root <span class="math inline">\(A[parent(i)] \geq A[i]\)</span>, that is the maximum value in the array is stored in the root and the subtree rooted at a node contains values no larger than that contained at the node itself (heap sorts).</li>
<li><code>Min heap</code>: satisfies the <code>min heap property</code> that is organized in the opposite way, for every node <span class="math inline">\(i\)</span> other than the root <span class="math inline">\(A[parent(i) \leq A[i]]\)</span> (priority queues)</li>
</ol>
<p><br></p>
<p><img src="/images/algo/heap_1.png" width="600px"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Heap</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, A, heap_size</span>):</span></span><br><span class="line">        <span class="comment"># A may not be a heap, call build_min_heap or build_max_heap to convert this instance to heap</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(A, <span class="built_in">list</span>)</span><br><span class="line">        self.heap_size = heap_size</span><br><span class="line">        self.length = <span class="built_in">len</span>(A)</span><br><span class="line">        self._A = A</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">left</span>(<span class="params">self, i</span>):</span></span><br><span class="line">        <span class="keyword">return</span> i &lt;&lt; <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">right</span>(<span class="params">self, i</span>):</span></span><br><span class="line">        <span class="keyword">return</span> i &lt;&lt; <span class="number">1</span> + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parent</span>(<span class="params">self, i</span>):</span></span><br><span class="line">        <span class="keyword">return</span> i &gt;&gt; <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._A[index]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">append</span>(<span class="params">self, val</span>):</span></span><br><span class="line">        self._A.append(val)</span><br><span class="line">        </span><br></pre></td></tr></table></figure>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/07/19/heaps/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/18/cnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/18/cnn/" class="post-title-link" itemprop="url">Backpropagation in CNN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-18 17:39:39" itemprop="dateCreated datePublished" datetime="2021-07-18T17:39:39+08:00">2021-07-18</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-09-07 13:50:28" itemprop="dateModified" datetime="2021-09-07T13:50:28+08:00">2021-09-07</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/" itemprop="url" rel="index"><span itemprop="name">DL</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/CNN/" itemprop="url" rel="index"><span itemprop="name">CNN</span></a>
        </span>
    </span>

  
    <span id="/2021/07/18/cnn/" class="post-meta-item leancloud_visitors" data-flag-title="Backpropagation in CNN" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>5.5k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>5 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="backpropagation-in-convolutional-neural-networks">Backpropagation In Convolutional Neural Networks</h1>
<h2 id="cross-correlation">Cross Correlation</h2>
<p>Given an input image <span class="math inline">\(I\)</span> and a filter <span class="math inline">\(K\)</span> of dimensions <span class="math inline">\(k_1 \times k_2\)</span>, then the cross correlation operation is defined as:</p>
<p><span class="math display">\[(I \otimes K)_{ij} = \sum^{k_2 - 1}_{m=0}\sum^{k_1 - 1}_{n=0} I(i + m, j + n)K(m, n)\]</span></p>
<h2 id="convolution">Convolution</h2>
<p><span class="math display">\[(I * K)^{d}_{ij} = \sum^{k_2 - 1}_{m=0}\sum^{k_1 - 1}_{n=0} I(i - m, j - n)K(m, n)\]</span></p>
<p>which is equivalent to <code>cross correlation</code> with flipped kernel (i.e flipped 180 degree)</p>
<p><span class="math display">\[(I * K)_{ij} = \sum^{k_2 - 1}_{m=0}\sum^{k_1 - 1}_{n=0} I(i + m, j + n) \text{ rot}_{180}(K(m, n))\]</span></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/07/18/cnn/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/15/probability-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/15/probability-3/" class="post-title-link" itemprop="url">probability-3</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-15 23:54:46" itemprop="dateCreated datePublished" datetime="2021-07-15T23:54:46+08:00">2021-07-15</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-24 23:09:41" itemprop="dateModified" datetime="2021-07-24T23:09:41+08:00">2021-07-24</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Background/" itemprop="url" rel="index"><span itemprop="name">Background</span></a>
        </span>
    </span>

  
    <span id="/2021/07/15/probability-3/" class="post-meta-item leancloud_visitors" data-flag-title="probability-3" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>10 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="statistics-inference">Statistics Inference</h1>
<h2 id="sampling-and-statistics">Sampling and Statistics</h2>
<p>In a typical statistical problem, we have <span class="math inline">\(p_X(x), f_X(x)\)</span> unknown. Our ignorance about the <span class="math inline">\(p_X(x), f_X(x)\)</span> can roughly be classified in one of two ways:</p>
<ol type="1">
<li><span class="math inline">\(f(x)\)</span> or <span class="math inline">\(p_X(x)\)</span> is completely unknown.</li>
<li>The form of <span class="math inline">\(f_X(x)\)</span> or <span class="math inline">\(p_X(x)\)</span> is known down to an unknown <code>parameter</code> vector <span class="math inline">\(\mathbf{\theta}\)</span>.</li>
</ol>
<p>We will focus on the second case. We often denote this problem by saying that the random variable <span class="math inline">\(X\)</span> has density or mass function of the form <span class="math inline">\(f_{X}(x; \theta)\)</span> or <span class="math inline">\(p_X(x;\theta)\)</span>, where <span class="math inline">\(\theta \in \Omega\)</span> for a special parameter space <span class="math inline">\(\Omega\)</span>. Since <span class="math inline">\(\theta\)</span> is unknown, we want to estimate it.</p>
<p>In the process, our information about case 1 or 2 comes from a sample on <span class="math inline">\(X\)</span>. The sample observations have the same distribution as <span class="math inline">\(X\)</span>, and we denote them as <span class="math inline">\(X_1, ...., X_n\)</span> where <span class="math inline">\(n\)</span> is the <code>sample size</code>. When the sample is actually drawn, we use lower case letters to represent the <code>realization</code> <span class="math inline">\(x_1, ..., x_n\)</span> of random samples.</p>
<p><img src="/images/RL/background/mprob_4_1_2.png" width="600"> <img src="/images/RL/background/mprob_4_1_1.png" width="600"></p>
<p>Once the sample is drawn, then <span class="math inline">\(t\)</span> is called the realization of <span class="math inline">\(T\)</span>, where <span class="math inline">\(t = T(x_1, ...., x_n)\)</span>.</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/07/15/probability-3/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>
<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhu, Zhaoyang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Symbols count total">697k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">10:34</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.3" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{&quot;enable&quot;:true,&quot;app_id&quot;:&quot;gPOc95wWW2Hwp2pkVSAAz28m-MdYXbMMI&quot;,&quot;app_key&quot;:&quot;7Sf65xCHXfEEdBvu29UHSYdV&quot;,&quot;server_url&quot;:&quot;https:&#x2F;&#x2F;swag1ong.github.io&quot;,&quot;security&quot;:true}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{&quot;enable&quot;:true,&quot;repo&quot;:&quot;swag1ong&#x2F;swag1ong.github.io&quot;,&quot;issue_term&quot;:&quot;pathname&quot;,&quot;theme&quot;:&quot;github-light&quot;,&quot;pathname&quot;:&quot;pathname&quot;}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
