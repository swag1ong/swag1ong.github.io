<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="1xbHmVEsS5Fta14x1d1FXkHFr93LRr1pMxipdYVcNI4">
  <meta name="baidu-site-verification" content="code-Srzz2vTuzA">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;swag1ong.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;Searching...&quot;,&quot;empty&quot;:&quot;We didn&#39;t find any results for the search: ${query}&quot;,&quot;hits_time&quot;:&quot;${hits} results found in ${time} ms&quot;,&quot;hits&quot;:&quot;${hits} results found&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:3,&quot;unescape&quot;:false,&quot;preload&quot;:true}}</script>
<meta property="og:type" content="website">
<meta property="og:title" content="GoGoGogo!">
<meta property="og:url" content="https://swag1ong.github.io/page/4/index.html">
<meta property="og:site_name" content="GoGoGogo!">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhu, Zhaoyang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://swag1ong.github.io/page/4/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:true,&quot;isPost&quot;:false,&quot;lang&quot;:&quot;en&quot;,&quot;comments&quot;:&quot;&quot;,&quot;permalink&quot;:&quot;&quot;,&quot;path&quot;:&quot;page&#x2F;4&#x2F;index.html&quot;,&quot;title&quot;:&quot;&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>GoGoGogo!</title><script src="/js/config.js"></script>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?852b005027c5efa81663f6f5c4c5b7fd"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">GoGoGogo!</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-igloo fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">102</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">16</span></a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">25</span></a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-address-card fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhu, Zhaoyang"
      src="/images/others/favicon.jpeg">
  <p class="site-author-name" itemprop="name">Zhu, Zhaoyang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">102</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/swag1ong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;swag1ong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyang.zhu@mail.utoronto.ca" title="E-Mail → mailto:zhaoyang.zhu@mail.utoronto.ca" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zhuzhaoyangzzyu0616" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zhuzhaoyangzzyu0616" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/22/sac/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/22/sac/" class="post-title-link" itemprop="url">SAC</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-22 11:29:33" itemprop="dateCreated datePublished" datetime="2021-07-22T11:29:33+08:00">2021-07-22</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-09-16 13:41:41" itemprop="dateModified" datetime="2021-09-16T13:41:41+08:00">2021-09-16</time>
      </span>

  
    <span id="/2021/07/22/sac/" class="post-meta-item leancloud_visitors" data-flag-title="SAC" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>227</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>1 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</h1>
<h2 id="introductions-and-notations">Introductions and Notations</h2>
<p>Maximum entropy reinforcement learning optimizes policies to maximize both the expected return and the expected entropy of the policy.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/21/em/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/21/em/" class="post-title-link" itemprop="url">EM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-21 00:04:45" itemprop="dateCreated datePublished" datetime="2021-07-21T00:04:45+08:00">2021-07-21</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-09-16 14:37:17" itemprop="dateModified" datetime="2021-09-16T14:37:17+08:00">2021-09-16</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/07/21/em/" class="post-meta-item leancloud_visitors" data-flag-title="EM" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>10k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>9 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="expectation-maximization-algorithm">Expectation-Maximization Algorithm</h1>
<p>Gaussian Mixture distribution can be written as a linear superposition of Gaussians in the form:</p>
<p><span class="math display">\[P(\mathbf{X}) = \sum^K_{k=1} \pi_k N(\mathbf{X} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\]</span></p>
<p>Where <span class="math inline">\(\pi_k\)</span> is the mixing coefficient for each normal component that satisfies the conditions:</p>
<p><span class="math display">\[0 \leq \pi_k \leq 1\]</span> <span class="math display">\[\sum^{K}_{k=1} \pi_k = 1\]</span></p>
<p><br></p>
<p>Let us introduce a <span class="math inline">\(K\)</span>-dimensional binary latent random vector <span class="math inline">\(\mathbf{Z}\)</span> having a 1-of-<span class="math inline">\(K\)</span> representation in which a particular element <span class="math inline">\(Z_k \in \{0, 1\}\)</span> is equal to 1 and all other elements are equal to 0 and <span class="math inline">\(\sum^{K}_{k=1} Z_k = 1\)</span>. We define:</p>
<ul>
<li><p>The joint distribution <span class="math inline">\(P(\mathbf{X}, \mathbf{Z}) = P(\mathbf{Z}) P(\mathbf{X} | \mathbf{Z})\)</span> in terms of a marginal distribution <span class="math inline">\(P(\mathbf{Z})\)</span> and a conditional distribution <span class="math inline">\(P(\mathbf{X} | \mathbf{Z})\)</span>.</p></li>
<li><p>The marginal distribution over <span class="math inline">\(\mathbf{Z}\)</span> is specified in terms of the mixing coefficient <span class="math inline">\(\pi_k\)</span>, such that: <span class="math display">\[P(Z_k = 1) = \pi_k\]</span> <span class="math display">\[P(\mathbf{Z}) = \prod^{K}_{k=1} \pi_k^{Z_k}\]</span></p></li>
<li><p>The conditional distribution of <span class="math inline">\(\mathbf{X}\)</span> given a particular value for <span class="math inline">\(\mathbf{Z}\)</span> is a Gaussian: <span class="math display">\[P(\mathbf{X} | Z_k = 1) = N(\mathbf{X} | \mu_k, \Sigma_k)\]</span> <span class="math display">\[P(\mathbf{X} | \mathbf{Z}) = \prod^{K}_{k=1} N(\mathbf{X} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)^{Z_k}\]</span></p></li>
<li><p>The conditional probability (posterior probability) of particular value of <span class="math inline">\(\mathbf{Z}\)</span> given a particular value for <span class="math inline">\(\mathbf{X}\)</span>, which can be found by Bayes rule: <span class="math display">\[\gamma(Z_k) = P(Z_k = 1 | \mathbf{X}) = \frac{P(\mathbf{X} | Z_k = 1) P(Z_k = 1)}{P(\mathbf{X})} = \frac{\pi_k N(\mathbf{X} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum^K_{j=1} \pi_k N(\mathbf{X} | \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}\]</span></p>
<p>This probability can be viewed as the <strong>responsibility</strong> that component <span class="math inline">\(k\)</span> takes for explaining the observation <span class="math inline">\(\mathbf{X}\)</span></p></li>
</ul>
<p><br></p>
<p>Then the marginal distribution of the gaussian mixture can be written using the distribution of latent random vector <span class="math inline">\(\mathbf{Z}\)</span> as:</p>
<p><span class="math display">\[P(\mathbf{X}) = \sum_{\mathbf{Z}} P(\mathbf{Z}) P(\mathbf{X} | \mathbf{Z}) = \sum^{K}_{k=1} \pi_k N(\mathbf{X} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\]</span></p>
<p>It follows that, since we are using a joint distribution, if we have a random sample <span class="math inline">\(\mathbf{X}_1, ..., \mathbf{X}_N\)</span>, for every random vector <span class="math inline">\(\mathbf{X}_n\)</span> there is a corresponding latent variable <span class="math inline">\(\mathbf{Z}_n\)</span>. Therefore, we have found an equivalent formulation of the Gaussian mixture involving an explicit latent variable. Now, we can work with the joint distribution <span class="math inline">\(P(\mathbf{X}, \mathbf{Z})\)</span> instead of the original marginal distribution <span class="math inline">\(P(\mathbf{X})\)</span>.</p>
<p>We can express the joint distribution as Bayesian network:</p>
<p><img src='/images/ML/em_1.png' width="600"></p>
<p>And we can use ancestral sampling to generate random samples distributed according to the Gaussian mixture model:</p>
<ol type="1">
<li>Sample from <span class="math inline">\(\hat{\mathbf{Z}} \sim P(\mathbf{Z})\)</span></li>
<li>Sample from <span class="math inline">\(P(\mathbf{X} | \hat{\mathbf{Z}})\)</span></li>
<li>Coloring them by the <span class="math inline">\(\mathbf{\hat{Z}}\)</span></li>
</ol>
<p><img src='/images/ML/em_2.png' width="600"></p>
<h2 id="em-for-gaussian-mixtures">EM for Gaussian Mixtures</h2>
<p>Suppose we have a dataset of observations <span class="math inline">\(\{\mathbf{x}_1, ...., \mathbf{x}_N; \; \mathbf{x} \in \mathbb{R}^M\}\)</span>, and we wish to model this data using a mixture of Gaussians. We can represent this dataset as an <span class="math inline">\(N \times M\)</span> matrix <span class="math inline">\(\mathbf{D}\)</span> in which the <span class="math inline">\(n\)</span>th row is given by <span class="math inline">\(\mathbf{x}^T_n\)</span>. Similarly, the corresponding latent variables will be denoted by an <span class="math inline">\(N \times K\)</span> matrix <span class="math inline">\(\mathbf{H}\)</span> with rows <span class="math inline">\(\mathbf{Z}^T_n\)</span>. If we assume that the data points are drawn independently from the distribution then we can express the Gaussian mixture model for this i.i.d dataset using the graphical representation:</p>
<p><img src='/images/ML/em_3.png' width="600"></p>
<p>The log-likelihood function is given by:</p>
<p><span class="math display">\[\ln(P(\mathbf{D} | \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma})) = \sum^{N}_{n=1} \ln (\sum^K_{k=1} \pi_k N(\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k))\]</span></p>
<p>An elegant and powerful method for finding maximum likelihood solutions for models with latent variables is called the <code>expectation-maximization algorithm</code>.</p>
<p>We know that at a maximum of the likelihood function (by taking the derivative with respect to <span class="math inline">\(\mathbf{\mu}_k\)</span>):</p>
<p><span class="math display">\[0 = - \sum^N_{n=1} \underbrace{\frac{\pi_k N(\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum^K_{j=1} \pi_k N(\mathbf{x}_n | \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}}_{\gamma(Z_{nk})} \boldsymbol{\Sigma}_k (\mathbf{x}_n - \boldsymbol{\mu}_k)\]</span></p>
<p>By assuming that <span class="math inline">\(\boldsymbol{\Sigma}_k\)</span> is invertible and rearranging we have:</p>
<p><span class="math display">\[\boldsymbol{\hat{\mu}}_k = \frac{1}{N_k} \sum^N_{n=1} \gamma(Z_{nk}) \mathbf{x}_n\]</span></p>
<p><span class="math display">\[N_k = \sum^N_{n=1} \gamma(Z_{nk})\]</span></p>
<p>Since <span class="math inline">\(Z_{nk} = P(Z_{nk} = 1 | \mathbf{X} = \mathbf{x}_n)\)</span> is the posterior probability, we can interpret <span class="math inline">\(N_k\)</span> as the total probability of samples assigned to cluster <span class="math inline">\(k\)</span>. We can see that the mean <span class="math inline">\(\boldsymbol{\mu}_k\)</span> for the <span class="math inline">\(k\)</span>th Gaussian component is obtained by taking a mean of all of the points in the dataset weighted by the posterior distribution for cluster <span class="math inline">\(k\)</span>.</p>
<p><br></p>
<p>By taking the derivative w.r.t <span class="math inline">\(\boldsymbol{\Sigma}_k\)</span>, we have:</p>
<p><span class="math display">\[\boldsymbol{\hat{\Sigma}}_k = \frac{1}{N_k} \sum^N_{n=1} \gamma(Z_{nk}) (\mathbf{x}_n - \boldsymbol{\mu}_n)(\mathbf{x}_n - \boldsymbol{\mu}_n)^T\]</span></p>
<p>By taking the derivative w.r.t the miximing coefficients <span class="math inline">\(\pi_k\)</span> and using lagrange multiplier, we have:</p>
<p><span class="math display">\[\hat{\pi}_k = \frac{N_k}{N}\]</span></p>
<p>So that the mixing coefficient for the <span class="math inline">\(k\)</span>th component is given by the average responsibility which that component takes for explaining the data points.</p>
<p>We can see that, <span class="math inline">\(\hat{\pi}_k, N_k, \boldsymbol{\hat{\mu}}_k, \boldsymbol{\hat{\Sigma}}_k\)</span> all depends on the value of <span class="math inline">\(\gamma(Z_{nk})\)</span> and <span class="math inline">\(\gamma(Z_{nk})\)</span> depends on the values of all other variables. Thus, we can do a simple iterative scheme for finding a solution to the maximum likelihood problem, which turns out to be an instance of the EM algorithm for the particular case of Gaussian Mixture Model:</p>
<ol type="1">
<li>We first choose some initial values for the means, covariances and mixing coefficients.</li>
<li>We alternate between E step and M step:
<ul>
<li><strong>Expectation Step</strong>: We use the current values for the parameters to evaluate the posterior probabilities (responsibilities).</li>
<li><strong>Maximization Step</strong>: We then use responsibilities to maximize the log likelihood function for parameters (Mean first, then covariance matrix).</li>
</ul></li>
</ol>
<p>In practice, the algorithm is deemed to have converged when the change in the log likelihood function or alternatively in the parameters falls below some threshold.</p>
<p><img src='/images/ML/em_4.png' width="600"> <img src='/images/ML/em_5.png' width="600"></p>
<h2 id="an-alternative-view-of-em">An Alternative View of EM</h2>
<p>The goal of EM Algorithm is to find maximum likelihood solutions for models having latent variables. We denote sthe set of all observed data by <span class="math inline">\(\mathbf{D}\)</span> in which the <span class="math inline">\(n\)</span>th row represents <span class="math inline">\(\mathbf{x}^T_n\)</span>, and similarily we denote the set of all latent variables by <span class="math inline">\(\mathbf{H}\)</span> with a corresponding row <span class="math inline">\(\mathbf{Z}^T_n\)</span>. The set of all parameters is denoted by <span class="math inline">\(\boldsymbol{\theta}\)</span> and so the log likelihood function is given by:</p>
<p><span class="math display">\[\ln L(\boldsymbol{\theta}; \; \mathbf{D}) = \ln(\sum_{\mathbf{H}} P(\mathbf{D}, \mathbf{H} | \boldsymbol{\theta}))\]</span></p>
<p>If we have continuous latent variable, we can replace the summation by integral.</p>
<p>A key observation is that the summation over the latent variables appears inside the logarithm which provides much complicated expression of log likelihood. Suppose, for each observation in <span class="math inline">\(\mathbf{D}\)</span>, we have observed corresponding random variable <span class="math inline">\(\mathbf{Z}^T_n = \mathbf{z}^T_n\)</span>. We call <span class="math inline">\(\{\mathbf{D}, \mathbf{H}\}\)</span> the <strong>complete dataset</strong> and if we only observed <span class="math inline">\(\mathbf{D}\)</span> we have <strong>incomplete</strong> dataset, the maximization of this complete-data log likelihood function is straightforward and much simpler than incomplete likelihood.</p>
<p>In practice, we are not given the complete dataset but only the incomplete data. Our state of knowledge of the values of the latent variables in <span class="math inline">\(\mathbf{H}\)</span> is given only by the posterior distribution:</p>
<p><span class="math display">\[P(\mathbf{H} | \mathbf{D}, \boldsymbol{\theta}) = \frac{P(\mathbf{H}, \mathbf{D} | \boldsymbol{\theta})}{P(\mathbf{D} | \boldsymbol{\theta})}\]</span></p>
<p>Since we cannot use the complete dataset log likelihood, we can consider using the expectation under the posterior distribution (<strong>E step</strong>). In E step, we use the current parameter values <span class="math inline">\(\boldsymbol{\theta}^{old}\)</span> to find the posterior distribution of the latent variables given by <span class="math inline">\(P(\mathbf{H} | \mathbf{D}, \boldsymbol{\theta})\)</span>. We then use this posterior distribution to find the expectation of the complete-data log likelihood evaluated for some general parameter value <span class="math inline">\(\boldsymbol{\theta}\)</span>. This expectation, denoted as:</p>
<p><span class="math display">\[Q(\boldsymbol{\theta}, \boldsymbol{\theta}^{old}) = E_{\mathbf{H} | \mathbf{D}, \boldsymbol{\theta}^{old}} [\ln P(\mathbf{D}, \mathbf{H} | \boldsymbol{\theta})] = \sum_{\mathbf{H}} P(\mathbf{H} | \mathbf{D}, \boldsymbol{\theta}^{old}) \ln P(\mathbf{D}, \mathbf{H} | \boldsymbol{\theta})\]</span></p>
<p>In the subsequent <strong>M step</strong>, we find parameters that maximize this expectation. If the current estimate for the parameters is denoted <span class="math inline">\(\boldsymbol{\theta}^{old}\)</span>, then a pair of successive E and M steps gives rise to a revised estimate <span class="math inline">\(\boldsymbol{\theta}^{new}\)</span>. The algorithm is initialized by choosing some starting value for the parameters <span class="math inline">\(\boldsymbol{\theta}_0\)</span>.</p>
<p><span class="math display">\[\theta^{new} = \underset{\boldsymbol{\theta}}{\arg\max} \; Q(\boldsymbol{\theta}, \boldsymbol{\theta}^{old})\]</span></p>
<p>Notice here we have complete data log likelihood, compare it with incomplete data log likelihood, the log likelihood function is much easier to compute.</p>
<h3 id="general-em-algorithm">General EM algorithm</h3>
<p><img src='/images/ML/em_6.png' width="600"> <img src='/images/ML/em_7.png' width="600"></p>
<h3 id="general-em-algorithm-for-gaussian-mixture">General EM Algorithm for Gaussian Mixture</h3>
<p>Previously, we used incomplete data log likelihood for Gaussian Mixture together with the EM algorithm, we have summation over <span class="math inline">\(k\)</span> over <span class="math inline">\(k\)</span> that occurs inside the logarithm. Now, we try to use general approach of EM algorithm.</p>
<p>We have the complete data likelihood:</p>
<span class="math display">\[\begin{aligned}
P(\mathbf{D}, \mathbf{H} | \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}) &amp;= P(\mathbf{D} | \mathbf{H},  \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}) P(\mathbf{H} | \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})\\
&amp;= \prod^{N}_{n=1}\prod^{K}_{k=1}\pi_k^{Z_{nk}} N(\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)^{Z_{nk}}\\
\end{aligned}\]</span>
<p>Where <span class="math inline">\(Z_{nk}\)</span> is the <span class="math inline">\(k\)</span> the component of <span class="math inline">\(\mathbf{Z}_n\)</span>. Taking the logarithm, we have the log likelihood:</p>
<p><span class="math display">\[\ln P(\mathbf{D}, \mathbf{H} | \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}) = \sum^{N}_{n=1}\sum^{K}_{k=1} Z_{nk} (\ln \pi_k + \ln  N(\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k))\]</span></p>
<p>Compare with the incomplete data log likelihood, we can see that this form is much easier to solve. In practice, we do not obtain the values of <span class="math inline">\(\mathbf{H}\)</span>, thus, we use expectation instead:</p>
<span class="math display">\[\begin{aligned}
E_{\mathbf{H} | \boldsymbol{\mu}} [\ln P(\mathbf{D}, \mathbf{H} | \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})] &amp;= \sum^{N}_{n=1}\sum^{K}_{k=1} P(Z_{nk}=1 | \mathbf{D}, \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}) (\ln \pi_k + \ln  N(\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k))\\
&amp;= \sum^{N}_{n=1}\sum^{K}_{k=1} \gamma(Z_{nk}) (\ln \pi_k + \ln  N(\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k))
\end{aligned}\]</span>
<p>We then proceed as follows:</p>
<ol type="1">
<li>First we start at some initial values for parameters.</li>
<li>We use these parameters to evaluate the responsibilities.</li>
<li>We then maximize the expected log likelihood function.</li>
</ol>
<p>Which is the same as the incomplete data log likelihood EM previously, but we have a much easier log likelihood function to maximize over.</p>
<h1 id="ref">Ref</h1>
<p>PRML Chapter 9</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/19/knn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/knn/" class="post-title-link" itemprop="url">knn</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-19 16:36:50" itemprop="dateCreated datePublished" datetime="2021-07-19T16:36:50+08:00">2021-07-19</time>
    </span>

  
    <span id="/2021/07/19/knn/" class="post-meta-item leancloud_visitors" data-flag-title="knn" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>0</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>1 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/19/adaboost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/adaboost/" class="post-title-link" itemprop="url">Adaboost</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-19 16:33:03" itemprop="dateCreated datePublished" datetime="2021-07-19T16:33:03+08:00">2021-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-27 15:20:43" itemprop="dateModified" datetime="2021-07-27T15:20:43+08:00">2021-07-27</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/07/19/adaboost/" class="post-meta-item leancloud_visitors" data-flag-title="Adaboost" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>7.9k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>7 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="adaboost-discrete">AdaBoost (Discrete)</h1>
<p>Consider a two-class problem, with the output variable coded as <span class="math inline">\(Y \in \{-1, 1\}\)</span>. Given a vector of input variables <span class="math inline">\(\mathbf{X}\)</span>, a classifier <span class="math inline">\(G(\mathbf{X})\)</span> produces a prediction taking one of the two values <span class="math inline">\(\{-1, 1\}\)</span>.</p>
<p>Suppose we have samples <span class="math inline">\(D = \{(\mathbf{x}_1, y_1), ...., (\mathbf{x}_N, y_N); \; \mathbf{x} \in \mathbb{R}^d\}\)</span>. The error rate (misclassification) on the training sample is defined as:</p>
<p><span class="math display">\[\bar{err} = \frac{1}{N} \sum^{N}_{i=1} I[y_i \neq G(\mathbf{x}_i)]\]</span></p>
<p>The expected error rate is defined as:</p>
<p><span class="math display">\[E_{X, Y} [I[Y \neq G(\mathbf{X})]]\]</span></p>
<p>A weak classifier is one whose error rate is only slightly better than random guessing. The purpose of boosting is to sequentially apply the weak classification algorithm to repeatedly modified versions of the data, thereby producing a sequence of weak classifiers <span class="math inline">\(G_{m} (\mathbf{x}), \; m = 1, ...., M\)</span></p>
<p>The predictions from all of them are then combined through a weighted majority vote to produce the final prediction:</p>
<p><span class="math display">\[G(x) = sign (\sum^{M}_{m=1} \alpha_m G_m (\mathbf{x}))\]</span></p>
<p>Here, <span class="math inline">\(\alpha_1, ...., \alpha_M\)</span> are parameters that weight the contribution of each corresponding classifier <span class="math inline">\(G(\mathbf{x})\)</span> and they are learned by the algorithm. They are here to give higher influence to the more accurate classifiers in the sequence.</p>
<p>The data modifications at each boosting step consist of applying weights <span class="math inline">\(w_1, ...., w_N\)</span> to each of the training observations <span class="math inline">\((\mathbf{x}_i, y_i), \; i=1, ....., N\)</span>:</p>
<ol type="1">
<li>Initially all of the weights are set to <span class="math inline">\(w_i = \frac{1}{N}\)</span>, so the first step simply trains the classifier on the data in usual manner.</li>
<li>For each successive iteration <span class="math inline">\(m = 2, 3, .... , M\)</span>, the observation weights are individually modified:
<ul>
<li>Observations that were misclassified by the classifier <span class="math inline">\(G_{m-1} (\mathbf{x})\)</span> induced at the previous step have their weights increased.</li>
<li>Observations that were classified correctly by the previous classifier have their weights decreased.</li>
</ul></li>
<li>Then the classification algorithm is reapplied to the weighted observations</li>
</ol>
<p>So misclassified samples have their weights increased so that each successive classifier is thereby forced to concentrate on those samples.</p>
<p><img src='/images/ML/adaboost_1.png' width="600"></p>
<p>At each iteration <span class="math inline">\(m\)</span>:</p>
<ol type="1">
<li>A normalized weighted error rate (high penalty for misclassifying high weight samples)<span class="math inline">\(err_m\)</span> is calculated.</li>
<li>The error rate is transformed by <span class="math inline">\(\log (\frac{1 - error_m}{error_m})\)</span>:
<ul>
<li>This function goes to positive infinity as <span class="math inline">\(error_m\)</span> goes to 0</li>
<li>This function goes to negative infinity as <span class="math inline">\(error_m\)</span> goes to 1 (<strong>we should just predict class inversely</strong>)</li>
<li>This function is 0 when error rate is 0.5</li>
</ul></li>
<li>The new weight update:
<ul>
<li>If <strong>correctly classified</strong>, then we simply keep the original weight for the sample (It will be smaller because we are weighting it by sum of error rate in the next classifier).</li>
<li>If <strong>incorrectly classified</strong>:
<ul>
<li>If error rate is small, then <span class="math inline">\(\alpha\)</span> is positively large so each misclassified sample's weight is <strong>increased</strong></li>
<li>If error rate is high, then <span class="math inline">\(\alpha\)</span> goes to negative infinity, so each misclassified sample's weight is <strong>decreased</strong> because this sample is correctly classified inversely so we decrease its weight.</li>
<li>If error rate is 0.5, then <span class="math inline">\(\alpha\)</span> is 0, so each misclassified sample's weight is <strong>maintained</strong></li>
</ul></li>
</ul></li>
</ol>
<p><img src='/images/ML/adaboost_3.png' width="600"></p>
<h2 id="boosting-fits-an-additive-model">Boosting Fits an Additive Model</h2>
<p>Boosting is a way of fitting an additive expansion in a set of elementary basis functions. Here the basis functions are the individual classifiers <span class="math inline">\(G_m(\mathbf{x}) \in \{-1, 1\}\)</span>. More generally, basis function expansions take the form:</p>
<p><span class="math display">\[f(\mathbf{x}) = \sum^{M}_{m=1} \beta_m b(\mathbf{x}; \boldsymbol{\gamma}_m)\]</span></p>
<p>Where <span class="math inline">\(\beta_m, \; m=1, 2, ...., M\)</span> are the expansion coefficients, and <span class="math inline">\(b(\mathbf{x}; \gamma) \in \mathbb{R}\)</span> are usually simple functions of multivariate argument <span class="math inline">\(\mathbf{x}\)</span> characterized by a set of parameters <span class="math inline">\(\{\boldsymbol{\gamma}_1, ...., \boldsymbol{\gamma}_M\}\)</span>.</p>
<p>Typically these models are fit and the parameters are found by minimizing a loss function averaged over the training data:</p>
<p><span class="math display">\[\min_{ \{\beta_m, \boldsymbol{\gamma}_m\}^{M}_1 }\sum^{N}_{i=1} L(y_i, \sum^{M}_{m=1} \beta_m b(\mathbf{x}_i; \boldsymbol{\gamma}_m))\]</span></p>
<h2 id="forward-stagewise-additive-modeling">Forward Stagewise Additive Modeling</h2>
<p>How to solve the above optimization problem? We can use forward stagewise modeling (greedy) to approximate the solution to above equation by sequentially adding new basis functions to the expansion without adjusting the parameters and coefficients of those that have already been added.</p>
<p><img src='/images/ML/adaboost_2.png' width="600"></p>
<p>The loss function of optimization problem at each step <span class="math inline">\(m\)</span> can be rewritten as:</p>
<p><span class="math display">\[L(y_i, f_m(\mathbf{x}_i)) = L(y_i, f_{m-1} (\mathbf{x}_i) + \beta b(\mathbf{x}_i; \boldsymbol{\gamma}))\]</span></p>
<p>Using the square loss, we have:</p>
<p><span class="math display">\[L(y_i, f_m(\mathbf{x}_i)) = (y_i - f_{m-1} (\mathbf{x}_i) - \beta b(\mathbf{x}_i; \boldsymbol{\gamma}))^2\]</span></p>
<p><span class="math display">\[\implies L(y_i, f_m(\mathbf{x}_i)) = (\epsilon_{im-1} -  \beta b(\mathbf{x}_i; \boldsymbol{\gamma}))^2\]</span></p>
<p>This implies that we are fitting a basis function at each step to minimizing the residual from the current model <span class="math inline">\(f_{m-1} (\mathbf{x})\)</span>.</p>
<h2 id="adaboost-as-forward-stagewise-additive-modeling">AdaBoost as Forward Stagewise Additive Modeling</h2>
<p>Let the loss function be the exponential function:</p>
<p><span class="math display">\[L(y_i, f(\mathbf{x}_i)) = e^{-y_i f(\mathbf{x}_i)}\]</span></p>
<p>For adaboost, the basis functions are individual weak classifier <span class="math inline">\(G_m(x) \in \{-1, 1\}\)</span>. Using the exponential loss for additive stagewise modeling, we must minimize the objective:</p>
<p><span class="math display">\[(\beta_m, G_m) = \underset{\beta, G}{\arg\min} \sum^{N}_{i=1} e^{-y_i(f_{m-1} (\mathbf{x}_i) + \beta G(\mathbf{x}_i))}\]</span></p>
<p>Let <span class="math inline">\(w^{(m)}_i = e^{-y_i \mathbf{x}_i}\)</span>, then the above optimization can be expressed as:</p>
<p><span class="math display">\[(\beta_m, G_m) = \underset{\beta, G}{\arg\min} \sum^{N}_{i=1} w^{(m)}_i e^{-y_i\beta G(\mathbf{x}_i)}\]</span></p>
<p>Since, <span class="math inline">\(w^{(m)}_i\)</span> does not depend on <span class="math inline">\(\beta, G(\mathbf{x})\)</span>, so it is just some constant and can be regard it as weight. Since this weight depends on previous prediction and current label, it is changing at each iteration <span class="math inline">\(m\)</span>.</p>
<h3 id="solve-for-beta">Solve for <span class="math inline">\(\beta\)</span></h3>
<p>In order to solve for <span class="math inline">\(\beta\)</span>, we can rewrite the objective as:</p>
<span class="math display">\[\begin{aligned}
\sum^{N}_{i=1} w^{(m)}_i e^{-y_i\beta G(\mathbf{x}_i)} &amp;= \sum^{N}_{i=1} w^{(m)}_i e^{-\beta} I[y_i = G(\mathbf{x}_i)] + w^{(m)}_i e^{\beta} I[y_i \neq G(\mathbf{x}_i)] \\
&amp;= \sum^{N}_{i=1} w^{(m)}_i e^{-\beta} (1 - I[y_i \neq G(\mathbf{x}_i)]) + w^{(m)}_i e^{\beta} I[y_i \neq G(\mathbf{x}_i)]\\
&amp;= (e^{\beta} - e^{-\beta})\sum^{N}_{i=1} w^{m}_i I[y_i \neq G(\mathbf{x}_i)] + e^{-\beta} \sum^{N}_{i=1} w^{(m)}_i
\end{aligned}\]</span>
<p>by taking the partial derivative w.r.t <span class="math inline">\(\beta\)</span> and set it to 0:</p>
<p><span class="math display">\[\implies(e^\beta + e^{-\beta}) \sum_{i=1}^{N} w_i^{(m)}I(y_i \neq G(\mathbf{x}_i)) - e^{-\beta} \sum_{i=1}^{N} w_i^{(m)} = 0\]</span></p>
<p><span class="math display">\[\implies \frac{e^\beta + e^{-\beta}}{e^{-\beta}} = \frac{\sum_{i=1}^{N} w_i^{(m)}}{\sum_{i=1}^{N} w_i^{(m)}I(y_i \neq G(\mathbf{x}_i))}\]</span></p>
<p><span class="math display">\[\implies \frac{e^\beta}{e^{-\beta}} = \frac{\sum_{i=1}^{N} w_i^{(m)}}{\sum_{i=1}^{N} w_i^{(m)}I(y_i \neq G(\mathbf{x}_i))} - 1\]</span></p>
<p><span class="math display">\[\implies 2\beta = \log(\frac{1}{error_m} - 1)\]</span></p>
<p><span class="math display">\[\implies \beta_m = \frac{1}{2} \log (\frac{1 - error_m}{error_m})\]</span></p>
<p>Where <span class="math inline">\(error_m = \frac{\sum_{i=1}^{N} w_i^{(m)}I(y_i \neq G(\mathbf{x}_i))}{\sum_{i=1}^{N} w_i^{(m)}}\)</span></p>
<p>That is, for any classifier <span class="math inline">\(G\)</span>:</p>
<ul>
<li><span class="math inline">\(\beta_m &gt; 0\)</span>, if <span class="math inline">\(error_m &lt; 0.5\)</span></li>
<li><span class="math inline">\(\beta_m &lt; 0\)</span>, if <span class="math inline">\(error_m &gt; 0.5\)</span></li>
</ul>
<h3 id="solve-for-g">Solve for <span class="math inline">\(G\)</span></h3>
<p>We want our basis function (weak classifier) to have better misclassification rate than random classifier, so we want <span class="math inline">\(error_m &lt; 0.5\)</span> which implies that we want <span class="math inline">\(\beta &gt; 0\)</span>. Thus, for any value <span class="math inline">\(\beta &gt; 0\)</span>, we can see that if we let <span class="math inline">\(G(\mathbf{x}_i) = y_i\)</span> for all samples, we have minimum objective. Thus, we can transform the minimization problem to minimize the error rate:</p>
<span class="math display">\[\begin{aligned}
G_m = \underset{G}{\arg\min} \sum^{N}_{i=1} w^{(m)}_i I[y_i \neq G(\mathbf{x}_i)]
\end{aligned}\]</span>
<p><br></p>
<p>By substitute <span class="math inline">\(G_m\)</span> back to the solution of <span class="math inline">\(\beta\)</span>, we have:</p>
<p><span class="math display">\[\beta_m = \frac{1}{2} \log (\frac{1 - error_m}{error_m})\]</span></p>
<p>Where <span class="math inline">\(error_m = \frac{\sum_{i=1}^{N} w_i^{(m)}I(y_i \neq G_m(\mathbf{x}_i))}{\sum_{i=1}^{N} w_i^{(m)}}\)</span></p>
<p>The approximation is then updated:</p>
<p><span class="math display">\[f_m (\mathbf{x}_i) = f_{m-1} (\mathbf{x}_i) + \beta_m G_m (\mathbf{x}_i)\]</span></p>
<p>Which causes the weights for the next iteration to be:</p>
<span class="math display">\[\begin{aligned}
w^{(m + 1)}_i &amp;= e^{-y_i f_{m} (\mathbf{x}_i)}\\
&amp;= e^{-y_i (f_{m - 1} (\mathbf{x}_i) + \beta_m G_m (\mathbf{x}_i))}\\
&amp;= e^{-y_i f_{m-1}(\mathbf{x}_i)} e^{-y_i \beta_m G_m (\mathbf{x}_i)}\\
&amp;= w^{(m)}_{i} e^{-y_i \beta_m G_m (\mathbf{x}_i)}
\end{aligned}\]</span>
<p>Notice here, <span class="math inline">\(-y_i G_m (x_i) = 2 I[y_i \neq G_m (\mathbf{x}_i)] - 1\)</span>:</p>
<p><span class="math display">\[\implies w^{(m + 1)}_i = w^{(m)}_{i} e^{\alpha_m I[y_i \neq G_m(\mathbf{x}_i)]} e^{-\beta_m }\]</span></p>
<p>Where, <span class="math inline">\(\alpha_m = 2 \beta_m\)</span> is the <span class="math inline">\(\alpha_m\)</span> in Adaboost algorithm, and <span class="math inline">\(e^{-\beta_m}\)</span> has no effect on weights because it is just a constant multiplying every sample.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Thus, we can think of Adaboost algorithm as stagewise additive modeling with basis function as a weak classifier (Minimizing the misclassification rate to have slightly better misclassification rate than random classifier) and exponential loss function.</p>
<h1 id="ref">Ref</h1>
<p>ESLII Chapter 10</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/19/random-forests/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/random-forests/" class="post-title-link" itemprop="url">random_forests</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-19 16:32:53" itemprop="dateCreated datePublished" datetime="2021-07-19T16:32:53+08:00">2021-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-09-06 14:26:15" itemprop="dateModified" datetime="2021-09-06T14:26:15+08:00">2021-09-06</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/07/19/random-forests/" class="post-meta-item leancloud_visitors" data-flag-title="random_forests" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>3.6k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>3 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="random-forest">Random Forest</h1>
<h2 id="bootstrap">Bootstrap</h2>
<h2 id="bagging">Bagging</h2>
<p>Earlier, we see that we can use bootstrap as a way of assessing the accuracy of a parameter estimate or a prediction. Here, we show how to use the bootstrap to improve the estimate or prediction itself.</p>
<p>Consider</p>
<h2 id="random-forest-1">Random Forest</h2>
<p><code>Random Forests</code> is a substantial modification of bagging that builds a large collection of <strong>de-correlated</strong> trees and then averages them. The essential idea in bagging is to average many noisy but approximately unbiased models and hence reduce the variance. Trees are ideal candidates for bagging, since they can capture complex interaction structures in the data, and if grown sufficiently deep, have relatively low bias. Since trees are notoriously noisy, they benefit greatly from the averaging (variance reduction).</p>
<p>Since each tree generated in bagging is identically distributed, the expectation of an average of <span class="math inline">\(B\)</span> such tres is the same as the expectation of any one of them, this means <strong>the bias of bagged trees is the same as that of the individual trees and the only hope is the variance reduction</strong>. This is contrast to boosting, <strong>where the trees are grown in an adaptive way to remove bias, and hence are not identically distributed</strong>.</p>
<p>An average of <span class="math inline">\(B\)</span> i.i.d random variables each with variance <span class="math inline">\(\sigma^2\)</span>, has variance <span class="math inline">\(\frac{1}{B} \sigma^2\)</span>. If the variables are simply i.d (not necessarily independent) with <strong>positive</strong> pairwise correlation <span class="math inline">\(\rho\)</span>, the variance of the average is:</p>
<p><span class="math display">\[\rho \sigma^2 + \frac{1 - \rho}{B} \sigma^2\]</span></p>
<p>As <span class="math inline">\(B\)</span> increases, the second term disappears, but the first remains, and hence <strong>the size of the correlation of paris of bagged trees limits the benefits of averaging</strong>. The idea in random forest is to improve the variance reduction of bagging by reducing the correlation between the trees without increasing the variance too much. This is achieved in the tree-growing process through random selection of the input variables.</p>
<p>Specifically, when growing a tree on a bootstrapped dataset:</p>
<ul>
<li>Before each split, select <span class="math inline">\(m \leq p\)</span> of the input variables at random as candidates for splitting (typically values for <span class="math inline">\(m\)</span> are <span class="math inline">\(\sqrt{p}\)</span> or even as low as 1)</li>
<li>After <span class="math inline">\(B\)</span> such trees <span class="math inline">\(\{T(\mathbf{x};\; \Theta_b)\}^{B}_{b=1}\)</span> are grown, the random forest regression predictor is: <span class="math display">\[\hat{f}^B_{rf} (\mathbf{x}) = \frac{1}{B}\sum^{B}_{b=1} T(\mathbf{x};\; \Theta_b)\]</span></li>
<li>For classification, a random forest obtains a class vote from each tree, and then classifies using majority vote.</li>
</ul>
<p><img src='/images/ML/rf_1.png' width="600"></p>
<h3 id="out-of-bag-samples">Out of Bag Samples</h3>
<p>An importance feature of random forest is its use of <code>out-of-bag</code> samples:</p>
<ul>
<li>For each observation <span class="math inline">\(\mathbf{z}_i = (\mathbf{x}_i, y_i)\)</span>, construct its random forest predictor by averaging only those trees corresponding to bootstrap samples in which <span class="math inline">\(\mathbf{z}_i\)</span> did not appear.</li>
</ul>
<p>An out-of-bag error estimate (If short of data, it can be used as validation error) is almost identical to that obtained by <span class="math inline">\(N\)</span>-fold cross validation, so unlike many other nonlinear estimators, random forest can be fit in one sequence, with cross-validation being performed along the way. Once the OOB error stabilizes, the training can be terminated.</p>
<h3 id="variable-importance">Variable Importance</h3>
<p>Random forests also use the OOB samples to construct a different variable-importance measure, apparently to measure the prediction strength of each variable. When the <span class="math inline">\(b\)</span>th tree is grown, the OOB samples are passed down the tree, and the prediction accuracy is recorded. The values for the <span class="math inline">\(j\)</span>th variable are randomly permuted in the OOB samples, and the accuracy is again computed. The decrease in accuracy as a result of this permuting is averaged over all trees, and is used as a measure of the importance of variable <span class="math inline">\(j\)</span> in the random forest.</p>
<h3 id="overfitting">Overfitting</h3>
<p>It is certainly true that increasing <span class="math inline">\(B\)</span> does not cause the random forest sequence to overfit. like bagging, the random forest estimate approximates the expectation:</p>
<p><span class="math display">\[\hat{f}_{rf} (\mathbf{x}) = E[T(\mathbf{x}; \; \Theta)] = \lim_{B \rightarrow \infty} \hat{f}(\mathbf{x})^B_{rf}\]</span></p>
<p>with an average over <span class="math inline">\(B\)</span> realizations of <span class="math inline">\(\Theta\)</span>. However, <strong>this limit can overfit the data, the average of fully grown trees can result in too rich a model and incur unnecessary variance.</strong> One possible solution is to reduce tree depth.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/19/k-means/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/k-means/" class="post-title-link" itemprop="url">K-Means</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-19 16:32:37" itemprop="dateCreated datePublished" datetime="2021-07-19T16:32:37+08:00">2021-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-08-19 15:11:17" itemprop="dateModified" datetime="2021-08-19T15:11:17+08:00">2021-08-19</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/07/19/k-means/" class="post-meta-item leancloud_visitors" data-flag-title="K-Means" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>3.8k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>3 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="k-means">K-Means</h1>
<p>Considering the problem of identifying groups, or clusters of data points in a multidimensional space.</p>
<p>Suppose we have a dataset <span class="math inline">\(\mathbf{x}_1, ....., \mathbf{x}_N\)</span> consisting of <span class="math inline">\(N\)</span> observations of a random <span class="math inline">\(D\)</span> dimensional euclidean variable <span class="math inline">\(\mathbf{x}\)</span>. <strong>Our goal is to partition these points in to <span class="math inline">\(K\)</span> clusters</strong>.</p>
<p>A cluster <span class="math inline">\(k\)</span> contains:</p>
<ol type="1">
<li><strong>Cluster center</strong>: <span class="math inline">\(\boldsymbol{\mu}_k \in \mathbb{R}^{D}\)</span>.</li>
<li><strong>Assignment indicator variables</strong>: <span class="math inline">\(\mathbf{r}_{n} \in \mathbb{R}^{K}, \; r_{nk} \in \{0, 1\}\)</span>, one for each data point and each dimension <span class="math inline">\(k\)</span> indicates whether the data point <span class="math inline">\(\mathbf{x}_n\)</span> belongs to cluster <span class="math inline">\(k\)</span></li>
</ol>
<p>So we can reformulate our goal to be: <strong>find an assignment of data points to clusters, as well as a set of cluster centers <span class="math inline">\(\{\boldsymbol{\mu}_k\}^{K}_{k=1}\)</span>, such that the sum of squares of the distances of each data point to its closest cluster center is a minimum.</strong> In equation, our objective is:</p>
<p><span class="math display">\[J(\mathbf{x}; \; \{\boldsymbol{\mu}_{k}, \mathbf{r}_{nk}\}) = \sum^{N}_{n=1}\sum^{K}_{k=1} r_{nk} \|\mathbf{x}_n - \boldsymbol{\mu}_k\|^2_2\]</span></p>
<p>Where the distance measure here is the L2 norm.</p>
<p>Thus, we want to find parameters <span class="math inline">\(\{\boldsymbol{\mu}_{k}, \mathbf{r}_{nk}\}\)</span> such that the objective is minimized.</p>
<h2 id="algorithm">Algorithm</h2>
<ol type="1">
<li>Initialize <span class="math inline">\(\boldsymbol{\mu}_k, \; \forall \; k=1, ...., K\)</span></li>
<li>Given <span class="math inline">\(\{\boldsymbol{\mu}_k\}\)</span>, minimize <span class="math inline">\(J\)</span> w.r.t <span class="math inline">\(\{\mathbf{r}_{n}\}\)</span></li>
<li>Given <span class="math inline">\(\{\mathbf{r}_{n}\}\)</span>, minimize <span class="math inline">\(J\)</span> w.r.t <span class="math inline">\(\{\boldsymbol{\mu}_k\}\)</span></li>
<li>Repeat 2, 3 until converges</li>
</ol>
<p><strong>For phase 2</strong>:</p>
<p>Given the cluster centers, it is obvious that if we assign each point to the closest cluster center, we have the minimum objective:</p>
<p><span class="math display">\[
r_{nk}=
\begin{cases}
1, \quad \text{if }\; k = \underset{k}{\arg\min} \|\mathbf{x}_n - \boldsymbol{\mu}_k\|^2_2\\
0, \quad o.w\\
\end{cases}
\]</span></p>
<p><strong>For phase 3</strong>:</p>
<p>Given the assignments, since the objective is convex, we can take gradient and solve for the optimal <span class="math inline">\(\boldsymbol{\mu}_k\)</span>:</p>
<p><span class="math display">\[\frac{\partial J}{\partial \boldsymbol{\mu}_k} = \sum^{N}_{n=1} -2 r_{nk} \mathbf{x}_n + 2\boldsymbol{\mu}_k \sum^{N}_{n=1} r_{nk} = 0\]</span></p>
<p><span class="math display">\[\implies \boldsymbol{\mu}_k \sum^{N}_{n=1} r_{nk} = \sum^{N}_{n=1} r_{nk} \mathbf{x}_n\]</span></p>
<p><span class="math display">\[\implies \boldsymbol{\mu}_k = \frac{\sum^{N}_{n=1} r_{nk} \mathbf{x}_n}{\sum^{N}_{n=1} r_{nk}}\]</span></p>
<p><span class="math inline">\(\sum^{N}_{n=1} r_{nk} \mathbf{x}_n\)</span> is the sum of all points that belongs to cluster <span class="math inline">\(k\)</span> and <span class="math inline">\(\sum^{N}_{n=1} r_{nk}\)</span> is the count of points in cluster <span class="math inline">\(k\)</span>, so the new <span class="math inline">\(\boldsymbol{\mu}_k\)</span> is just the <strong>sample mean</strong> of the points in cluster <span class="math inline">\(k\)</span>.</p>
<h2 id="convergence">Convergence</h2>
<p>Since:</p>
<ol type="1">
<li><span class="math inline">\(\{\boldsymbol{\mu}_{k}, \mathbf{r}_{nk}\}\)</span> can only take finite values (i.e they are derived from finite subsets of data and uniquely defined for each subset).</li>
<li>At each step, we minimize the objective (i.e we either decrease or maintain the objective).</li>
<li>The cost function is bounded below zero.</li>
<li>Ties are broken consistently.</li>
</ol>
<p>Thus, the algorithm can only take a finite number of non-decreasing steps before terminating at a local minimum.</p>
<h2 id="implementation">Implementation</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KMeans</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, k=<span class="number">2</span>, max_iteration=<span class="number">10</span>, init_method=<span class="string">&#x27;random&#x27;</span></span>):</span></span><br><span class="line">        self.k = k</span><br><span class="line">        self.max_iteration = max_iteration</span><br><span class="line">        self.init_method = init_method</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span>(<span class="params">self, x: np.array</span>):</span></span><br><span class="line">        n, d = x.shape</span><br><span class="line">        curr_mu = self._init_mu(x)</span><br><span class="line">        curr_iter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> curr_iter &lt;= self.max_iteration:</span><br><span class="line">            r_matrix = np.zeros((n, self.k))</span><br><span class="line">            <span class="comment"># step one, map each instance to cluster center muk</span></span><br><span class="line">            <span class="keyword">for</span> sample <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                distance = []</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(self.k):</span><br><span class="line">                    distance.append(np.linalg.norm(x[sample] - curr_mu[j]))</span><br><span class="line"></span><br><span class="line">                mu_assigned = np.argmin(distance)</span><br><span class="line">                r_matrix[sample][mu_assigned] = <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">            <span class="comment"># step two calculate new mu_k</span></span><br><span class="line">            prev_mu = curr_mu.copy()</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(self.k):</span><br><span class="line">                curr_mu[j] = np.mean(x[r_matrix[:, j] == <span class="number">1</span>], axis=<span class="number">0</span>)</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># check stopping criteria</span></span><br><span class="line">            <span class="keyword">if</span> np.linalg.norm(prev_mu - curr_mu) == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            curr_iter += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;finished k-means algorithm, on iteration <span class="subst">&#123;curr_iter&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> r_matrix, curr_mu</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_mu</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.init_method == <span class="string">&#x27;random&#x27;</span>:</span><br><span class="line">            col_max = x.<span class="built_in">max</span>(axis=<span class="number">0</span>)</span><br><span class="line">            col_min = x.<span class="built_in">min</span>(axis=<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> (col_max - col_min) * np.random.random_sample((self.k, x.shape[<span class="number">1</span>])) + col_min</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> self.init_method == <span class="string">&#x27;random_points&#x27;</span>:</span><br><span class="line">            random_int = np.random.randint(<span class="number">0</span>, x.shape[<span class="number">0</span>], size=self.k)</span><br><span class="line">            <span class="keyword">return</span> x[random_int]</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/19/pca/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/pca/" class="post-title-link" itemprop="url">PCA</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-19 16:32:26" itemprop="dateCreated datePublished" datetime="2021-07-19T16:32:26+08:00">2021-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-08-26 16:46:58" itemprop="dateModified" datetime="2021-08-26T16:46:58+08:00">2021-08-26</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/07/19/pca/" class="post-meta-item leancloud_visitors" data-flag-title="PCA" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>3.6k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>3 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="principal-component-analysis">Principal Component Analysis</h1>
<p>Let <span class="math inline">\([\mathbf{X}_1 ... \mathbf{X}_N]\)</span> be a <span class="math inline">\(p \times N\)</span> matrix of random sample where <span class="math inline">\(\mathbf{X}_i \in \mathbb{R}^p\)</span>. The sample mean <span class="math inline">\(\mathbf{M}\)</span> of the random sample is defined as:</p>
<p><span class="math display">\[\mathbf{M} = \frac{1}{N} \sum^{N}_{i=1} \mathbf{X}_{i}\]</span></p>
<p>Let <span class="math inline">\(\hat{\mathbf{X}}_k\)</span> be the centered random vector for <span class="math inline">\(k = 1, ...., N\)</span>:</p>
<p><span class="math display">\[\hat{\mathbf{X}}_k = \mathbf{X}_k - \mathbf{M}\]</span></p>
<p>Then we can defined the centered random sample matrix as:</p>
<p><span class="math display">\[B = [\hat{\mathbf{X}}_1 ... \hat{\mathbf{X}}_N]_{[p\times N]}\]</span></p>
<p>The sample covariance matrix <span class="math inline">\(S\)</span> is then:</p>
<p><span class="math display">\[S = [\frac{1}{N} BB^T]_{[p\times p]}\]</span></p>
<p>We can easily show that <span class="math inline">\(S\)</span> is positive semi-definite. Assume that <span class="math inline">\(\mathbf{x} &gt; 0\)</span>:</p>
<span class="math display">\[\begin{aligned}
BB^T \mathbf{x} &amp;= \lambda \mathbf{x}\\
\implies \mathbf{x}^T BB^T \mathbf{x} &amp;= \lambda \mathbf{x}^T \mathbf{x}\\
\implies \frac{\mathbf{x}^T BB^T \mathbf{x}}{\mathbf{x}^T \mathbf{x}} &amp;= \lambda
\end{aligned}\]</span>
<p>Let <span class="math inline">\(\mathbf{A} = B^T\mathbf{x}\)</span>, then:</p>
<p><span class="math display">\[\frac{\mathbf{A}^T \mathbf{A}}{\mathbf{x}^T \mathbf{x}} = \lambda\]</span></p>
<p>Since, <span class="math inline">\(\mathbf{A}^T \mathbf{A} \geq 0\)</span> for any vector <span class="math inline">\(A\)</span>, similar for <span class="math inline">\(\mathbf{x}^T \mathbf{x}\)</span>, then:</p>
<p><span class="math display">\[\lambda \geq 0\]</span></p>
<p>Since, <span class="math inline">\(S\)</span> is symmetric and <span class="math inline">\(\lambda \geq 0\)</span> for all eigenvalues of <span class="math inline">\(S\)</span>, we can conclude that <span class="math inline">\(S\)</span> is <strong>positive semi-definite</strong>.</p>
<p>The <strong>total variance</strong> in the data is:</p>
<p><span class="math display">\[tr(S)\]</span></p>
<h2 id="pca">PCA</h2>
<p><strong>The goal of PCA is to find an orthogonal <span class="math inline">\(p \times p\)</span> matrix <span class="math inline">\(P = [\mathbf{u}_1 .... \mathbf{u}_p]\)</span> that determines a change of variable, <span class="math inline">\(\mathbf{X} = P\mathbf{Y}\)</span> with the property that the features of <span class="math inline">\(\mathbf{Y}\)</span> are uncorrelated and are arranged in order of decreasing variance.</strong></p>
<p>Assume the <span class="math inline">\(\mathbf{X}\)</span> is already being centered, that is <span class="math inline">\(B = [\mathbf{X}_1 .... \mathbf{X}_N]\)</span>, then <span class="math inline">\(\mathbf{Y}\)</span> is also centered since <span class="math inline">\(P \neq 0\)</span>:</p>
<p><span class="math display">\[E[\mathbf{X}] = P\cdot E[\mathbf{Y}] = 0\]</span></p>
<p>Then the sample covariance matrix of <span class="math inline">\(\mathbf{Y}\)</span> is:</p>
<p><span class="math display">\[S_Y = \frac{1}{N-1}[P^{-1}\mathbf{X}_1 .... P^{-1}\mathbf{X}_N] [P^{-1}\mathbf{X}_1 .... P^{-1}\mathbf{X}_N]^T\]</span></p>
<p><span class="math display">\[\implies S_{\mathbf{Y}} = \frac{1}{N-1} P^{T}BB^{T}P = P^{T} S_{\mathbf{X}} P\]</span></p>
<p>So the desired orthogonal matrix is the one that makes <span class="math inline">\(\hat{Cov}[Y_i, Y_j] = 0, \; \forall i \neq j\)</span> (features are uncorrelated), which means that the we want the sample covariance matrix <span class="math inline">\(S_Y\)</span> to be diagonal.</p>
<p><br></p>
<p>Let <span class="math inline">\(D\)</span> be a diagonal matrix with eigenvalues of <span class="math inline">\(S_{\mathbf{X}}\)</span>, <span class="math inline">\(\lambda_1 ,...., \lambda_p\)</span> on the diagonal s.t <span class="math inline">\(\lambda_1 \geq \lambda_2 \geq ..... \lambda_p \geq 0\)</span>, let <span class="math inline">\(P\)</span> be an orthogonal matrix whose columns are the corresponding unit eigenvectors <span class="math inline">\(\mathbf{u}_1, ....., \mathbf{u}_p\)</span> (Symmetric matrices have property that the eigenspaces are mutually orthogonal, in the sense that eigenvectors corresponding to different eigenvalues are orthogonal). Then, we can use <span class="math inline">\(P\)</span> and <span class="math inline">\(\mathbf{X}\)</span> to represent <span class="math inline">\(Y\)</span>, and the sample covariance matrix is:</p>
<p><span class="math display">\[S_{\mathbf{X}} = PDP^T \implies P^{-1} S_{\mathbf{X}} P = D\]</span></p>
<p>Thus, <span class="math inline">\(S_{\mathbf{Y}} = D\)</span>.</p>
<p>Then, the eigenvectors <span class="math inline">\(\mathbf{u}_1, ...., \mathbf{u}_p\)</span> are called <code>Principal Components</code> of the data. The first PC is the eigenvector corresponding to the largest eigenvalue of <span class="math inline">\(S_{\mathbf{X}}\)</span>.</p>
<p>The result transformation is defiend as:</p>
<p><span class="math display">\[P^{T} \mathbf{X} = \mathbf{Y}\]</span></p>
<p><span class="math display">\[
\mathbf{Y} = 
\begin{bmatrix}
\mathbf{u}^T_1 \mathbf{X} \\
.\\
.\\
.\\
\mathbf{u}^T_p \mathbf{X}\\
\end{bmatrix}
\]</span></p>
<h2 id="total-variance">Total Variance</h2>
<p>It can be shown that the PCA transformation does not change the total variance of the data, that is the total variance of the data is the sum of eigenvalues:</p>
<p><span class="math display">\[tr(S_{\mathbf{Y}}) = tr(D)\]</span></p>
<p>This is only true when we are dealing with sample covariance matrix,</p>
<h1 id="implementation">Implementation</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span>(<span class="params">X, k=<span class="number">2</span></span>):</span></span><br><span class="line">    u, s, vt = np.linalg.svd(X, full_matrices=<span class="literal">False</span>)</span><br><span class="line">    xv = u * s</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> xv[:, :k]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span>(<span class="params">X</span>):</span></span><br><span class="line">    n, d = X.shape</span><br><span class="line">    x_trans = X.copy()</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(d):</span><br><span class="line">        <span class="comment"># normalization</span></span><br><span class="line">        x_trans[:, j] = (X[:, j] - X[:, j].mean()) / X[:, j].std()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_trans</span><br></pre></td></tr></table></figure>
<h1 id="ref">Ref</h1>
<p>https://stats.stackexchange.com/questions/266864/why-is-the-sum-of-eigenvalues-of-a-pca-equal-to-the-original-variance-of-the-dat</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/19/svm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/svm/" class="post-title-link" itemprop="url">SVM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-19 16:32:20" itemprop="dateCreated datePublished" datetime="2021-07-19T16:32:20+08:00">2021-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-09-10 20:11:23" itemprop="dateModified" datetime="2021-09-10T20:11:23+08:00">2021-09-10</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/07/19/svm/" class="post-meta-item leancloud_visitors" data-flag-title="SVM" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>10k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>9 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="support-vector-machine">Support Vector Machine</h1>
<p>Given dataset <span class="math inline">\(D = \{(\mathbf{x}_1, y_1) ....., (\mathbf{x}_N, y_N); \; \mathbf{x} \in \mathbb{R}^m, \; y_i \in \{-1, 1\}\}\)</span>. Let <span class="math inline">\(\mathbf{w}^T \mathbf{x} + b = 0\)</span> be a hyperplane, we classify a new point <span class="math inline">\(\mathbf{x}_i\)</span> by</p>
<p><span class="math display">\[
\hat{y}_i (\mathbf{x}_i) =
\begin{cases}
\mathbf{w}^T \mathbf{x}_i + b \geq 0, \quad \;\;\; 1\\
\mathbf{w}^T \mathbf{x}_i + b &lt; 0, \quad -1\\
\end{cases}
\]</span></p>
<p>Suppose that our data is <strong>linear separable</strong>. Then, <span class="math inline">\(\exists\)</span> at least one possible combination of parameters <span class="math inline">\(\{\mathbf{w}, b\}\)</span>, such that:</p>
<p><span class="math display">\[\hat{\gamma}_i = y_i \hat{y}_i(\mathbf{x}_i) &gt; 0, \; \forall i=1, ...., N\]</span></p>
<p>When <span class="math inline">\(y_i = 1\)</span>, a confident classifier would have <span class="math inline">\(\hat{y}_i(\mathbf{x}_i)\)</span> as large as possible. On the other hand, when <span class="math inline">\(y_i = -1\)</span>, the confident classifier would have <span class="math inline">\(\hat{y}_i(\mathbf{x}_i)\)</span> as negative as possible. Thus, we want <span class="math inline">\(\hat{\gamma}_i\)</span> as large as possible, this <span class="math inline">\(\hat{\gamma}_i\)</span> is called <code>functional margin</code> associated with training example for specific set of parameters <span class="math inline">\(\{\mathbf{w}, b\}\)</span>. And <code>functional margin</code> of <span class="math inline">\(\{\mathbf{w}, b\}\)</span> is defined as minimum of these functions margins:</p>
<p><span class="math display">\[\hat{\gamma} = \min_{i=1, ..., N} \hat{\gamma}_i\]</span></p>
<p><strong>In support vector machines the decision boundary is chosen to be the one for which the functional margin (confidence) is maximized.</strong></p>
<h2 id="distance-to-plane">Distance to Plane</h2>
<p>Let <span class="math inline">\(\mathbf{x}_i\)</span> be a sample that has label <span class="math inline">\(y_i = 1\)</span>, thus, it is on the positive side of the hyperplane <span class="math inline">\(\mathbf{w}^T \mathbf{x} + b = 0\)</span>. Define <span class="math inline">\(r\)</span> to be the shortest distance between point <span class="math inline">\(\mathbf{x}_i\)</span> and the hyperplane. Then <span class="math inline">\(r\)</span> is the distance between <span class="math inline">\(\mathbf{x}_i\)</span> to its projection on the hyperplane <span class="math inline">\(\mathbf{x}^{\prime}_i\)</span>.</p>
<p><img src='/images/ML/svm_1.png' width="600"></p>
<p>Since <span class="math inline">\(\mathbf{w}\)</span> is the normal vector that is orthogonal to the plane and <span class="math inline">\(\frac{\mathbf{w}}{\|\mathbf{w}\|_2}\)</span> is the unit vector that represents its direction. We can write the <span class="math inline">\(r\)</span> as:</p>
<p><span class="math display">\[\mathbf{x}_i - \mathbf{x}^{\prime}_i = r \frac{\mathbf{w}}{\|\mathbf{w}\|_2}\]</span></p>
<p><span class="math display">\[\implies r = \frac{\mathbf{x}_i - \mathbf{x}^{\prime}_i}{\frac{\mathbf{w}}{\|\mathbf{w}\|_2}}\]</span></p>
<h2 id="goal">Goal</h2>
<p>The concept of the margin is intuitively simple: it is the distance of the separating hyperplane to the closest examples in the dataset, assuming that our dataset is <strong>linearly separable.</strong> That is:</p>
<span class="math display">\[\begin{aligned}
&amp;\max \quad &amp;&amp; \hat{\gamma}\\
&amp;\;\text{s.t} \quad &amp;&amp;y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq \hat{\gamma} \quad \quad \forall i=1, ...., N
\end{aligned}\]</span>
<p>This optimization problem is unbounded, because one can make the functional margin large by simply scaling the parameters by a constant <span class="math inline">\(c\)</span>, <span class="math inline">\(\{c\mathbf{w}, cb\}\)</span>:</p>
<p><span class="math display">\[y_i (c\mathbf{w}^T \mathbf{x}_i + cb) &gt; y_i (\mathbf{w}^T \mathbf{x}_i + b) = \hat{\gamma}\]</span></p>
<p>This has no effect on the decision plane because:</p>
<p><span class="math display">\[\mathbf{w}^T \mathbf{x}_i + b = c\mathbf{w}^T \mathbf{x}_i + cb = 0\]</span></p>
<p>Thus, we need to transform the optimization problem to <strong>maximize the distance between the samples and decision boundary</strong> instead of maximizing functional margin. Suppose we let all functional margins to be at least 1 (can easily achieve by multiplying parameters by a constant):</p>
<p><span class="math display">\[y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1\]</span></p>
<p><span class="math display">\[\implies \mathbf{w}^T \mathbf{x}_i + b \geq 1\]</span></p>
<p><span class="math display">\[\implies \mathbf{w}^T \mathbf{x}_i + b \leq -1\]</span></p>
<p>Then, for point <span class="math inline">\(\mathbf{x}_i\)</span> on <span class="math inline">\(\mathbf{w}^T \mathbf{x}_i + b = 1\)</span>, we have its distance to the decision plane:</p>
<p><span class="math display">\[\mathbf{w}^T \mathbf{x}_i + b - r \frac{\|\mathbf{w}\|^2_2}{\|\mathbf{w}\|_2} = 0\]</span></p>
<p><span class="math display">\[\implies r = \frac{1}{\|\mathbf{w}\|}\]</span></p>
<p>Then, we can formulate our objective as:</p>
<span class="math display">\[\begin{aligned}
&amp;\max_{\mathbf{w}, b} \quad &amp;&amp; \frac{1}{\|\mathbf{w}\|_2}\\
&amp;\;\text{s.t} \quad &amp;&amp;y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 \quad \quad \forall i=1, ...., N
\end{aligned}\]</span>
<p>Which is equivalent to:</p>
<span class="math display">\[\begin{aligned}
&amp;\min_{\mathbf{w}, b} \quad &amp;&amp; \frac{1}{2}\|\mathbf{w}\|^2_2\\
&amp;\;\text{s.t} \quad &amp;&amp;y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 \quad \quad \forall i=1, ...., N
\end{aligned}\]</span>
<p><br></p>
<p><img src='/images/ML/svm_2.png' width="600"></p>
<h2 id="soft-margin-svm">Soft Margin SVM</h2>
<h3 id="slack-variables">Slack Variables</h3>
<p>Notice, in the above formulation, we have hard constraints on the margins which do not allow misclassification of points. However, in real world, data points are rarely linear separable and there will be outliers in the dataset, we may wish to allow some examples to be on the wrong side of the hyperplane or to have margin less than 1 .</p>
<p><img src='/images/ML/svm_3.png' width="600"></p>
<p>To resolve this problem, we can introduce slack variables one for each data point to relax the hard constraints:</p>
<p><span class="math display">\[y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i\]</span> <span class="math display">\[\quad\xi_i \geq 0, \; \; \forall i=1, ...., N\]</span></p>
<p>To encourage correct classification of the samples, we add <span class="math inline">\(\xi_i\)</span> to the objective:</p>
<span class="math display">\[\begin{aligned}
&amp;\min_{\mathbf{w}, b, \boldsymbol{\xi}} \quad &amp;&amp; \frac{1}{2}\|\mathbf{w}\|^2_2 + C\sum^{N}_{n=1} \xi_i\\
&amp;\;\text{s.t} \quad &amp;&amp;y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i  \quad &amp;&amp;&amp;\forall i=1, ...., N\\
&amp; &amp;&amp; \xi_i \geq 0, &amp;&amp;&amp;\forall i=1, ...., N
\end{aligned}\]</span>
<p>Thus, sample points are now permitted to have margin less than 1, and if an example <span class="math inline">\(\mathbf{x}_i\)</span> has slack variable greater than 0, we would have penalty in the objective function <span class="math inline">\(C\xi_i\)</span>. The parameter <span class="math inline">\(C\)</span> controls the relative weighting between the twin goals of making the <span class="math inline">\(\|\mathbf{w}\|\)</span> small and of ensuring that most examples have functional margin at least 1.</p>
<p><br></p>
<h3 id="dual-problem">Dual Problem</h3>
<p>Using <strong>Lagrange Multiplier</strong>, we can transform the constrained problem into an unconstrained concave problem:</p>
<p><span class="math display">\[\max_{\boldsymbol{\alpha}, \boldsymbol{\eta}}\;\min_{\mathbf{w}, b, \boldsymbol{\xi}} \; \frac{1}{2}\|\mathbf{w}\|^2_2 + C\sum^{N}_{n=1} \xi_i - \sum^N_{i=1} \alpha_i [y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 + \xi_i] - \sum^{N}_{i=1} \eta_i \xi_i\]</span></p>
<p>Where the inner minimization is the dual function and the maximization w.r.t <span class="math inline">\(\alpha\)</span> is called dual problem.</p>
<h3 id="kkt">KKT</h3>
<p>For an unconstrained convex optimization problem, we know we are at global minimum if the gradient is zero. The KKT conditions are the equivalent conditions for the global minimum of a constrained convex optimization problem. <span class="math inline">\(\forall i=1, ...., N\)</span>:</p>
<ol type="1">
<li><p><strong>Stationarity</strong>, If the strong duality holds, <span class="math inline">\((\mathbf{w}^*, \boldsymbol{\alpha}^*)\)</span> is optimal, then <span class="math inline">\(\mathbf{w}^*\)</span> minimizes <span class="math inline">\(L(\mathbf{w}^*, \boldsymbol{\alpha}^*)\)</span> (same for <span class="math inline">\(b^*, \xi^*\)</span> which are formulated as constraints in the dual problem):</p>
<p><span class="math display">\[\nabla_{\mathbf{w}} L(\mathbf{w}^*, \boldsymbol{\alpha}^*) = 0\]</span> <span class="math display">\[\implies \mathbf{w}^* = \sum_{i=1}^{n} \alpha^*_i y_i \mathbf{x}_i\]</span></p></li>
<li><p><strong>Complementary Slackness</strong>: <span class="math display">\[\alpha_i [y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 + \xi_i] = 0\]</span></p></li>
<li><p><strong>Primal Feasibility</strong>: <span class="math display">\[y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 + \xi_i \geq 0\]</span> <span class="math display">\[\eta_i\xi_i = 0\]</span></p></li>
<li><p><strong>Dual Feasibility</strong>: <span class="math display">\[\alpha_i, \eta_i, \xi_i \geq 0\]</span></p></li>
</ol>
<h4 id="solving-dual-problem">Solving Dual Problem</h4>
<p>We now solve for the dual function by fixing <span class="math inline">\(\{\alpha_i\, \eta_i\}\)</span> (satisfying Stationarity condition):</p>
<p><span class="math display">\[\min_{\mathbf{w}, b, \boldsymbol{\xi}} \frac{1}{2}\|\mathbf{w}\|^2_2 + C\sum^{N}_{n=1} \xi_i - \sum^N_{i=1} \alpha_i [y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 + \xi_i] - \sum^{N}_{i=1} \eta_i \xi_i\]</span></p>
<span class="math display">\[\begin{aligned}
&amp; \frac{\partial L(\mathbf{w}, b, \boldsymbol{\xi})}{\partial \mathbf{w}} = \mathbf{w} - \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i = 0 \\
&amp; \implies \mathbf{w}^* = \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i \\
&amp; \frac{\partial L(\mathbf{w},, b, \boldsymbol{\xi})}{\partial b} =\sum_{i=1}^{n} \alpha_i y_i = 0 \\
&amp; \implies \sum_{i=1}^{n} \alpha_i y_i = 0 \\
&amp; \frac{\partial L(\mathbf{w}, b, \boldsymbol{\xi})}{\partial \xi_n} = C - \alpha_n - \eta_n = 0 \\
&amp; \implies \alpha_n = C - \eta_n
\end{aligned}\]</span>
<p>Substitute back to the original equation, we obtain the dual function:</p>
<p><span class="math display">\[g(\boldsymbol{\alpha}) = -\frac{1}{2}\sum_{i=1}^{N}\sum_{k=1}^{N}\alpha_i \alpha_k {\mathbf{x}_i}^T \mathbf{x}_k y_i y_k + \sum_{i=1}^{N} \alpha_i\]</span></p>
<p>Then, we have the dual problem:</p>
<span class="math display">\[\begin{aligned}
&amp; \underset{\boldsymbol{\alpha}}{\text{max}}
&amp; &amp;  g(\boldsymbol{\alpha}) = -\frac{1}{2}\sum_{i=1}^{N}\sum_{k=1}^{N}\alpha_i \alpha_k {\mathbf{x}_i}^T \mathbf{x}_k y_i y_k + \sum_{i=1}^{N} \alpha_i\\
&amp; \text{subject to}
&amp; &amp; 0 \leq \alpha_i \leq C \\
&amp; &amp; &amp; \sum_{i=1}^{n} \alpha_i y_i = 0 \\
\end{aligned}\]</span>
<p>This is a quadratic programming problem that we can solve using quadratic programming.</p>
<h3 id="interpretation">Interpretation</h3>
<p>We could conclude:</p>
<ol type="1">
<li><p>if <span class="math inline">\(0 &lt; \alpha_i &lt; C \implies y_i(w^T x_i + b) = 1 - \xi_i\)</span> Since <span class="math inline">\(\alpha_i = C - \mu_i, \mu_i \geq 0\)</span>, we have <span class="math inline">\(\xi_i =0 \implies\)</span> the points are with <span class="math inline">\(0 &lt; \alpha_i &lt; C\)</span> are on the margin</p></li>
<li><p>if <span class="math inline">\(\alpha_i = C\)</span></p>
<ul>
<li><span class="math inline">\(0 &lt; \xi_i &lt; 1\)</span>: the points are inside the margin on the correct side</li>
<li><span class="math inline">\(\xi_i = 1\)</span>: the points are on the decision boundary</li>
<li><span class="math inline">\(\xi_i &gt; 1\)</span>: the points are inside the wrong side of the margin and misclassified</li>
</ul></li>
<li><p>if <span class="math inline">\(\alpha_i = 0\)</span>, the points are not support vectors, have no affect on the weight.</p></li>
</ol>
<p>After finding the optimal values for <span class="math inline">\(\boldsymbol{\alpha}\)</span>, we obtain optimal <span class="math inline">\(\mathbf{w}^*\)</span> by solving:</p>
<p><span class="math display">\[\mathbf{w}^* = \sum_{i=1}^{n} \alpha^*_i y_i \mathbf{x}_i\]</span></p>
<p>We obtain optimal <span class="math inline">\(b^*\)</span> by realizing that the points on the margins have <span class="math inline">\(0 &lt; \alpha_i &lt; C\)</span>. Let <span class="math inline">\(\mathbf{x}_i\)</span> be one of those points, then:</p>
<p><span class="math display">\[{\mathbf{w^*}}^T \mathbf{x}_i + b = y_i\]</span></p>
<p>Let <span class="math inline">\(M\)</span> be the set of all points that lies exactly on the margin, a more stable solution is obtained by averaging over all points:</p>
<p><span class="math display">\[b^* = \frac{1}{N_m} \sum^{N_m}_{i=1} (y_i - {\mathbf{w^*}}^T\mathbf{x}_i)\]</span></p>
<h2 id="kernel-tricks">Kernel Tricks</h2>
<h1 id="implementation">Implementation</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cvxopt <span class="keyword">import</span> matrix, solvers</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> qpsolvers <span class="keyword">import</span> solve_qp</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVM</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, c=<span class="number">1</span>, kernel=<span class="string">&#x27;linear&#x27;</span></span>):</span></span><br><span class="line">        self.c = c</span><br><span class="line">        self.kernel = kernel</span><br><span class="line">        self.b = <span class="literal">None</span></span><br><span class="line">        self.dual_coef_ = <span class="literal">None</span></span><br><span class="line">        self.decision_matrix = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y</span>):</span></span><br><span class="line">        n, d = X.shape</span><br><span class="line">        y = y.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        yyt = np.matmul(y, y.T)</span><br><span class="line">        P = np.zeros((n, n))</span><br><span class="line">        q = matrix(-np.ones((n, <span class="number">1</span>)))</span><br><span class="line">        a = matrix(y.T, tc=<span class="string">&#x27;d&#x27;</span>)</span><br><span class="line">        b = matrix([<span class="number">0.0</span>])</span><br><span class="line">        G = matrix(np.row_stack([np.diag([-<span class="number">1</span>] * n), np.diag([<span class="number">1</span>] * n)]), tc=<span class="string">&#x27;d&#x27;</span>)</span><br><span class="line">        h = matrix(np.row_stack([np.array([<span class="number">0</span>] * n).reshape(n, <span class="number">1</span>),</span><br><span class="line">                                 np.array([self.c] * n).reshape(n, <span class="number">1</span>)]), tc=<span class="string">&#x27;d&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                P[i][j] = self.apply_kernel(X[i], X[j])</span><br><span class="line"></span><br><span class="line">        P = matrix(P * yyt)</span><br><span class="line">        alpha = np.array(solvers.qp(P, q, G, h , a, b)[<span class="string">&#x27;x&#x27;</span>])</span><br><span class="line">        alpha[alpha &lt; np.mean(alpha) * <span class="number">0.1</span>] = <span class="number">0</span></span><br><span class="line">        temp_x = np.column_stack([X, alpha, y])</span><br><span class="line">        m = temp_x[(temp_x[:, -<span class="number">2</span>] &gt; <span class="number">0</span>) &amp; (temp_x[:, -<span class="number">2</span>] &lt; self.c)]</span><br><span class="line">        N_m = <span class="built_in">len</span>(m)</span><br><span class="line">        self.decision_matrix = m[:, :-<span class="number">2</span>]</span><br><span class="line">        self.b = <span class="number">0</span></span><br><span class="line">        self.dual_coef_ = m[:, -<span class="number">1</span>] * m[:, -<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">## get b</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N_m):</span><br><span class="line">            self.b += m[i, -<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(N_m):</span><br><span class="line">                self.b -= m[j, -<span class="number">2</span>] * m[j, -<span class="number">1</span>] * self.apply_kernel(m[i, :-<span class="number">2</span>], m[j, :-<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        self.b = self.b / N_m</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply_kernel</span>(<span class="params">self, x_1, x_2</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.kernel == <span class="string">&#x27;linear&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> np.dot(x_1, x_2)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decision_function</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        pred_results = np.array([])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">            pred = self.b</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.decision_matrix)):</span><br><span class="line">                pred += self.dual_coef_[j] * self.apply_kernel(X[i], self.decision_matrix[j])</span><br><span class="line"></span><br><span class="line">            pred_results = np.append(pred_results, pred)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> pred_results</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        pred_results = self.decision_function(X)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> np.where(pred_results &gt;= <span class="number">0</span>, <span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="ref">Ref</h1>
<p>https://www.ccs.neu.edu/home/vip/teach/MLcourse/6_SVM_kernels/lecture_notes/svm/svm.pdf</p>
<p>http://www.cs.cmu.edu/~guestrin/Class/10701-S06/Slides/svms-s06.pdf</p>
<p>MML book</p>
<p>Lagrangian Duality for Dummies, David Knowles</p>
<p>PRML Chapter 7</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/19/naive-bayes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/naive-bayes/" class="post-title-link" itemprop="url">Naive Bayes</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-19 16:32:15" itemprop="dateCreated datePublished" datetime="2021-07-19T16:32:15+08:00">2021-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-24 23:06:03" itemprop="dateModified" datetime="2021-07-24T23:06:03+08:00">2021-07-24</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/07/19/naive-bayes/" class="post-meta-item leancloud_visitors" data-flag-title="Naive Bayes" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>7.2k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>7 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="naive-bayes">Naive Bayes</h1>
<p>Suppose our training set consists of data samples <span class="math inline">\(D = \{(\mathbf{x}_1, y_1), ...., (\mathbf{x}_N, y_N)\}, \; \mathbf{x_i} \in \mathbb{R}^d\)</span>, where <span class="math inline">\(D = \{(\mathbf{x}_i, y_i)\}\)</span> are realizations of a random sample that follows unknown joint distribution <span class="math inline">\(P(\mathbf{X}, Y)\)</span>.</p>
<p><strong>Assumptions</strong>:</p>
<ol type="1">
<li><p><strong>Features are conditionally independent (Naive bayes assumption)</strong>: <span class="math display">\[P(\mathbf{X} | Y) = \prod^{d}_{j=1} P(X_j | Y)\]</span></p></li>
<li><p><strong>MLE assumption</strong>: Random sample is identically distributed.</p></li>
<li><p><strong>Positional independence</strong>: The position of features does not matter (used in Multinomial case).</p></li>
</ol>
<p>By applying bayes rule (applying on distribution <span class="math inline">\(P (\cdot)\)</span> to make things general), we have:</p>
<p><span class="math display">\[P(Y | \mathbf{X}) = \frac{P(\mathbf{X}, Y)}{P(\mathbf{X})} = \frac{P(\mathbf{X} | Y) P(Y)}{P(\mathbf{X})}\]</span></p>
<p>By substituting the assumption:</p>
<p><span class="math display">\[P(Y | \mathbf{X}) = \frac{\prod^{d}_{j=1} P(X_j | Y)P(Y)}{P(\mathbf{X})}\]</span></p>
<p>Since the probability distribution <span class="math inline">\(P(\mathbf{X})\)</span> characterised by <span class="math inline">\(F_{\mathbf{X}}(\mathbf{x})\)</span> is constant for any given <span class="math inline">\(\mathbf{x}\)</span>, we can drop it from the equation because it only changes <span class="math inline">\(P(Y | \mathbf{X})\)</span> by a proportion:</p>
<p><span class="math display">\[P(Y | \mathbf{X}) \propto P(Y) \prod^{d}_{j=1} P(X_j | Y)\]</span></p>
<p>Our goal is to find a class <span class="math inline">\(\hat{y}\)</span> that maximize the probability given input <span class="math inline">\(\mathbf{X} = \mathbf{x}\)</span>:</p>
<span class="math display">\[\begin{aligned}
\hat{y} = \underset{y}{\arg\max} \sum^{d}_{j=1} \log P_{X_j|Y}(x_j | y) + \log P_{Y}(y)
\end{aligned}\]</span>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/07/19/naive-bayes/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/19/logistic-regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/logistic-regression/" class="post-title-link" itemprop="url">Logistic Regression</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-19 16:32:07" itemprop="dateCreated datePublished" datetime="2021-07-19T16:32:07+08:00">2021-07-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-08-09 17:48:42" itemprop="dateModified" datetime="2021-08-09T17:48:42+08:00">2021-08-09</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/07/19/logistic-regression/" class="post-meta-item leancloud_visitors" data-flag-title="Logistic Regression" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>5.4k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>5 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="logistic-regression">Logistic Regression</h1>
<p>Suppose we have training examples <span class="math inline">\(D = \{(\mathbf{x}_1, y_1), ...., (\mathbf{x}_N, y_N); \; \mathbf{x}_i \in \mathbb{R}^d\}\)</span>, our goal is to make decision about the class of new input <span class="math inline">\(\mathbf{x}\)</span>. The logistic regression does this by learning from a training set, a vector of bias and a matrix of weights.</p>
<h2 id="binary-class">Binary-Class</h2>
<p>In binary class problem, our target <span class="math inline">\(Y\)</span> takes values <span class="math inline">\(\{0, 1\}\)</span>. To model the distribution <span class="math inline">\(P(Y | \mathbf{X}; \; \mathbf{w}, b)\)</span>, we apply sigmoid function on the dot product of weights and inputs which transform the output to a value between <span class="math inline">\([0, 1]\)</span> (one criteria for probability):</p>
<p><span class="math display">\[z = \mathbf{x}^T \mathbf{w} + b\]</span></p>
<p><span class="math display">\[y = \sigma(z)\]</span></p>
<p>To make sure that class random variable <span class="math inline">\(Y\)</span>'s conditional pmf sums to 1:</p>
<p><span class="math display">\[P(Y=1 | X=\mathbf{x} ;\; \mathbf{w}, b) = \frac{1}{1 + e^{-z}} = p\]</span></p>
<p><span class="math display">\[P(Y=0 | X=\mathbf{x} ;\; \mathbf{w}, b) = 1 - \frac{1}{1 + e^{-z}} = \frac{e^{-z}}{1 + e^{-z}} = 1 - p\]</span></p>
<p>Then, it is equivalently to express this conditional pmf as Bernoulli pmf:</p>
<p><span class="math display">\[p_{Y|\mathbf{X}} (y | \mathbf{x}; \; \mathbf{w}, b) = p^y + (1 - p)^{1 - y}\]</span></p>
<p>If we have the conditional pmf of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X= \mathbf{x}\)</span>, then we can use simple decision rule to make decisions:</p>
<p><span class="math display">\[
\hat{y} =
\begin{cases}
P(Y=1 | X=\mathbf{x}) &gt; 0.5, \quad 1\\
P(Y=1 | X=\mathbf{x}) \leq 0.5, \quad 0
\end{cases}
\]</span></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/07/19/logistic-regression/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>
<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhu, Zhaoyang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Symbols count total">799k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">12:07</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.3" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{&quot;enable&quot;:true,&quot;app_id&quot;:&quot;gPOc95wWW2Hwp2pkVSAAz28m-MdYXbMMI&quot;,&quot;app_key&quot;:&quot;7Sf65xCHXfEEdBvu29UHSYdV&quot;,&quot;server_url&quot;:&quot;https:&#x2F;&#x2F;swag1ong.github.io&quot;,&quot;security&quot;:true}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{&quot;enable&quot;:true,&quot;repo&quot;:&quot;swag1ong&#x2F;swag1ong.github.io&quot;,&quot;issue_term&quot;:&quot;pathname&quot;,&quot;theme&quot;:&quot;github-light&quot;,&quot;pathname&quot;:&quot;pathname&quot;}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
