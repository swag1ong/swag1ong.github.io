<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="1xbHmVEsS5Fta14x1d1FXkHFr93LRr1pMxipdYVcNI4">
  <meta name="baidu-site-verification" content="code-Srzz2vTuzA">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;swag1ong.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;Searching...&quot;,&quot;empty&quot;:&quot;We didn&#39;t find any results for the search: ${query}&quot;,&quot;hits_time&quot;:&quot;${hits} results found in ${time} ms&quot;,&quot;hits&quot;:&quot;${hits} results found&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:3,&quot;unescape&quot;:false,&quot;preload&quot;:true}}</script>
<meta property="og:type" content="website">
<meta property="og:title" content="GoGoGogo!">
<meta property="og:url" content="https://swag1ong.github.io/page/4/index.html">
<meta property="og:site_name" content="GoGoGogo!">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhu, Zhaoyang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://swag1ong.github.io/page/4/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:true,&quot;isPost&quot;:false,&quot;lang&quot;:&quot;en&quot;,&quot;comments&quot;:&quot;&quot;,&quot;permalink&quot;:&quot;&quot;,&quot;path&quot;:&quot;page&#x2F;4&#x2F;index.html&quot;,&quot;title&quot;:&quot;&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>GoGoGogo!</title><script src="/js/config.js"></script>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?852b005027c5efa81663f6f5c4c5b7fd"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">GoGoGogo!</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-igloo fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">110</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">16</span></a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">25</span></a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-address-card fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhu, Zhaoyang"
      src="/images/others/favicon.jpeg">
  <p class="site-author-name" itemprop="name">Zhu, Zhaoyang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">110</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/swag1ong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;swag1ong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyang.zhu@mail.utoronto.ca" title="E-Mail → mailto:zhaoyang.zhu@mail.utoronto.ca" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zhuzhaoyangzzyu0616" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zhuzhaoyangzzyu0616" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/09/09/graphical-models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/09/09/graphical-models/" class="post-title-link" itemprop="url">Graphical Models</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-09-09 13:17:03" itemprop="dateCreated datePublished" datetime="2021-09-09T13:17:03+08:00">2021-09-09</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2022-07-04 22:04:09" itemprop="dateModified" datetime="2022-07-04T22:04:09+08:00">2022-07-04</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/09/09/graphical-models/" class="post-meta-item leancloud_visitors" data-flag-title="Graphical Models" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>20k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>18 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="graphical-models">Graphical Models</h1>
<h2 id="conditional-independence">Conditional Independence</h2>
<h3 id="conditional-independence-of-random-variable">Conditional Independence of Random Variable</h3>
<p>Let <span class="math inline">\(\mathbf{X}, \mathbf{Y}, \mathbf{Z}\)</span> be a set of random variables. We say that <span class="math inline">\(\mathbf{X}\)</span> is conditionally independent of <span class="math inline">\(\mathbf{Y}\)</span> given <span class="math inline">\(\mathbf{Z}\)</span> in a distribution <span class="math inline">\(P\)</span> if <span class="math inline">\(P\)</span> satisfies <span class="math inline">\(P(\mathbf{X} = x \perp \mathbf{Y}=y | \mathbf{Z}=z)\)</span> for all values of <span class="math inline">\(x, y, z \in (Val(\mathbf{X}), Val(\mathbf{Y}), Val(\mathbf{Z}))\)</span>. If the set <span class="math inline">\(\mathbf{Z}\)</span> is empty, we write <span class="math inline">\((\mathbf{X} \perp \mathbf{Y})\)</span> and say that <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span> are marginally independent.</p>
<p>The distribution <span class="math inline">\(P\)</span> satisfies <span class="math inline">\((\mathbf{X} \perp \mathbf{Y} | \mathbf{Z})\)</span> IFF <span class="math inline">\(P(\mathbf{X}, \mathbf{Y} | \mathbf{Z}) = P(\mathbf{X}| \mathbf{Z}) P(\mathbf{Y} | \mathbf{Z})\)</span></p>
<ul>
<li><strong>Symmetry</strong>: <span class="math display">\[(\mathbf{X} \perp \mathbf{Y} | \mathbf{Z}) \implies (\mathbf{Y} \perp \mathbf{X} | \mathbf{Z})\]</span></li>
<li><strong>Decomposition</strong>: <span class="math display">\[(\mathbf{X} \perp \mathbf{Y}, \mathbf{W} | \mathbf{Z}) \implies (\mathbf{X} \perp \mathbf{Y} | \mathbf{Z}), \;(\mathbf{X} \perp \mathbf{W} | \mathbf{Z})\]</span></li>
<li><strong>Weak Union</strong>: <span class="math display">\[(\mathbf{X} \perp \mathbf{Y}, \mathbf{W} | \mathbf{Z}) \implies (\mathbf{X} \perp \mathbf{Y} | \mathbf{Z}, \mathbf{W}), \; (\mathbf{X} \perp \mathbf{W} | \mathbf{Z}, \mathbf{Y})\]</span></li>
<li><strong>Contraction</strong>: <span class="math display">\[(\mathbf{X} \perp \mathbf{W} | \mathbf{Z}, \mathbf{Y}) \cap (\mathbf{X} \perp \mathbf{Y} | \mathbf{Z}) \implies (\mathbf{X} \perp \mathbf{Y}, \mathbf{W}| \mathbf{Z})\]</span></li>
</ul>
<p>We can represent complicated probabilistic models using diagrammatic representations of probability distributions called <code>probabilistic graphical models</code>. These offer several useful properties:</p>
<ol type="1">
<li>They provide a simple way to visualize the structure of a probabilistic model and can be used to design and motivate new models.</li>
<li>Insights into the properties of the model, including conditional independence properties can be obtained by inspection of the graph.</li>
<li>Complex computations, required to perform inference and learning in sophisticated models, can be expressed in terms of graphical manipulations, in which underlying mathematical expressions are carried along implicitly.</li>
</ol>
<p><br></p>
<p>A probabilistic graphical model consists of:</p>
<ol type="1">
<li><strong>Nodes</strong>: each random variable (or group of random variables) is represented as a node in the graph</li>
<li><strong>Edges (links)</strong>: links express probabilistic relationship between these random variables, the edges encode our intuition about the way the world works.
<ul>
<li><strong>Directed graphical models</strong>: in which the edges of the graphs have a particular directionality indicated by arrows (Bayesian networks). Directed graphs are useful for expressing causal relationships between random variables.</li>
<li><strong>Undirected graphical models</strong>: in which the edges of the graph do not carry arrows and have no directional significance (Markov random fields). Undirected graphs are better suited to expressing soft constraints between random variables.</li>
</ul></li>
</ol>
<p>The graph then captures the way in which the joint distribution over all of the random variables can be decomposed into a product of factors each depending only on a subset of the variables.</p>
<h2 id="bayesian-networks">Bayesian Networks</h2>
<p>The DAG of random variables can be viewed in two very different ways (Also strongly equivalent):</p>
<ul>
<li>As a data structure that provides the skeleton for representing a joint distribution compactly in a factorized way.</li>
<li>As a compact representation for a set of conditional independence assumptions about a distribution.</li>
</ul>
<blockquote>
<p><strong>We can view the graph as encoding a generative sampling process executed by nature, where the value for each variable is selected by nature using a distribution that depends only on its parents. In other words, each variable is a stochastic function of its parents.</strong></p>
</blockquote>
<blockquote>
<p><strong>In general, there are many weak influences that we might choose to model, but if we put in all of them, the network can become very complex, such networks are problematic from a representational perspective.</strong></p>
</blockquote>
<h3 id="bayesian-network-represents-a-joint-distribution-compactly-in-a-factorized-way">Bayesian Network Represents a Joint Distribution Compactly in a Factorized Way</h3>
<p>Consider first an arbitrary joint distribution defined by <span class="math inline">\(P(\mathbf{Z})\)</span> over random vector <span class="math inline">\(\mathbf{Z} = &lt;A, B, C&gt;\)</span>, by product rule, we have:</p>
<p><span class="math display">\[P(\mathbf{Z}) = P(C| A, B) P(A, B) = P(C | A, B) P(B | A) P(A)\]</span></p>
<p>We now represent the right-hand side in terms of a simgple graphical model as follows:</p>
<ol type="1">
<li>First, we introduce a node for each of the random variables <span class="math inline">\(A, B, C\)</span> and associate each node with the corresponding conditional distribution on the right-hand side.</li>
<li>Then, for each conditional distribution we add directed links to the graph from the nodes to the variables on which the distribution is conditioned.</li>
</ol>
<p><img src='/images/ML/gm_1.png' width="600"></p>
<p>If there is a link going from a node <span class="math inline">\(A\)</span> to a node <span class="math inline">\(B\)</span>, then we say that node <span class="math inline">\(A\)</span> is parent of node <span class="math inline">\(B\)</span> and <span class="math inline">\(B\)</span> is the child of node <span class="math inline">\(A\)</span> (change ordering of the decomposition will change the graph).</p>
<p>We can extend the idea to joint distribution of <span class="math inline">\(K\)</span> random variables given by <span class="math inline">\(P(X_1, ...., X_K)\)</span>. By repeated application of the product rule of the probability, this joint distribution can be written as a product of conditional distributions:</p>
<p><span class="math display">\[P(X_1, ...., X_K) = P(X_K | X_{K-1}, ..., X_{1}) ... P(X_2 | X_1) P(X_1)\]</span></p>
<p>We can generate a graph similar to three-variable case, each node having incoming links from all lower numbered nodes. We say this graph is <code>fully connected</code> because there is a link between every pair of nodes. However, it is the <strong>absence</strong> (not fully connected) of links in the graph that conveys interesting information about the properties of the class of distributions that the graph represents.</p>
<p><img src='/images/ML/gm_2.png' width="600"></p>
<p><br></p>
<p>We can now state in general terms the relationship between a given directed graph and the corresponding distribution over the variables. Thus, for a graph with K nodes <span class="math inline">\(\mathbf{X} = &lt;X_1, ...., X_K&gt;\)</span>, the joint distribution is given by:</p>
<p><span class="math display">\[P(\mathbf{X}) = \prod^K_{k=1} P(X_k | \text{Parent}(X_k))\]</span></p>
<p>Where <span class="math inline">\(\text{Parent}(X_k)\)</span> denotes the set of parents of <span class="math inline">\(X_k\)</span>.</p>
<p>Notice that, the directed graphs that we are considering are subject to an important restriction namely that there must be <strong>no</strong> directed cycles, that is, we are working with <code>directed acyclic graphs</code> or DAGs.</p>
<h4 id="example-generative-models">Example: Generative Models</h4>
<p>There are many situations in which we wish to draw samples from a given probability distribution. One technique which is particularly relevant to graphical models is called <code>ancestral sampling</code>.</p>
<p>Consider a joint distribution <span class="math inline">\(P(\mathbf{X}), \mathbf{X} = &lt;X_1, ...., X_K&gt;\)</span> that factorizes into a DAG. We shall suppose that the variables have been ordered from <span class="math inline">\(X_1\)</span> to <span class="math inline">\(X_K\)</span>, in other words each node has a higher index than any of its parents. Our goal is to draw samples <span class="math inline">\(\hat{X}_1, ..., \hat{X}_K\)</span> from the joint distribution.</p>
<p>To do this, we start from <span class="math inline">\(X_1\)</span>, and draw sample <span class="math inline">\(\hat{X}_1\)</span> from the distribution <span class="math inline">\(P(X_1)\)</span>. We then work through each of the nodes in order, so that for node <span class="math inline">\(n\)</span> we draw a sample from the conditional distribution <span class="math inline">\(P(X_n | \text{Parent}(X_n))\)</span>, in which the parent variables have been set to their sampled values.</p>
<p>To obtain a sample from some marginal distribution corresponding to a subset of the random variables, we simply take the sampled values for the required nodes and discard the rest. For example, to draw a sample from the distribution <span class="math inline">\(P(X_2, X_4)\)</span>, we simply sample from the full joint distribution and then retain the values <span class="math inline">\(\hat{X}_2, \hat{X}_4\)</span> and discard the remaining values.</p>
<p>For practical applications of probabilistic models, it will typically be the higher-numbered variables corresponding to terminal nodes of the graph that represent the observations, with lower-numbered nodes corresponding to latent variables. The primary role of the latent variables is to allow a complicated distribution over the observed variables to tbe represented in terms of a model constructed from simpler conditional distributions.</p>
<blockquote>
<blockquote>
<p>Consider an object recognition task in which each observed data point corresponds to an image of on of the objects (vector of pixels). In this case, we can have latent variables be position and orientation of the object. Given a particular observed image, our goal is to find the posterior distribution over objects in which we integrate over all possible positions and orientations. <img src='/images/ML/gm_3.png' width="600"> Given object, position, orientation, we can sample from the conditional distribution of image and generate pixels.</p>
</blockquote>
</blockquote>
<p>The graphical model captures causal process by which the observed data was generated. For this reason, such models are often called <code>generative models</code>.</p>
<p><br></p>
<h3 id="independence-in-bayesian-networks">Independence in Bayesian Networks</h3>
<p><strong>Our intuition tells us that the parents of a variable “shield” it from probabilistic influence that is causal in nature. In other words, once I know the value of the parents, no information relating directly or indirectly to its parents or other ancestors can influence my beliefs about it. However, information about its descendants can change my beliefs about it, via an evidential reasoning process.</strong></p>
<h4 id="bayesian-network-semantics">Bayesian Network Semantics</h4>
<h5 id="definition-3.1-bayesian-network-structure">Definition 3.1: Bayesian Network Structure</h5>
<p>A Bayesian network structure <span class="math inline">\(G\)</span> is a directed acyclic graph whose nodes represent random variables <span class="math inline">\(X_1, ...., X_n\)</span>. Let <span class="math inline">\(Pa^{G}_{X_i}\)</span> denote the parents of <span class="math inline">\(X_i\)</span> in <span class="math inline">\(G\)</span>, and <span class="math inline">\(\text{NonDescendants}_{X_i}\)</span> denote the variables in the graph that are not descendants of <span class="math inline">\(X_i\)</span>. Then <span class="math inline">\(G\)</span> encodes the following set of <strong>conditional independence assumptions</strong>, called <strong>local independence</strong>, and denoted by <span class="math inline">\(I_l (G)\)</span>:</p>
<p><span class="math display">\[I_l (G) = \{\text{For each variable $X_i$: ($X_i \perp \text{NonDescendants}_{X_i} | Pa^G_{X_i}$})\}\]</span></p>
<p>In other words, the local independence state that each node <span class="math inline">\(X_i\)</span> is conditionally independent of its non-descendants given its parents.</p>
<p><br></p>
<h3 id="graphs-and-distributions">Graphs and Distributions</h3>
<p>We now show that the previous two definitions of BN are equivalent. That is, <strong>a distribution <span class="math inline">\(P\)</span> satisfies the local independence associated with a graph <span class="math inline">\(G\)</span> IFF <span class="math inline">\(P\)</span> is representable as a set of CPDs associated with the graph <span class="math inline">\(G\)</span>.</strong></p>
<h4 id="i-maps">I-Maps</h4>
<h5 id="definition-3.2-independencies-in-p">Definition 3.2: Independencies in <span class="math inline">\(P\)</span></h5>
<p>Let <span class="math inline">\(P\)</span> be a distribution over <span class="math inline">\(\mathbf{X}\)</span>. We define <span class="math inline">\(I(P)\)</span> to be the set of independence assertions of the form <span class="math inline">\((\mathbf{X} \perp \mathbf{Y} | \mathbf{Z})\)</span></p>
<p><br></p>
<h5 id="definition-3.3-i-map">Definition 3.3: I-Map</h5>
<p>Let <span class="math inline">\(K\)</span> be any graph object associated with a set of independencies <span class="math inline">\(I(K)\)</span>. We say that <span class="math inline">\(K\)</span> is an I-map for a set of independencies <span class="math inline">\(I\)</span> if <span class="math inline">\(I(K) \subseteq I\)</span>.</p>
<p>We now say that <span class="math inline">\(G\)</span> is a I-map for <span class="math inline">\(P\)</span> if <span class="math inline">\(G\)</span> is an I-map for <span class="math inline">\(I(P)\)</span>. That is <span class="math inline">\(I(G) \subseteq I(P)\)</span>.</p>
<p><br></p>
<p>That is, for <span class="math inline">\(G\)</span> to be an I-map of <span class="math inline">\(P\)</span>, it is necessary that <span class="math inline">\(G\)</span> does not mis-lead us regarding independencies in <span class="math inline">\(P\)</span>: any dependence that <span class="math inline">\(G\)</span> asserts must also hold in <span class="math inline">\(P\)</span>. Conversely, <span class="math inline">\(P\)</span> may have additional independencies that are not reflected in <span class="math inline">\(G\)</span>.</p>
<p><br></p>
<h4 id="i-map-to-factorization">I-Map to Factorization</h4>
<p>A BN structure <span class="math inline">\(G\)</span> encodes a set of conditional independence assumptions, every distribution for which <span class="math inline">\(G\)</span> is an I-map must satisfy these assumptions.</p>
<h5 id="definition-3.4-factorization-chain-rule-of-bayesian-network">Definition 3.4: Factorization (Chain Rule of Bayesian Network)</h5>
<p>Let <span class="math inline">\(G\)</span> be a BN graph over the variables <span class="math inline">\(X_1, ...., X_n\)</span>. We say that a distribution <span class="math inline">\(P\)</span> over the same space factorizes according to <span class="math inline">\(G\)</span> if <span class="math inline">\(P\)</span> can be expressed as a product:</p>
<p><span class="math display">\[P(X_1, ...., X_n) = \prod^n_{i=1} P(X_i | Pa^G_{X_i})\]</span></p>
<p><br></p>
<h5 id="definition-3.5-bayesian-network">Definition 3.5: Bayesian Network</h5>
<p>A Bayesian network is a pair <span class="math inline">\(B = (G, P)\)</span> where <span class="math inline">\(P\)</span> factorizes over <span class="math inline">\(G\)</span>, and where <span class="math inline">\(P\)</span> is specified as a set of CPDs associated with <span class="math inline">\(G\)</span>'s nodes. The distribution <span class="math inline">\(P\)</span> is often annotated <span class="math inline">\(P_B\)</span>.</p>
<p><br></p>
<h5 id="theorem-3.1-i-map-factorization">Theorem 3.1: I-Map Factorization</h5>
<p>Let <span class="math inline">\(G\)</span> be a BN structure over a set of random variables <span class="math inline">\(\mathbb{X}\)</span>, and let <span class="math inline">\(P\)</span> be a joint distribution over the same space. If <span class="math inline">\(G\)</span> is a I-map for <span class="math inline">\(P\)</span>, then <span class="math inline">\(P\)</span> factorizes according to <span class="math inline">\(G\)</span> (can be written in the form as in definition 3.4).</p>
<p><br></p>
<h4 id="factorization-to-i-map">Factorization to I-Map</h4>
<h5 id="theorem-3.2-factorization-to-i-map">Theorem 3.2: Factorization to I-Map</h5>
<p>Let <span class="math inline">\(G\)</span> be a BN structure over a set of random variables <span class="math inline">\(\mathbb{X}\)</span> and let <span class="math inline">\(P\)</span> be a joint distribution over the same space. If <span class="math inline">\(P\)</span> factorizes according to <span class="math inline">\(G\)</span>, then <span class="math inline">\(G\)</span> is an I-map for <span class="math inline">\(P\)</span>.</p>
<p><br></p>
<h3 id="independencies-in-graphs">Independencies in Graphs</h3>
<p>Knowing only that a distribution <span class="math inline">\(P\)</span> factorizes over <span class="math inline">\(G\)</span>, we can conclude that it satisfies <span class="math inline">\(I_l (G)\)</span>. Are there other independencies that hold for every distribution <span class="math inline">\(P\)</span> that factorizes over <span class="math inline">\(G\)</span>? Our goal is to understand when we can guarantee that an independence <span class="math inline">\((\mathbf{X} \perp \mathbf{Y} | \mathbf{Z})\)</span> holds in a distribution associated with a BN structure <span class="math inline">\(G\)</span>.</p>
<h4 id="d-separation">D-Separation</h4>
<p>When influence can flow from <span class="math inline">\(X, Y\)</span> via <span class="math inline">\(Z\)</span> (<span class="math inline">\(X, Y\)</span> are correlated), we say that the trail is <strong>active</strong></p>
<ul>
<li><strong>Direct connection</strong>: When <span class="math inline">\(X, Y\)</span> are directly connected via edge. For any network structure <span class="math inline">\(G\)</span> they are <strong>always</strong> correlated regardless of any evidence about any of the other variables in the network.</li>
<li><strong>Indirect connection</strong>: <span class="math inline">\(X, Y\)</span> are not directly connected via edge, but there is a trail between then in the graph via <span class="math inline">\(Z\)</span>.
<ul>
<li><strong>Indirect causal effect</strong>: <span class="math inline">\(X \rightarrow Z \rightarrow Y\)</span>. If we observe <span class="math inline">\(Z\)</span>, then <span class="math inline">\(X, Y\)</span> are conditionally independent, if <span class="math inline">\(Z\)</span> is not observed, <span class="math inline">\(X\)</span> influences <span class="math inline">\(Y\)</span> by first sampling from <span class="math inline">\(P(X)\)</span> then sampling <span class="math inline">\(Z\)</span> from <span class="math inline">\(P(Z | X)\)</span>, so <span class="math inline">\(X, Y\)</span> are not independent. (Active IFF <span class="math inline">\(Z\)</span> is <strong>not</strong> observed)</li>
<li><strong>Indirect evidential effect</strong>: <span class="math inline">\(Y \rightarrow Z \rightarrow X\)</span>, same as indirect causal effect. (Active IFF <span class="math inline">\(Z\)</span> is <strong>not</strong> observed)</li>
<li><strong>Common cause</strong>: <span class="math inline">\(X \leftarrow Z \rightarrow Y\)</span>, same as above, <span class="math inline">\(X\)</span> can influence <span class="math inline">\(Y\)</span> via <span class="math inline">\(Z\)</span> IFF <span class="math inline">\(Z\)</span> is not observed. (Active IFF <span class="math inline">\(Z\)</span> is <strong>not</strong> observed)</li>
<li><strong>Common effect</strong>: <span class="math inline">\(X \rightarrow Z \leftarrow Y\)</span>, if <span class="math inline">\(Z\)</span> is unobserved, then <span class="math inline">\(X, Y\)</span> are independent, if it is observed then they are not. (Active IFF <span class="math inline">\(Z\)</span> or one of <span class="math inline">\(Z\)</span>'s descendants is <strong>observed</strong>)</li>
</ul></li>
</ul>
<p><img src='/images/RL/background/pgm_1.png' width="600"></p>
<p><br></p>
<h5 id="definition-3.6-general-case-one-path-from-x_1-to-x_n">Definition 3.6: General Case (One path from <span class="math inline">\(X_1\)</span> to <span class="math inline">\(X_n\)</span>)</h5>
<p>Let <span class="math inline">\(G\)</span> be a BN structure, and <span class="math inline">\(X_1 \rightleftharpoons .... \rightleftharpoons X_n\)</span> a trail in <span class="math inline">\(G\)</span>. Let <span class="math inline">\(\mathbf{Z}\)</span> be a subset of observed variables. The trail <span class="math inline">\(X_1 \rightleftharpoons .... \rightleftharpoons X_n\)</span> is <strong>active</strong> given <span class="math inline">\(\mathbf{Z}\)</span> if:</p>
<ul>
<li>Whenever we have a <span class="math inline">\(v\)</span>-structure (Common effect), then <span class="math inline">\(X_i\)</span> or one of its descendants are in <span class="math inline">\(\mathbf{Z}\)</span>.</li>
<li>No other node along the trail is in <span class="math inline">\(\mathbf{Z}\)</span>.</li>
</ul>
<p>Note if <span class="math inline">\(X_1\)</span> or <span class="math inline">\(X_n\)</span> are in <span class="math inline">\(\mathbf{Z}\)</span>, the trail is not active.</p>
<p><br></p>
<h5 id="definition-3.7-d-seperation">Definition 3.7: D-seperation</h5>
<p>Let <span class="math inline">\(\mathbf{X}, \mathbf{Y}, \mathbf{Z}\)</span> be three sets of nodes in <span class="math inline">\(G\)</span>. We say that <span class="math inline">\(\mathbf{X}, \mathbf{Y}\)</span> are <strong>d-seperated</strong> given <span class="math inline">\(\mathbf{Z}\)</span>, denoted <span class="math inline">\(d-sep_G(\mathbf{X}; \mathbf{Y} | \mathbf{Z})\)</span>, if there is <strong>no</strong> active trail between any node <span class="math inline">\(X \in \mathbf{Z}\)</span> and <span class="math inline">\(Y \in \mathbf{Y}\)</span> given <span class="math inline">\(\mathbf{Z}\)</span>. We use <span class="math inline">\(I(G)\)</span> to denote the set of independencies that correspond to d-separation:</p>
<p><span class="math display">\[I(G) = \{(\mathbf{X} \perp \mathbf{Y} | \mathbf{Z}) : d-sep_{G} (\mathbf{X}; \mathbf{Y} | \mathbf{Z})\}\]</span></p>
<p>The independencies in <span class="math inline">\(I(G)\)</span> are precisely those that are guaranteed to hold for every distribution over <span class="math inline">\(G\)</span>.</p>
<p><br></p>
<h4 id="soundness-and-completeness">Soundness and Completeness</h4>
<h5 id="theorem-3.3-soundness">Theorem 3.3: Soundness</h5>
<p>If a distribution <span class="math inline">\(P\)</span> factorizes according to <span class="math inline">\(G\)</span>, then <span class="math inline">\(I(G) \subseteq I(P)\)</span>.</p>
<p>In other words, any independence reported by d-separation is satisfied by the underlying distribution.</p>
<p><br></p>
<h5 id="definition-3.8-faithful">Definition 3.8: Faithful</h5>
<p>A distribution <span class="math inline">\(P\)</span> is faithful to <span class="math inline">\(G\)</span> if, whenever <span class="math inline">\((X \perp Y | \mathbf{Z}) \in I(P)\)</span>, then <span class="math inline">\(d-sep_G(X; Y | \mathbf{Z})\)</span>.</p>
<p>In other words, any independence in <span class="math inline">\(P\)</span> is reflected in the d-separation properties of the graph.</p>
<p><br></p>
<h5 id="theorem-3.4-completeness">Theorem 3.4: Completeness</h5>
<p>Let <span class="math inline">\(G\)</span> be a BN structure. If <span class="math inline">\(X, Y\)</span> are not d-separated given <span class="math inline">\(\mathbf{Z}\)</span> in <span class="math inline">\(G\)</span>, then <span class="math inline">\(X, Y\)</span> are dependent given <span class="math inline">\(\mathbf{Z}\)</span> in some distribution <span class="math inline">\(P\)</span> that factorizes over <span class="math inline">\(G\)</span>.</p>
<p><br></p>
<p><strong>THese results state that for almost all parameterizations <span class="math inline">\(P\)</span> of the graph <span class="math inline">\(G\)</span>, the d-separation test precisely characterizes the independencies that hold for <span class="math inline">\(P\)</span>.</strong></p>
<h2 id="conditional-independence-1">Conditional Independence</h2>
<p>An important concept for probability distributions over multiple variables is that of <strong>conditional independence</strong>. Consider three random variables <span class="math inline">\(A, B, C\)</span> and suppose that the conditional distribution of <span class="math inline">\(A\)</span>, given <span class="math inline">\(B, C\)</span> is such that it does not depend on the value of <span class="math inline">\(B\)</span>, so that:</p>
<p><span class="math display">\[P(A | B, C) = P(A | C)\]</span></p>
<p>Then:</p>
<p><span class="math display">\[P(A, B | C) = P(A | B, C) P(B | C) = P(A | C) P (B | C)\]</span></p>
<p>Thus, we can see that <span class="math inline">\(A, B\)</span> are statistically independent given <span class="math inline">\(C, \; \forall C\)</span>. Note that this definition of conditional independence will require the above equation holds for all values fo <span class="math inline">\(C\)</span> and not just for some values. The shorthand notation for conditional independence is:</p>
<p><span class="math display">\[A \perp \!\!\! \perp B \;|\; C\]</span></p>
<p>An important and elegant feature of graphical models is that conditional independence properties of the joint distribution can be read directly from the graph without having to perform any analytical manipulations. The general framework for achieving this is called <code>d-seperation</code> (d stands for directed).</p>
<h3 id="three-example-graphs">Three example graphs</h3>
<p>We start by illustrating the key concepts of d-separation by three motivating examples.</p>
<ol type="1">
<li><blockquote>
<p><span class="math inline">\(P(A, B, C) = P(A | C) P(B | C) P(C)\)</span> <img src='/images/ML/gm_4.png' width="600"> <span class="math inline">\(A, B\)</span> are generally <strong>not</strong> statistically independent. However, we can easily see that <span class="math inline">\(A, B\)</span> are conditionally independent given <span class="math inline">\(C\)</span>: <span class="math display">\[P(A, B | C) = \frac{P(A, B, C)}{P(C)} = P(A | C) P (B | C)\]</span> <img src='/images/ML/gm_5.png' width="600"></p>
</blockquote></li>
</ol>
<p>We can provide a simple graphical interpretation of this result by considering the path from node <span class="math inline">\(A\)</span> to node <span class="math inline">\(B\)</span> via <span class="math inline">\(C\)</span>. The node <span class="math inline">\(C\)</span> is said to be <strong>tail-to-tail</strong> with respect to this path because the node is connected to the tails of the two arrows. However, when we condition on node <span class="math inline">\(C\)</span> (observed <span class="math inline">\(C\)</span>), the conditional node blocks the path from <span class="math inline">\(A\)</span> to <span class="math inline">\(B\)</span> so causes then to become conditionally independent.</p>
<ol start="2" type="1">
<li><blockquote>
<p><span class="math inline">\(P(A, B, C) = P(B | C) P(C | A) P (A)\)</span> <img src='/images/ML/gm_6.png' width="600"> <span class="math inline">\(A, B\)</span> are generally <strong>not</strong> statistically independent. However, we can easily see that <span class="math inline">\(A, B\)</span> are conditionally independent given <span class="math inline">\(C\)</span> by: <span class="math display">\[P(A, B | C) = \frac{P(A, B, C)}{P(C)} = P(A | C) P (B | C)\]</span></p>
</blockquote></li>
</ol>
<p>We can provide a simple graphical interpretation of this result by considering the path from node <span class="math inline">\(A\)</span> to node <span class="math inline">\(B\)</span> via <span class="math inline">\(C\)</span>. The node <span class="math inline">\(C\)</span> is said to be <strong>head-to-tail</strong> with respect to this path because the node is connected to the head and tail of the two arrows. However, when we condition on node <span class="math inline">\(C\)</span> (observed <span class="math inline">\(C\)</span>), the conditional node blocks the path from <span class="math inline">\(A\)</span> to <span class="math inline">\(B\)</span> so causes then to become conditionally independent.</p>
<ol start="3" type="1">
<li><blockquote>
<p><span class="math inline">\(P(A, B, C) = P(A)P(B)P(C | A, B)\)</span> <img src='/images/ML/gm_7.png' width="600"> We can easily see that <span class="math inline">\(A, B\)</span> are <strong>not</strong> conditionally independent. However, we can see that <span class="math inline">\(A, B\)</span> are statistically independent: <span class="math inline">\(P(A, B) = \sum_{C} P(A)P(B)P(C | A, B) = P(A)P(B)\)</span></p>
</blockquote></li>
</ol>
<p>Thus, our third example has the opposite behaviour from the first two. The node <span class="math inline">\(C\)</span> is said to be <strong>head-to-head</strong> with respect to this path because the node is connected to the heads of the two arrows. When the node <span class="math inline">\(C\)</span> is not given (unobserved), it blocks the path so <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is independent, however, when the node <span class="math inline">\(C\)</span> is given, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> becomes dependent.</p>
<p>There is one more relationship associate with third example. First we say that node <span class="math inline">\(Y\)</span> is a <strong>descendant</strong> of node <span class="math inline">\(X\)</span> if there is a path from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span> in which each step of the path follows the directions of the arrows. Then it can be shown that a <strong>head to head</strong> path will become unblocked if either the node or any of its descendants is observed.</p>
<h3 id="d-separation-1">D-separation</h3>
<p>Consider a general directed graph in which <span class="math inline">\(A, B, C\)</span> are arbitrary sets of nodes. We wish to ascertain whether a particular conditional independence statement <span class="math inline">\(A \perp \!\!\! \perp B \;|\; C\)</span> is implied by a given directed acyclic graph. To do so, we consider all possible paths from any node in <span class="math inline">\(A\)</span> to any node in <span class="math inline">\(B\)</span>. Any such path is said to be <strong>blocked</strong> if it includes a node such that either:</p>
<ol type="1">
<li>The arrows on the path meet either head-to-tail or tail-to-tail at the node, and the node is in the set <span class="math inline">\(C\)</span>.</li>
<li>The arrows meet head-to-head at the node, and neither the node, nor any of its descendants, is in the set <span class="math inline">\(C\)</span>.</li>
</ol>
<p>If <strong>all</strong> paths are blocked, then <span class="math inline">\(A\)</span> is said to be <code>d-separated</code> from <span class="math inline">\(B\)</span> by <span class="math inline">\(C\)</span>, and the joint distribution over all of the variables in the graph will satisfy <span class="math inline">\(A \perp \!\!\! \perp B \;|\; C\)</span>.</p>
<blockquote>
<p>Consider the problem of finding the posterior distribution for the mean of an univariate Gaussian distribution. This can be represented by the directed graph in which the joint distribution is defined by a prior <span class="math inline">\(P(\mu)\)</span> and <span class="math inline">\(P(\mathbf{X} | \mu)\)</span> to form the posterior distribution: <span class="math display">\[P(\mu | \mathbf{X}) = P(\mu) P(\mathbf{X} | \mu) \]</span> <img src='/images/ML/gm_8.png' width="600"> In practice, we observe <span class="math inline">\(D = \{X_1, ...., X_N\}\)</span> with conditional distribution <span class="math inline">\(P(X_1 | \mu) , ...., P(X_N | \mu)\)</span> respectively, and our goal is to infer <span class="math inline">\(\mu\)</span>. Using d-separation, we note that there is a unique path from any <span class="math inline">\(X_i\)</span> to any other <span class="math inline">\(X_{j\neq i}\)</span> and that this path is tail-to-tail with respect to the observed node <span class="math inline">\(\mu\)</span>. Every such path is blocked and so the observations <span class="math inline">\(D=\{X_1, ..., X_N\}\)</span> are independent given <span class="math inline">\(\mu\)</span>: <span class="math display">\[P(\mathbf{X} | \mu) = \prod^N_{i=1} P(X_i | \mu)\]</span> However, if we do not conditional on <span class="math inline">\(\mu\)</span>, the data samples are not independent: <span class="math display">\[P(\mathbf{X}) = \int_{\mu} P(\mathbf{X} | \mu) P(\mu) \neq \prod^N_{i=1} P(X_i)\]</span></p>
</blockquote>
<h2 id="markov-random-fields">Markov Random Fields</h2>
<p>Directed Graphical models specify a factorization of the joint distribution over a set of variables into a product of local conditional distributions. They also defined a set of conditional independence properties that must be satisfied by any distribution that factorizes according to the graph. A <code>Markove random field</code> has:</p>
<ol type="1">
<li>A set of nodes each of which corresponds to a random variable or group of random variables</li>
<li>A set of links each of which connects a pair of nodes. The links are <strong>undirected</strong> that is they do not carry arrows.</li>
</ol>
<h3 id="conditional-independence-properties">Conditional Independence Properties</h3>
<p>Testing for conditional independence in undirected graph is simpler than in directed graph. Let <span class="math inline">\(A, B, C\)</span> be three sets of nodes and we consider the conditional independence property <span class="math inline">\(A \perp \!\!\! \perp B \;|\; C\)</span>. To test whether this property is satisfied by a probability distribution defined by the graph:</p>
<ul>
<li>Consider all possible paths that connect nodes in set <span class="math inline">\(A\)</span> to nodes in set <span class="math inline">\(B\)</span>. If all such paths pass through one or more nodes in set <span class="math inline">\(C\)</span>, then <strong>all</strong> such paths are <strong>blocked</strong> and so the conditional independence properties holds. If there is <strong>at least one</strong> such path that is not blocked, then there will exist at least some distributions corresponding to the graph that do not satisfy this conditional independence relation.</li>
</ul>
<p><img src='/images/ML/gm_9.png' width="600"></p>
<h3 id="factorization-properties">Factorization Properties</h3>
<p>We now express the joint distribution <span class="math inline">\(P(\mathbf{X})\)</span> as a product of functions defined over set of random variables that are local to the graph.</p>
<p>If we consider two nodes <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> that are not connected by a link, then these variables must be conditionally independent given all other nodes in the graph, because there is no direct path between the two nodes and all other paths are blocked:</p>
<p><span class="math display">\[P(X_i, X_j | \mathbf{X}_{k\notin \{i, j\}}) = P(X_i | \mathbf{X}_{k\notin \{i, j\}}) P(X_j | \mathbf{X}_{k\notin \{i, j\}})\]</span></p>
<p><br></p>
<p>A <code>clique</code> is a subset of nodes in a graph such that there exists a link between all pairs of nodes in the subset. In other words, the nodes in the set are fully connected. Furthermore, a <code>maximal clique</code> is a clique such that it is not possible to include any other nodes from the graph in the set without it ceasing to be a clique.</p>
<p><img src='/images/ML/gm_10.png' width="600"></p>
<p><br></p>
<p>We can therefore define the factors in the decomposition of the joint distribution to be functions of the variables in the cliques. In fact, we can consider functions of the maximal cliques, without loss of generality because other cliques must be subsets of maximal cliques.</p>
<p>Let <span class="math inline">\(C\)</span> be a clique and the set of random variables in that clique by <span class="math inline">\(\mathbf{X}_C\)</span>. Then the joint distribution is written as a product of <code>potential functions</code> <span class="math inline">\(\psi_C(\mathbf{x}_C) \geq 0\)</span> over the maximal cliques of the graph:</p>
<p><span class="math display">\[P(\mathbf{X}) = \frac{1}{Z} \prod_{C} \psi_{C} (\mathbf{X}_C)\]</span></p>
<p>Here the quantity <span class="math inline">\(Z\)</span> is called <code>partition function</code> which is used for normalization to ensure the result is a proper joint distribution:</p>
<p><span class="math display">\[Z = \sum_{X} \prod_{C} \psi_{C} (\mathbf{X}_C)\]</span></p>
<p>In directed graph, we have the links to be conditional distribution, in undirected graph, we do not restrict the choice of potential functions.</p>
<h1 id="ref">Ref</h1>
<p>PRML chapter 8</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/09/06/lgb/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/09/06/lgb/" class="post-title-link" itemprop="url">LGBM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-09-06 14:37:43" itemprop="dateCreated datePublished" datetime="2021-09-06T14:37:43+08:00">2021-09-06</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-09-08 20:48:22" itemprop="dateModified" datetime="2021-09-08T20:48:22+08:00">2021-09-08</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/09/06/lgb/" class="post-meta-item leancloud_visitors" data-flag-title="LGBM" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>4.6k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>4 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="lightgbm-a-highly-efficient-gradient-boosting-decision-tree">LightGBM: A Highly Efficient Gradient Boosting Decision Tree</h1>
<h2 id="background">Background</h2>
<h3 id="gbdt">GBDT</h3>
<p>GBDT is an ensemble model of decision trees, which are trained in sequence. In each iteration, GBDT learns the decision trees by fitting the negative gradients (also known as residual errors).</p>
<p>The main cost in GBDT lies:</p>
<ol type="1">
<li>Learning the decision trees (<strong>finding the best split points</strong>) when there is large number of features and samples.
<ul>
<li><strong>Histogram based algorithm</strong> buckets continuous feature values into discrete bins and uses these bins to construct feature histograms during training. It finds the best split points based on the feature histogram, the criterion is calculated at the interval boundaries. It costs <span class="math inline">\(O(N \times M)\)</span> for histogram building and <span class="math inline">\(O(\text{Number of bins} \times M)\)</span> for split point finding. Since number of bins is much smaller than number of data points, histogram building will dominate the computational complexity. <img src='/images/ML/lgb_1.png' width="600"> <img src='/images/ML/lgb_2.png' width="600"></li>
<li><strong>Pre-sorted algorithm</strong> sorts the values of each numeric attribute, and evaluates the criterion at each possible split point to find the splitting point with the minimum criterion. The sorting requires <span class="math inline">\(O(n\log (n))\)</span>.</li>
</ul></li>
<li><strong>Number of Samples</strong>:
<ul>
<li>Down sampling the data instance (i.e weights, random subsets), most of the algorithms are based on adaboost which has weights but not GBDT which does not have weights natively. Random subsets hurt the performance.</li>
</ul></li>
<li><strong>Number of Features</strong>:
<ul>
<li>PCA to remove weak correlated features (depends on assumption that features contain significant redundancy which might not always be true in practice)</li>
</ul></li>
</ol>
<h3 id="gradient-based-one-side-sampling">Gradient-based One-Side Sampling</h3>
<p>In AdaBoost, the sample weight serves as a good indicator for the importance of data instances. In GBDT, gradient for each data instance provides us with useful information for data sampling. That is, if an instance is associated with a small gradient, the training error for this instance is small and it is already well-trained. A straightforward idea is to discard those data instances with small gradients. However, <strong>the data distribution will be changed by doing so</strong>. To avoid this problem, GOSS keeps all the instances with large gradients and performs random sampling on the instances with samll gradients.</p>
<p>In order to compensate the influence to the data distribution, when computing the information gain, GOSS introduces a constant multiplier for the data instances with samll gradients. Specifically, GOSS:</p>
<ol type="1">
<li>Firstly sorts the data instances according to the absolute value of their gradients.</li>
<li>Selects the top <span class="math inline">\(a \times 100%\)</span> instances.</li>
<li>Then it randomly samples <span class="math inline">\(b \times 100%\)</span> from the rest of the data.</li>
<li>Amplifies the sampled data with small gradients by a constant <span class="math inline">\(\frac{1 - a}{b}\)</span> when calculating the criterion to normalize the sum of the gradients.</li>
</ol>
<p><img src='/images/ML/lgb_3.png' width="600"></p>
<h3 id="exclusive-feature-bundling">Exclusive Feature Bundling</h3>
<p>High-dimensional data are usually very sparse. The sparsity of the feature space provides us a possibility of designing a nearly lossless approach to reduce the number of features. Specifically, in a sparse feature space, many features are mutually exclusive (they never take nonzero values simultaneously), we can safely bundle these features into a single feature. In this way, the complexity of histogram building changes from <span class="math inline">\(O(N \times M)\)</span> to <span class="math inline">\(O(N \times \text{Number of bundles})\)</span>. Then we can significantly speed up the training of GBDT without hurting the accuracy.</p>
<h4 id="which-features-to-bundle">Which Features to Bundle</h4>
<p>Partitioning features into smallest number of exclusive bundles is NP-hard, thus it is impossible to find an exact solution within polynomial time. Thus, a greedy algorithm which can produce reasonable good results are being used. Furthermore, we can allow a small fraction of conflicts which is controlled by <span class="math inline">\(\gamma\)</span> (there are usually quite a few features, although not 100% mutually exclusive, also rarely take nonzero values simultaneously) to have an even smaller number of feature bundles and further improve the computational efficiency. For small <span class="math inline">\(\gamma\)</span>, we will be able to achieve a good balance between accuracy and efficiency.</p>
<p>Intuitively, it:</p>
<ol type="1">
<li>Builds a graph with features as vertices and weighted edges as total conflicts between features (Edge only occurs when two features are not mutually exclusive).</li>
<li>Sort the features by their degrees (Number of edges)</li>
<li>Check each feature in the ordered list and either append it to existing list (if total conflicts between feature and bundle less than <span class="math inline">\(\gamma\)</span>) or create a new bundle.</li>
</ol>
<p><img src='/images/ML/lgb_4.png' width="600"></p>
<p><br></p>
<p>The time complexity of algorithm 3 is <span class="math inline">\(O(M^2)\)</span> and it is only processed once before training. This complexity is acceptable when the number of features is not very large.</p>
<h4 id="how-to-construct-the-bundle">How to Construct the Bundle</h4>
<p>The key is to ensure that the values of the original features can be identified from the feature bundles. This can be done by adding offsets to the original values of the features:</p>
<ol type="1">
<li>Calculate the offset to be added to every feature in feature bundle.</li>
<li>Iterate over every data instance and feature.</li>
<li>Initialize the new bucket as zero for instances where all features are zero.</li>
<li>Calculate the new bucket for every non zero instance of a feature by adding respective offset to original bucket of that feature</li>
</ol>
<p><img src='/images/ML/lgb_5.png' width="600"> <img src='/images/ML/lgb_6.png' width="600"></p>
<h1 id="ref">Ref</h1>
<p>https://www.researchgate.net/publication/351133481_Comparison_of_Gradient_Boosting_Decision_Tree_Algorithms_for_CPU_Performance</p>
<p>https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf</p>
<p>https://robotenique.github.io/posts/gbm-histogram/</p>
<p>https://www.aaai.org/Papers/KDD/1998/KDD98-001.pdf</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/08/20/topology/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/20/topology/" class="post-title-link" itemprop="url">topology</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-08-20 21:58:54" itemprop="dateCreated datePublished" datetime="2021-08-20T21:58:54+08:00">2021-08-20</time>
    </span>

  
    <span id="/2021/08/20/topology/" class="post-meta-item leancloud_visitors" data-flag-title="topology" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>0</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>1 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/08/09/roc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/09/roc/" class="post-title-link" itemprop="url">ROC</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-08-09 16:46:50 / Modified: 17:27:36" itemprop="dateCreated datePublished" datetime="2021-08-09T16:46:50+08:00">2021-08-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2021/08/09/roc/" class="post-meta-item leancloud_visitors" data-flag-title="ROC" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>2.4k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>2 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="roc-curve">ROC Curve</h1>
<p><code>ROC Curve</code> is a graph showing the performance of a classification model at all classification thresholds (e.g for logistic regression the threshold is default 0.5, we can adjust this threshold to 0.8 and assign examples to classes using this threhold). This curve plots two parameters:</p>
<ul>
<li>True Positive Rate (<span class="math inline">\(\text{sensitivity} = \frac{\text{True Positive}}{\text{True Positive + False Negative}}\)</span>)</li>
<li>False Positive Rate (<span class="math inline">\(1 - \text{specificity} = 1 -\frac{\text{True Negative}}{\text{True Negative + False Positive}} = \frac{\text{False Positive}}{\text{True Negative + False Positive}}\)</span>)</li>
</ul>
<p>An ROC curve plots TRP vs FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive (e.g threshold 0.1 for logistic regression), thus increasing both FP and TP.</p>
<p><img src='/images/ML/roc_1.png' width="600"></p>
<p>To compute the points an ROC curve, we could evaluate a classification model many times with different threshold and repeat this for all thresholds, but this is inefficient. Fortunately, there's an effective algorithm that can provide this information called <code>AUC</code>.</p>
<h2 id="auc">AUC</h2>
<p>AUC stands for <strong>Area under ROC curve</strong>. It measures the entire two-dimensional area underneath the entire ROC curve. Since <span class="math inline">\(TPR \in [0, 1]\)</span> and <span class="math inline">\(FPR \in [0, 1]\)</span>, the AUC can be interpreted as probability. It is the probability that the model ranks a random positive example more highly than a random negative example. Thus, by rearranging the predictions from left to right, AUC is the probability that a random positive example is positioned to the right of a random negative example:</p>
<p><img src='/images/ML/roc_2.png' width="600"></p>
<p>A model whose predictions are 100% wrong will have AUC 0, a model whose predictions are 100% correct has an AUC of 1.</p>
<p><br></p>
<h3 id="properties">Properties</h3>
<p>AUC is desirable for the following two reasons:</p>
<ul>
<li><strong>Scale-invariant</strong>: It measures hwo well predictions are ranked, rather than their absolute values.</li>
<li><strong>Classification-threshold-invariant</strong>: It measures the quality of the model's predictions irrespective of what classification threshold is chosen. (when we draw ROC, we are varying the threshold, the only thing matters to AUC is the predicted values (not class) and how true positive class samples ranked against negative class samples w.r.t their predicted values)</li>
</ul>
<p>However, AUC is not desirable when:</p>
<ol type="1">
<li><strong>Sometimes probabilities matters</strong>, AUC does not tell you how good is a predicted probability.</li>
<li><strong>Sometimes classification threshold matters</strong>, In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn't a useful metric for this type of optimization.</li>
</ol>
<h1 id="ref">Ref</h1>
<p>https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/08/06/attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/06/attention/" class="post-title-link" itemprop="url">Attention</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-08-06 15:25:09" itemprop="dateCreated datePublished" datetime="2021-08-06T15:25:09+08:00">2021-08-06</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-10-08 12:49:35" itemprop="dateModified" datetime="2021-10-08T12:49:35+08:00">2021-10-08</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/" itemprop="url" rel="index"><span itemprop="name">DL</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/RNN/" itemprop="url" rel="index"><span itemprop="name">RNN</span></a>
        </span>
    </span>

  
    <span id="/2021/08/06/attention/" class="post-meta-item leancloud_visitors" data-flag-title="Attention" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>4.9k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>4 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="neural-machine-translation-by-jointly-learning-to-align-and-translate">Neural Machine Translation By Jointly Learning to Align and Translate</h1>
<p>One problem with traditional encoder-decoder structure is that a neural network needs to be able to compress all the necessary information of a source sentence into a <strong>fixed-length</strong> vector <span class="math inline">\(\mathbf{c}\)</span>. <code>Attention</code> does not attempt to encode a whole input sentence into a single fixed-length vector. Instead, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation.</p>
<h2 id="background-rnn-encoder-decoder">Background: RNN Encoder-Decoder</h2>
<p>In the Encoder-Decoder framework, an encoder reads the input sentence, a sequence of vectors <span class="math inline">\(\mathbf{x} = (\mathbf{x}_1, ...., \mathbf{x}_{T_x})\)</span>, into a fixed length context vector <span class="math inline">\(\mathbf{c}\)</span>:</p>
<p><span class="math display">\[\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1})\]</span> <span class="math display">\[\mathbf{c} = q(\mathbf{h}_{1}, ...., \mathbf{h}_{T_x})\]</span></p>
<p>Where <span class="math inline">\(f\)</span> and <span class="math inline">\(q\)</span> are non-linear functions. Typically, <span class="math inline">\(q\)</span> is the identity function defined as <span class="math inline">\(q(\mathbf{h}_{1}, ...., \mathbf{h}_{T_x}) = \mathbf{h}_{T_x}\)</span>.</p>
<p>The decoder is often trained to predict the next word <span class="math inline">\(\mathbf{y}_{t^{\prime}}\)</span> given all the previous predicted words <span class="math inline">\(\{\hat{\mathbf{y}}_1, ..., \hat{\mathbf{y}}_{t^{\prime} - 1}\}\)</span>. In training step <span class="math inline">\(t\)</span>, we can use feed the true target value <span class="math inline">\(\mathbf{y}_{t-1}\)</span>. Then, in training of decoder, we are maximizing the log joint conditional probability (minimize the sum of per time step cross entropy):</p>
<p><span class="math display">\[L = \sum^{T_y}_{t=1} L^{t}\]</span> <span class="math display">\[-L = P_{\mathbf{Y}}(\mathbf{y}) = \sum^{T_y}_{t=1} P_{\mathbf{Y_t} | \mathbf{Y_1} ,...., \mathbf{Y_{t-1}}, \mathbf{c}}(\mathbf{y}_t | \{\mathbf{y}_{1}, ...., \mathbf{y}_{t-1}, \mathbf{c}\})\]</span></p>
<p>Where in RNN, each conditional probability distribution <span class="math inline">\(P_{\mathbf{Y_t} | \mathbf{Y_1} ,...., \mathbf{Y_{t-1}}, \mathbf{c}}\)</span> is model as <span class="math inline">\(g(\mathbf{y}_{t-1}, \mathbf{s}_t, \mathbf{c})\)</span>.</p>
<h2 id="learning-to-align-and-translate">Learning to Align and Translate</h2>
<p><code>Attention</code> contains bidirectional RNN as an encoder and a decoder that emulates searching through a source sentence during decoding a tranlation.</p>
<h3 id="decoder-general-description">Decoder: General Description</h3>
<p>In the new model, we define each conditional distribution as:</p>
<p><span class="math display">\[\hat{P}_{\mathbf{Y_t} | \mathbf{Y_1} ,...., \mathbf{Y_{t-1}}, \mathbf{c}} \triangleq g(\mathbf{y}_{t-1}, \mathbf{s}_t, \mathbf{c}_t)\]</span></p>
<p>Notice here, we have different <span class="math inline">\(\mathbf{c}_t\)</span> for each time step <span class="math inline">\(t\)</span>.</p>
<p>The context vector <span class="math inline">\(\mathbf{c}_i\)</span> depends on a sequence of <code>annotations</code> <span class="math inline">\((\mathbf{h}_1, ....., \mathbf{h}_{T_x})\)</span> which contains information about the whole input sequence (similar to hidden units in encoder) with a strong focus on the parts surrounding the <span class="math inline">\(i\)</span>th word of the input sequence.</p>
<p>The context vector <span class="math inline">\(\mathbf{c}_i\)</span> is, then computed as a weighted sum of these annotations <span class="math inline">\(\mathbf{h}_{i}\)</span>:</p>
<p><span class="math display">\[\mathbf{c}_i = \sum^{T_x}_{j=1} \alpha_{ij} \mathbf{h}_j\]</span></p>
<p><span class="math display">\[\alpha_{ij} = \frac{\exp^{e_{ij}}}{\sum^{T_x}_{k=1} \exp^{e_{ik}}}\]</span></p>
<p>Where</p>
<p><span class="math display">\[e_{ij} = a(\mathbf{s}_{i-1}, \mathbf{h}_j)\]</span></p>
<p>is an <code>alignment model</code> which scores how well the inputs around position <span class="math inline">\(j\)</span> and the output at position <span class="math inline">\(i\)</span> match. This model <span class="math inline">\(a\)</span> is parameterized by a MLP which is jointly trained with all the other components of the proposed system. The score is based on the RNN decoder's hidden state <span class="math inline">\(\mathbf{s}_{i-1}\)</span> and the <span class="math inline">\(j\)</span>th annotation <span class="math inline">\(\mathbf{h}_j\)</span> of the input sentence. <strong>It reflects the importance of each annotation vector <span class="math inline">\(\mathbf{h}_j\)</span> with respect to the previous hidden state <span class="math inline">\(\mathbf{s}_{i-1}\)</span> in deciding the current state <span class="math inline">\(\mathbf{s}_i\)</span> and generating prediction <span class="math inline">\(\hat{\mathbf{y}}_i\)</span></strong>.</p>
<p>We can think the approach of taking a weighted sum of all the annotations as computing an expected annotation, where the expectation is over possible alignments (<span class="math inline">\(\alpha_{ij}\)</span>). In other words, let <span class="math inline">\(\alpha_{ij}\)</span> be a probability that the target word <span class="math inline">\(\mathbf{y}_i\)</span> is aligned to, or translated from a source input word <span class="math inline">\(\mathbf{x}_{j}\)</span>. <strong>Then, the <span class="math inline">\(i\)</span>th context vector <span class="math inline">\(\mathbf{c}_i\)</span> is the expected value of annotations (input sequence) distributed according to probability distribution defined by <span class="math inline">\(\alpha_{ij}\)</span>.</strong></p>
<p>Intuitively, this implements a mechanism of <strong>attention</strong> in the decoder. The decoder decides parts of the source sentence to pay attention to. By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixed length vector.</p>
<h3 id="encoder-bidirectional-rnn-for-annotating-sequences">Encoder: Bidirectional RNN for Annotating Sequences</h3>
<p>A bidirectional RNN consists of forward and backward RNNs. The forward RNN <span class="math inline">\(\overset{\rightarrow}{f}\)</span> reads the input sequence from the front and has <strong>forward hidden units <span class="math inline">\(\{\overset{\rightarrow}{\mathbf{h}}_1, ...., \overset{\rightarrow}{\mathbf{h}}_{T_x}\}\)</span></strong>. <span class="math inline">\(\overset{\leftarrow}{f}\)</span> reads the sequence in the reverse order (from <span class="math inline">\(\mathbf{x}_{T_x}\)</span> to <span class="math inline">\(\mathbf{x}_1\)</span>), resulting in a sequence of <strong>backward hidden units <span class="math inline">\(\{\overset{\leftarrow}{\mathbf{h}}_1, ...., \overset{\leftarrow}{\mathbf{h}}_{T_x}\}\)</span></strong>.</p>
<p>The <strong>annotation</strong> vector <span class="math inline">\(\mathbf{h}_j\)</span> is then calculated by concatenating the forward hidden state <span class="math inline">\(\overset{\rightarrow}{\mathbf{h}}_j\)</span> and the backward hidden state <span class="math inline">\(\overset{\leftarrow}{\mathbf{h}}_j\)</span>:</p>
<p><span class="math display">\[\mathbf{h}_j = [\overset{\rightarrow}{\mathbf{h}}_j, \overset{\leftarrow}{\mathbf{h}}_j]\]</span></p>
<p>This sequence of annotations is used by the decoder and the alignment model later to compute the context vector <span class="math inline">\(\mathbf{c}_{i}\)</span>.</p>
<h1 id="effective-approaches-to-attention-based-neural-machine-translation">Effective Approaches to Attention-based Neural Machine Translation</h1>
<h1 id="long-short-term-memory-networks-for-machine-reading">Long Short-Term Memory-Networks for Machine Reading</h1>
<h1 id="ref">Ref</h1>
<p>https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#self-attention</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/08/02/rnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/02/rnn/" class="post-title-link" itemprop="url">RNN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-08-02 13:09:55" itemprop="dateCreated datePublished" datetime="2021-08-02T13:09:55+08:00">2021-08-02</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-08-25 14:17:31" itemprop="dateModified" datetime="2021-08-25T14:17:31+08:00">2021-08-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/" itemprop="url" rel="index"><span itemprop="name">DL</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/RNN/" itemprop="url" rel="index"><span itemprop="name">RNN</span></a>
        </span>
    </span>

  
    <span id="/2021/08/02/rnn/" class="post-meta-item leancloud_visitors" data-flag-title="RNN" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>13k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>11 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="recurrent-neural-network">Recurrent Neural Network</h1>
<h2 id="introduction">Introduction</h2>
<p>Consider the recurrent equation:</p>
<p><span class="math display">\[\mathbf{s}^{t} = f(\mathbf{s}^{t-1}; \; \boldsymbol{\theta})\]</span></p>
<p>For a finite time step <span class="math inline">\(\tau\)</span>, this equation can be unfolded by applying the definition <span class="math inline">\(\tau - 1\)</span> times:</p>
<p><span class="math display">\[f(f(....f(\mathbf{s}^{1}; \; \boldsymbol{\theta}) ... ; \; \boldsymbol{\theta}) ; \; \boldsymbol{\theta})\]</span></p>
<p>Then, this expression can now be represented as a DAG because it no longer involves recurrence:</p>
<p><img src="/images/ML/rnn_1.png" width="600"></p>
<p>Notice here, the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> are shared. The idea extends smoothly to:</p>
<p><span class="math display">\[\mathbf{s}^{t} = f(\mathbf{s}^{t-1}, \mathbf{x}^{t}; \; \boldsymbol{\theta})\]</span></p>
<p>We can see that now, <span class="math inline">\(s^{t}\)</span> contains information about the whole past <span class="math inline">\(\mathbf{x}^1 , ....., \mathbf{x}^t\)</span></p>
<p>Many Recurrent Neural Networks use similar idea to express their hidden units:</p>
<span class="math display">\[\begin{aligned}
\mathbf{h}^t &amp;= f(\mathbf{h}^{t-1}, \mathbf{x}^t ; \; \boldsymbol{\theta})\\
&amp;= g^t (\mathbf{x}^1 , ....., \mathbf{x}^t)
\end{aligned}\]</span>
<p>Typically, RNN will have output layers to output predictions at given timesteps. When the recurrent network is trained to perform a task that requires predicting the future from the past, the network typically learns to use <span class="math inline">\(\mathbf{h}^t\)</span> to give a lossy summary of past sequence up to time <span class="math inline">\(t\)</span>. The summary is lossy because we are mapping <span class="math inline">\(\mathbf{x}^1 , ....., \mathbf{x}^t\)</span> to a fixed length <span class="math inline">\(\mathbf{h}^t\)</span></p>
<p><img src="/images/ML/rnn_2.png" width="600"></p>
<p>The unfolded structure has several advantages:</p>
<ol type="1">
<li>The learned model <span class="math inline">\(f\)</span> is defined as transition from hidden units (input) <span class="math inline">\(h^{t - 1}\)</span> to <span class="math inline">\(h^{t}\)</span> (output) regardless the value of time <span class="math inline">\(t\)</span>. Thus, we can have one model for different lengths of sequences.</li>
<li>The parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> are shared.</li>
</ol>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/08/02/rnn/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/29/adam/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/29/adam/" class="post-title-link" itemprop="url">Adam</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-29 10:38:43" itemprop="dateCreated datePublished" datetime="2021-07-29T10:38:43+08:00">2021-07-29</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-30 00:01:46" itemprop="dateModified" datetime="2021-07-30T00:01:46+08:00">2021-07-30</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/" itemprop="url" rel="index"><span itemprop="name">DL</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/Techniques/" itemprop="url" rel="index"><span itemprop="name">Techniques</span></a>
        </span>
    </span>

  
    <span id="/2021/07/29/adam/" class="post-meta-item leancloud_visitors" data-flag-title="Adam" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>4.8k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>4 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="adam-a-method-for-stochastic-optimization">ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION</h1>
<p><img src='/images/ML/adam_1.png' width="600"></p>
<p>Let <span class="math inline">\(F\)</span> be a noisy objective function (stochastic function) defined as <span class="math inline">\(F(\boldsymbol{\theta})\)</span> that is differentiable w.r.t <span class="math inline">\(\boldsymbol{\theta}\)</span>, we are interested in minimizing the expected value of this random function:</p>
<p><span class="math display">\[\min_{\boldsymbol{\theta}} E_{F}[F(\boldsymbol{\theta})]\]</span></p>
<p>Let <span class="math inline">\(F_1(\boldsymbol{\theta}), ...., F_{T} (\boldsymbol{\theta})\)</span> be a random sample of <span class="math inline">\(F(\boldsymbol{\theta})\)</span> and let <span class="math inline">\(f_1 (\boldsymbol{\theta}), ..., f_T(\boldsymbol{\theta})\)</span> be the realization of random sample. <strong>This random sample can be forms of mini-batches of data which the distribution does not depend on the parameter <span class="math inline">\(\boldsymbol{\theta}\)</span>.</strong></p>
<p>Then:</p>
<p><span class="math display">\[\nabla_{\boldsymbol{\theta}} E_F[F (\boldsymbol{\theta})] = E_F[\nabla_{\boldsymbol{\theta}} F (\boldsymbol{\theta})]\]</span></p>
<p><br></p>
<p>Given the individual sample gradient <span class="math inline">\(\nabla_{\boldsymbol{\theta}} f_1 (\boldsymbol{\theta}), ...., \nabla_{\boldsymbol{\theta}} f_T(\boldsymbol{\theta})\)</span>, one way to estimate the expectation of gradient, <span class="math inline">\(E_{F} [\nabla_{\boldsymbol{\theta}} F_t(\boldsymbol{\theta})]\)</span>, is to use stochastic approximation (similar to Momentum):</p>
<p><span class="math display">\[m_t \leftarrow \beta_1 {m}_{t} + (1 - \beta_1) \nabla_{\boldsymbol{\theta}} f_t(\boldsymbol{\theta})\]</span></p>
<p>Where <span class="math inline">\(m_t\)</span> is the average up to sample <span class="math inline">\(t\)</span>, and <span class="math inline">\(\beta_1 \in [0, 1)\)</span> is the decay rates.</p>
<p>At the same time, we can use SA to estimate the second moment of the gradient which is the un-centered variance (This is similar to RMSProp):</p>
<p><span class="math display">\[v_t \leftarrow \beta_2 {v}_{t} + (1 - \beta_2) \nabla_{\boldsymbol{\theta}} f_t(\boldsymbol{\theta})^2\]</span></p>
<p>However, these estimates are biased toward 0 if we initialize them to be 0 especially during initial timesteps and especially when the decay rates are small (<span class="math inline">\(\beta_1, \beta_2\)</span> close to 1). Thus, we need to apply bias correction.</p>
<h2 id="initial-bias-correction">Initial Bias Correction</h2>
<p>Let <span class="math inline">\(\mathbf{G} = \nabla_{\boldsymbol{\theta}} F\)</span> be the gradient of the stochastic objective <span class="math inline">\(F\)</span>, we wish to estimate its second raw moment using SA of the squared gradient with decay rate <span class="math inline">\(\beta_2\)</span>. Let <span class="math inline">\(\mathbf{G}_1, ...., \mathbf{G}_T\)</span> be random sample of <span class="math inline">\(\mathbf{G}\)</span> that draws from the gradient distribution <span class="math inline">\(P(\mathbf{G})\)</span>. Suppose we initialize our SA procedure at <span class="math inline">\({v}_0 = 0\)</span>, then after <span class="math inline">\(t\)</span> steps, we have:</p>
<p><span class="math display">\[v_1 = (1 - \beta_2) \mathbf{G}^2_1\]</span></p>
<p><span class="math display">\[v_2 = \beta_2(1 - \beta_2) \mathbf{G}^2_1 + (1 - \beta_2) \cdot \mathbf{G}^2_2 = \beta_2 (1 - \beta_2) (\mathbf{G}^2_1 + \mathbf{G}^2_2)\]</span></p>
<p><span class="math display">\[\implies {v}_t = (1 - \beta_2) \sum^{t}_{i=1} \beta^{t - i}_2 \mathbf{G}^2_{i}\]</span></p>
<p>Where <span class="math inline">\(\mathbf{G}^2 = \|\mathbf{G}\|^2_2\)</span>. We want the SA estimator to be unbiased estimator of second moment of gradient but we know that there is initialization bias (discrepancy) of SA estimator, we denote this discrepancy <span class="math inline">\(\eta\)</span>. Since additive discrepancy can be keep small by assigning less weight to history, we want two sides to be equal:</p>
<p><span class="math display">\[E_{\mathbf{G}} [\mathbf{G}^2] = E_\mathbf{G}[\mathbf{v}^2_t] + \eta = E_\mathbf{G} [(1 - \beta_2) \sum^{t}_{i=1} \beta_2^{t - i} \mathbf{G}^2_{i}] + \eta\]</span></p>
<p>However, since <span class="math inline">\(t &lt; \infty\)</span>, we have a proportion bias:</p>
<span class="math display">\[\begin{aligned}
E_\mathbf{G}[\mathbf{v}^2_t] + \eta &amp;= E_{\mathbf{G}} [\mathbf{G}^2] (1 - \beta_2)  \sum^{t}_{i=1} \beta_2^{t - i} + \eta\\
&amp;= E_{\mathbf{G}} [\mathbf{G}^2] (1 - \beta_2)  \sum^{t}_{i=1} \beta_2^{t}(\frac{1}{\beta_2})^i + \eta\\
&amp;= E_{\mathbf{G}} [\mathbf{G}^2] (1 - \beta_2)  \frac{1}{\beta_2} \sum^{t}_{i=0}\beta_2^{t}(\frac{1}{\beta_2})^i + \eta\\
&amp;= E_{\mathbf{G}} [\mathbf{G}^2] (1 - \beta_2)  \frac{1}{\beta_2} \beta_2^{t} \frac{\beta^t_2 - 1}{\beta^t_2} \frac{\beta_2}{\beta_2 - 1}+ \eta\\
&amp;= E_{\mathbf{G}} [\mathbf{G}^2] \underbrace{(1 - \beta^t_2)}_{\text{This term we do not want}} + \eta 
\end{aligned}\]</span>
<p>Thus, we can apply a bias correction term on the estimator to correct for this proportion bias <span class="math inline">\(\frac{1}{1 - \beta^t_2}\)</span>.</p>
<p>The same correction <span class="math inline">\(\frac{1}{1 - \beta^t_1}\)</span>is applied on first moment estimator of the gradient.</p>
<h2 id="adamax">AdaMax</h2>
<p>In Adam, the current average gradient estimate <span class="math inline">\(\hat{m}_t\)</span> is scaled inversely to history proportional to the scaled <span class="math inline">\(L^2\)</span> norm of their individual current and past gradients (i.e <span class="math inline">\(\frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon}\)</span>). We can generalize <span class="math inline">\(L^2\)</span> norm to a <span class="math inline">\(L^\infty\)</span> norm based update rule. This leads to a surprisingly simple and stable algorithm:</p>
<p><img src='/images/ML/adam_2.png' width="600"></p>
<p><br></p>
<p>In case of <span class="math inline">\(L^p\)</span> norm, <span class="math inline">\(\mathbf{v}_t\)</span> is defined to be:</p>
<span class="math display">\[\begin{aligned}
{v}_t &amp;\leftarrow \beta^p_2  + {v}_t (1 - \beta^p_2) \|\mathbf{G}_i\|^p_p\\
&amp;\leftarrow (1 - \beta_2^p) \sum^{t}_{i=1} \beta^{p(t - i)}_2 \|\mathbf{G}_i\|^p_p\\
\end{aligned}\]</span>
<p>Note define:</p>
<p><span class="math display">\[u_t = \lim_{p \rightarrow \infty} (v_t)^{\frac{1}{p}}\]</span></p>
<p>Then:</p>
<span class="math display">\[\begin{aligned}
u_t &amp;= \lim_{p \rightarrow \infty} (v_t)^{\frac{1}{p}}\\
&amp;= \lim_{p \rightarrow \infty} ((1 - \beta_2^p) \sum^{t}_{i=1} \beta^{p(t - i)}_2 \|\mathbf{G}_i\|^p_p)^\frac{1}{p}\\
&amp;= \lim_{p \rightarrow \infty} (1 - \beta_2^p)^\frac{1}{p} (\sum^{t}_{i=1} \beta^{p(t - i)}_2 \|\mathbf{G}_i\|^p_p)^\frac{1}{p}\\
&amp;= \lim_{p \rightarrow \infty} (\sum^{t}_{i=1}(\beta^{(t - i)}_2 \|\mathbf{G}_i\|_p)^p)^\frac{1}{p}\\
&amp;= \| \beta^{(t - i)}_2 \|\mathbf{G}_i\|_{\infty}\|_{\infty}\\
&amp;= \max(\beta^{(t - 1)}_2 \|\mathbf{G}_1\|_{\infty}, .... , \|\mathbf{G}_t\|_{\infty}) 
\end{aligned}\]</span>
<p>Which corresponding to:</p>
<p><span class="math display">\[u_t \leftarrow \max(\beta_2 u_{t}, \|\mathbf{G}_t\|_{\infty})\]</span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/29/adaptive-lr/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/29/adaptive-lr/" class="post-title-link" itemprop="url">Basic Adaptive LR Algorithms</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-29 10:38:32" itemprop="dateCreated datePublished" datetime="2021-07-29T10:38:32+08:00">2021-07-29</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-08-18 16:12:42" itemprop="dateModified" datetime="2021-08-18T16:12:42+08:00">2021-08-18</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/" itemprop="url" rel="index"><span itemprop="name">DL</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/Techniques/" itemprop="url" rel="index"><span itemprop="name">Techniques</span></a>
        </span>
    </span>

  
    <span id="/2021/07/29/adaptive-lr/" class="post-meta-item leancloud_visitors" data-flag-title="Basic Adaptive LR Algorithms" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>2.6k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>2 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="algorithms-with-adaptive-learning-rates">Algorithms with Adaptive Learning Rates</h1>
<p>Learning rate is reliably one of the hyperparameters that is the most difficult to set because it has a significant impact on model performance. At each iteration, the cost is often highly sensitive to some directions in parameter space and insensitive to others. Momentum solves some of the problems in the cost of introducing another hyperparameter. Thus, it is natural to consider algorithms that have separate learning rate for each parameter and automatically adapt learning rates for each of the parameter.</p>
<p>The <code>delta-bar-delta</code> algorithm (base on full-batch) is an early heuristic approach to adapting individual learning rates for model parameters during training, it is based on intuitive idea similar to momentum:</p>
<blockquote>
<p>If the partial derivative of the loss, with respect to a given model parameter, remains the same sign, then the learning rate should increase, if the partial derivative with respect to the parameter changes sign, then the learning rate should decrease.</p>
</blockquote>
<p>We would like to extend the idea to mini-batch scenario.</p>
<h2 id="adagrad">AdaGrad</h2>
<p><img src='/images/ML/adagrad_1.png' width="600"></p>
<p>The <code>AdaGrad</code> algorithm, individually adapts the learning rates of all model parameters by scaling them inversely proportional to square root of the sum of all of their historical squared values:</p>
<ol type="1">
<li><p>If <span class="math inline">\(g_1\)</span> is large constantly larger than <span class="math inline">\(g_2\)</span>, then:</p>
<p><span class="math display">\[\sqrt{r_1} &gt; \sqrt{r_2} \implies \epsilon_1 &lt; \epsilon_2\]</span></p>
<p>This makes sense because we want to take a small step in the gradient direction when the magnitude of gradient is large, especially when we have noisy gradient. Conversely, we would like to take slightly larger step than large gradient case when we have smaller gradient.</p></li>
<li><p>If <span class="math inline">\(r_i\)</span> is less than 1, we have increasing learning rate compare to base learning rate:</p>
<p><span class="math display">\[r_i &lt; 1 \implies \epsilon_i &gt; \epsilon\]</span></p>
<p>This helps us to get out of the local minimum or flat region of the surface by taking larger steps.</p></li>
</ol>
<p>AdaGrad is designed to converge rapidly when applied to a convex optimization problem, so when it finds a convex structure, it can converge rapidly.</p>
<p>However, in non-convex optimization problem (training neural networks) the accumulated gradient <span class="math inline">\(\mathbf{r}\)</span> starts accumulating at the beginning, <strong>this will introduce excessive decrease in the effective learning in later training steps (at end, we will have large <span class="math inline">\(\mathbf{r}\)</span>, so the learning rates will be small to prevent effective learning in later stages or early large gradients will prevent learning in early stages)</strong>.</p>
<h2 id="rmsprop">RMSProp</h2>
<p>The <code>RMSProp</code> algorithm modifies AdaGrad so that it can perform better in non-convex setting by changing the gradient accumulation into an exponentially weighted moving average, so the early accumulation becomes less and less important. This structure helps in non-convex problem by discarding the history from extreme past so when we arrive at a convex bowl, we have sufficiently large learning rate to converge rapidly.</p>
<p><img src='/images/ML/rmsprop_1.png' width="600"></p>
<p>The algorithm introduces a new parameter <span class="math inline">\(\rho\)</span> that controls for the weight of accumulated gradient.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/29/momentum/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/29/momentum/" class="post-title-link" itemprop="url">Momentum</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-07-29 10:37:44 / Modified: 14:25:49" itemprop="dateCreated datePublished" datetime="2021-07-29T10:37:44+08:00">2021-07-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/" itemprop="url" rel="index"><span itemprop="name">DL</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/Techniques/" itemprop="url" rel="index"><span itemprop="name">Techniques</span></a>
        </span>
    </span>

  
    <span id="/2021/07/29/momentum/" class="post-meta-item leancloud_visitors" data-flag-title="Momentum" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>3.2k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>3 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="momentum">Momentum</h1>
<p>While SGD remains a very popular optimization strategy, learning with it can be slow. The method of momentum is designed to accelerate learning, especially in the face of <strong>high curvature</strong> (large change of direction of the curve in small amount time), <strong>small but consistent gradients</strong> (flat surface) or <strong>noisy gradients</strong> (with high variance). The momentum algorithm accumulates an exponentially decaying moving average of past gradients and continues to move in their direction.</p>
<p><br></p>
<p>The momentum algorithm introduces several variables:</p>
<ol type="1">
<li>A vector <span class="math inline">\(\mathbf{v}\)</span> that plays a role of velocity (with direction and speed). The velocity is set to an exponentially decaying average of the negative gradient.</li>
<li>A hyparameter <span class="math inline">\(\alpha \in [0, 1)\)</span> determins how quickly the contributions of previous gradients exponentially decay.</li>
</ol>
<p>The update rule is given by:</p>
<p><span class="math display">\[\mathbf{v} \leftarrow \alpha \mathbf{v} - \epsilon \nabla_{\boldsymbol{\theta}} (\frac{1}{N} \sum^{N}_{i=1} L(\mathbf{f} (\mathbf{x}_i; \; \boldsymbol{\theta}), \mathbf{y}_i))\]</span></p>
<p><span class="math display">\[\boldsymbol{\theta} \rightarrow \boldsymbol{\theta} + \mathbf{v}\]</span></p>
<p>Previously, in SGD, the size of the step was simply the norm of the gradient multiplied by the learning rate:</p>
<p><span class="math display">\[\epsilon \nabla_{\boldsymbol{\theta}} (\frac{1}{N} \sum^{N}_{i=1} L(\mathbf{f} (\mathbf{x}_i; \; \boldsymbol{\theta}), \mathbf{y}_i))\]</span></p>
<p>Now the size of the step depends on <strong>how large and how aligned a sequence of gradients</strong> are. The step size is <strong>largest</strong> when many successive gradients point in exactly the same direction. If the momentum algorithm always observe gradient <span class="math inline">\(\mathbf{g}\)</span>, then it will accelerate in the direction of <span class="math inline">\(-\mathbf{g}\)</span>:</p>
<p><span class="math display">\[\mathbf{v}_1 \leftarrow - \epsilon \mathbf{g}\]</span></p>
<p><span class="math display">\[\mathbf{v}_2 \leftarrow -\mathbf{g} \epsilon(\alpha + 1)\]</span></p>
<p><span class="math display">\[\mathbf{v}_N \leftarrow -\mathbf{g} \epsilon \sum^{N-1}_{i=0} \alpha^i\]</span></p>
<p><span class="math display">\[\implies \|\mathbf{v}_{\infty}\| = \frac{\epsilon \|\mathbf{g}\|}{1 - \alpha}\]</span></p>
<p>The terminal velocity will have speed <span class="math inline">\(\frac{\epsilon \|\mathbf{g}\|}{1 - \alpha} \gg \epsilon \|\mathbf{g}\|\)</span>. This makes sense because if we receive consistent small gradients, we would like to take larger steps because we are confident we are in the right direction. One the other hand, consistently changing direction gradients (high curvature) would cause the gradient to be small to allow convergence.</p>
<p><img src='/images/ML/momentum_1.png' width="600"></p>
<h2 id="nesterov-momentum">Nesterov Momentum</h2>
<p>Nesterov Momentum is inspired by Nesterov's accelerated gradient method, the update rules in this case are given by:</p>
<p><span class="math display">\[\mathbf{v} \leftarrow \alpha \mathbf{v} - \epsilon \nabla_{\boldsymbol{\theta}} (\frac{1}{N} \sum^{N}_{i=1} L(\mathbf{f} (\mathbf{x}_i; \; \boldsymbol{\theta} + \alpha \mathbf{v}), \mathbf{y}_i))\]</span></p>
<p><span class="math display">\[\boldsymbol{\theta} \rightarrow \boldsymbol{\theta} + \mathbf{v}\]</span></p>
<p>This is similar to momentum, but before taking the gradient, we first take one step forward using previous velocity, then we take the gradient there and adjust velocity accordingly. We can also think of this as attempting to add a <strong>correlation factor</strong> to the standard method of momentum.</p>
<p><img src='/images/ML/momentum_2.png' width="600"></p>
<h1 id="implementation">Implementation</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SGD</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, lr, model_vars</span>):</span></span><br><span class="line">        self.model_vars = model_vars</span><br><span class="line">        self.lr = lr</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, grad</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i, v <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.model_vars):</span><br><span class="line">            self.model_vars[i] = v - self.lr * grad[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.model_vars</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Momentum</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, lr, model_vars, alpha=<span class="number">0.9</span></span>):</span></span><br><span class="line">        self.lr = lr</span><br><span class="line">        self.model_vars = model_vars</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.v = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, grad</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i, v <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.model_vars):</span><br><span class="line">            self.v = self.v * self.alpha - self.lr * grad[i]</span><br><span class="line">            self.model_vars[i] = self.model_vars[i] + self.v</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.model_vars</span><br></pre></td></tr></table></figure>
<h1 id="ref">Ref</h1>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://swag1ong.github.io/2021/07/22/sac/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/others/favicon.jpeg">
      <meta itemprop="name" content="Zhu, Zhaoyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GoGoGogo!">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/22/sac/" class="post-title-link" itemprop="url">SAC</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-07-22 11:29:33" itemprop="dateCreated datePublished" datetime="2021-07-22T11:29:33+08:00">2021-07-22</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-09-16 13:41:41" itemprop="dateModified" datetime="2021-09-16T13:41:41+08:00">2021-09-16</time>
      </span>

  
    <span id="/2021/07/22/sac/" class="post-meta-item leancloud_visitors" data-flag-title="SAC" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>227</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>1 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</h1>
<h2 id="introductions-and-notations">Introductions and Notations</h2>
<p>Maximum entropy reinforcement learning optimizes policies to maximize both the expected return and the expected entropy of the policy.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>
<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhu, Zhaoyang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Symbols count total">889k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">13:28</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.3" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{&quot;enable&quot;:true,&quot;app_id&quot;:&quot;gPOc95wWW2Hwp2pkVSAAz28m-MdYXbMMI&quot;,&quot;app_key&quot;:&quot;7Sf65xCHXfEEdBvu29UHSYdV&quot;,&quot;server_url&quot;:&quot;https:&#x2F;&#x2F;swag1ong.github.io&quot;,&quot;security&quot;:true}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{&quot;enable&quot;:true,&quot;repo&quot;:&quot;swag1ong&#x2F;swag1ong.github.io&quot;,&quot;issue_term&quot;:&quot;pathname&quot;,&quot;theme&quot;:&quot;github-light&quot;,&quot;pathname&quot;:&quot;pathname&quot;}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
